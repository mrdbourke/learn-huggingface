[
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Goal\n# Start with dataset\n  # Could generate this dataset or pre-existing\n  # Can have the dataset labelled manually or labelled with an LLM\n  # Could label this dataset manually or have it zero-shot labelled\n# Build a custom text classifier on labelled data\n  # Test text classifier on labelled data vs zero-shot model\n# Next:\n # Create a small dataset with GPT4o, e.g. 50x spam/not_spam emails and train a classifier on it âœ…\n    # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n # Save the dataset to Hugging Face Datasets âœ…\n    # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n # Train a classifier on it\n # Save the model to the Hugging Face Model Hub\n # Create a with Gradio and test the model in the wild\ntry:\n  import datasets, evaluate, accelerate\nexcept:\n  !pip install -U datasets, evaluate, accelerate\n  # !pip install -U datasets, evaluate, accelerate\n  import datasets, evaluate, accelerate\n\nfrom datasets import Dataset\n\nimport random\nimport pandas as pd\n\nimport transformers\n\n# from google.colab import drive\n# drive.mount('/content/drive')\n# Load the dataset\ndataset = datasets.load_dataset(\"mrdbourke/learn_hf_food_not_food_image_captions\")\nimport random\n\nrandom_idx = random.randint(0, len(dataset[\"train\"]))\nrandom_sample = dataset[\"train\"][random_idx]\n\nprint(f\"[INFO] Random sample from dataset:\\n{random_sample}\")\n\n[INFO] Random sample from dataset:\n{'text': 'Barbecue grill waiting on a patio', 'label': 'not_food'}\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\"), see: https://huggingface.co/docs/datasets/en/process#map\ndef map_labels_to_number(example):\n  example[\"label\"] = 0 if example[\"label\"] == \"not_food\" else 1\n  return example\n\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\ndataset.shuffle()[:5]\n\n{'text': ['Silverware organizer keeping cutlery tidy in a kitchen drawer',\n  'Gluten-free sushi roll using tamari sauce instead of soy sauce.',\n  'A bowl of sliced oranges with a sprinkle of cinnamon and a side of cloves',\n  'A slice of veggie pizza loaded with colorful and nutritious vegetables',\n  'Two people sitting at a dining room table with a newspaper on it'],\n 'label': [0, 1, 1, 1, 0]}\n# Create train/test splits, see: https://huggingface.co/docs/datasets/en/process#split\ndataset = dataset.train_test_split(test_size=0.2)\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\\n{random_sample_train}\")\nprint(f\"[INFO] Random sample from testing dataset:\\n{random_sample_test}\")\n\n[INFO] Random sample from training dataset:\n{'text': \"Wooden cutting board with a chef's knife ready for use\", 'label': 0}\n[INFO] Random sample from testing dataset:\n{'text': 'Fennel in a bowl, sprinkled with lemon zest and served with a side of olive oil for a light, refreshing dish.', 'label': 1}",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#preprocess",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#preprocess",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 Preprocess",
    "text": "1 Preprocess\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#preprocess\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n  return tokenizer(examples[\"text\"], truncation=True)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\ntokenized_dataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\n\ntokenized_dataset[\"train\"][0], tokenized_dataset[\"test\"][0]\n\n({'text': 'Set of books stacked on a desk',\n  'label': 0,\n  'input_ids': [101, 2275, 1997, 2808, 16934, 2006, 1037, 4624, 102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'Set of tea towels folded in a kitchen',\n  'label': 0,\n  'input_ids': [101, 2275, 1997, 5572, 24213, 6999, 1999, 1037, 3829, 102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n\n\n# Collate examples and pad them each batch\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ndata_collator\n\nDataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 Evaluation",
    "text": "2 Evaluation\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 Train",
    "text": "3 Train\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\n3 steps for training:\n\nDefine model\nDefine training arguments\nPass training arguments to Trainer\nCall train()\n\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\n\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id\n)\n\n\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n# TODO: Turn off Weights & Biases logging? Or add it in?\ntraining_args = TrainingArguments(\n    output_dir=\"food_not_food_text_model\", # TODO: change this path to model save path, e.g. 'learn_hf_food_not_food_text_classifier_model' \n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    # push_to_hub=True\n)\n\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n\n# Batch size 16\n#  [ 391/15234 00:22 &lt; 14:27, 17.12 it/s, Epoch 0.05/2]\n\n# Batch size 32\n# [ 724/7618 01:08 &lt; 10:51, 10.58 it/s, Epoch 0.19/2]\n\n# Batch size 64\n#  [ 150/3810 00:31 &lt; 12:52, 4.74 it/s, Epoch 0.08/2]\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n0.076706\n0.980000\n\n\n2\nNo log\n0.110371\n0.980000\n\n\n3\nNo log\n0.116438\n0.980000\n\n\n4\nNo log\n0.118240\n0.980000\n\n\n5\nNo log\n0.123329\n0.980000\n\n\n6\nNo log\n0.127266\n0.980000\n\n\n7\nNo log\n0.128845\n0.980000\n\n\n8\nNo log\n0.129170\n0.980000\n\n\n9\nNo log\n0.129970\n0.980000\n\n\n10\nNo log\n0.130197\n0.980000\n\n\n\n\n\n\nTrainOutput(global_step=70, training_loss=0.0031478273017065865, metrics={'train_runtime': 7.0065, 'train_samples_per_second': 285.449, 'train_steps_per_second': 9.991, 'total_flos': 17222831628384.0, 'train_loss': 0.0031478273017065865, 'epoch': 10.0})\n\n\n\n# Optional: push the model to Hugging Face Hub for re-use later\n# Note: Requires Hugging Face login\n# trainer.push_to_hub()\n\n\n# Save model\n# See: https://discuss.huggingface.co/t/how-to-save-my-model-to-use-it-later/20568/4\n# TODO: Make a models/ dir to save models to (so we don't have to commit them to git)\ntrainer.save_model(\"learn_hf_food_not_food_text_classifier_model\")\n\n\n# TK - Push model to hub (for later re-use)\n# TODO: Push this model to the hub to be able to use it later",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#inference",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#inference",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 Inference",
    "text": "4 Inference\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nsample_text = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n\n4.1 Pipeline mode\n\nfrom transformers import pipeline\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", model=\"./learn_hf_food_not_food_text_classifier_model\")\nfood_not_food_classifier(sample_text)\n\n[{'label': 'food', 'score': 0.9857270121574402}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9952113032341003}]\n\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"I'm feeling overwhelmed by this workload â€“ it's a real information buffet.\",\n    \"We're brainstorming new content ideas, hoping to cook up something innovative.\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'food', 'score': 0.5004892349243164},\n {'label': 'not_food', 'score': 0.8031824827194214},\n {'label': 'food', 'score': 0.5688744187355042},\n {'label': 'food', 'score': 0.517037570476532},\n {'label': 'not_food', 'score': 0.6362245678901672},\n {'label': 'not_food', 'score': 0.7544245719909668},\n {'label': 'not_food', 'score': 0.7407550811767578},\n {'label': 'not_food', 'score': 0.5384439826011658},\n {'label': 'not_food', 'score': 0.8630059957504272},\n {'label': 'not_food', 'score': 0.9562842845916748}]\n\n\n\n\n4.2 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food\n\n\n\n# TODO: Make a demo of the model with Gradio and test it in the wild",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Iâ€™d like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nItâ€™s made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ðŸ¤— (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nTODO:"
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ðŸ¤— (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]