[
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Goal\n# Start with dataset\n  # Could generate this dataset or pre-existing\n  # Could label this dataset manually or have it zero-shot labelled\n# Build a custom text classifier on labelled data\n  # Test text classifier on labelled data vs zero-shot model\nWhat happens if I put a markdown cell here, is that easy to tell apart?\ntry:\n  import datasets, evaluate, accelerate\nexcept:\n  !pip install datasets\n  !pip install accelerate -U\n  !pip install evaluate\n  import datasets, evaluate, accelerate\n\nfrom datasets import Dataset\n\nimport random\nimport pandas as pd\n\nimport transformers\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nCollecting datasets\n  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 542.0/542.0 kB 7.6 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\nRequirement already satisfied: pyarrow&gt;=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\nCollecting dill&lt;0.3.9,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 116.3/116.3 kB 8.8 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 194.1/194.1 kB 9.3 MB/s eta 0:00:00\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134.8/134.8 kB 11.2 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&lt;=2024.3.1,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\nCollecting huggingface-hub&gt;=0.21.2 (from datasets)\n  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 401.2/401.2 kB 14.7 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.21.2-&gt;datasets) (4.11.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2023.4)\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.16.0)\nInstalling collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\nSuccessfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\nCollecting accelerate\n  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 302.4/302.4 kB 6.1 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.14.0)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (4.11.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2023.6.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2.2.0)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch&gt;=1.10.0-&gt;accelerate)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2024.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)\nInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\nSuccessfully installed accelerate-0.30.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 84.1/84.1 kB 3.1 MB/s eta 0:00:00\nRequirement already satisfied: datasets&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec[http]&gt;=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate) (3.14.0)\nRequirement already satisfied: pyarrow&gt;=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate) (0.6)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate) (3.9.5)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.7.0-&gt;evaluate) (4.11.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2023.4)\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2024.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate) (23.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate) (1.9.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate) (4.0.3)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nMounted at /content/drive\nEven a markdown cell here, how does this look?\nfile_path = \"/content/drive/MyDrive/nutrify/coyo700m/food_not_food_captions/food_not_food_captions_rows_1000000.csv\"\n\ndf = pd.read_csv(file_path)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nid\nurl\ntext\nwidth\nheight\nimage_phash\ntext_length\nword_count\nnum_tokens_bert\nnum_tokens_gpt\n...\nclip_similarity_vitb32\nclip_similarity_vitl14\nnsfw_score_opennsfw2\nnsfw_score_gantman\nwatermark_score\naesthetic_score_laion_v2\ntext_food_not_food_labels\ntext_food_not_food_scores\ntext_food_not_food_label\ntext_food_not_food_score\n\n\n\n\n0\n68719569143\nhttps://m.media-amazon.com/images/I/513i2gEKSK...\nMUSCLETECH Amino NRG SX-7 Revolution Ultimate ...\n353\n500\naf99d06cd0e2c31d\n97\n16\n26\n29\n...\n0.404541\n0.375977\n0.000806\n0.014266\n0.002690\n4.147951\n['food', 'not_food']\n[0.6513671875, 0.348388671875]\nfood\n0.651367\n\n\n1\n5274220575161\nhttps://therapydiaportland.com/wp-content/uplo...\n9 yoga poses for runners therapydia portland\n470\n353\nac3c3b73664c6562\n44\n7\n8\n10\n...\n0.285645\n0.264648\n0.003700\n0.078907\n0.007236\n4.882448\n['not_food', 'food']\n[0.9609375, 0.039337158203125]\nnot_food\n0.960938\n\n\n2\n7962869880803\nhttps://cdn.fashiola.ph/L43220811/ted-baker-qi...\nTed Baker Qinala heeled ankle boot with T bran...\n300\n450\nb93687dbc8cca4d0\n60\n11\n13\n14\n...\n0.318115\n0.261230\n0.006603\n0.024137\n0.032537\n5.102950\n['not_food', 'food']\n[0.99267578125, 0.00788116455078125]\nnot_food\n0.992676\n\n\n3\n3427384496754\nhttps://photos.classiccars.com/cc-temp/listing...\n1947 Plymouth Street Rod (CC-1206989) for sale...\n1280\n960\n8e44d53b6bc614ea\n68\n11\n19\n18\n...\n0.315674\n0.323730\n0.005131\n0.039114\n0.000400\n4.575576\n['not_food', 'food']\n[0.99365234375, 0.006145477294921875]\nnot_food\n0.993652\n\n\n4\n2611340287260\nhttps://cdn.shopify.com/s/files/1/0425/2244/39...\nFive Nights at Freddy's, Fazbear Frights #1, I...\n300\n300\na9a586dd8696985d\n57\n11\n19\n17\n...\n0.336914\n0.264893\n0.004673\n0.006453\n0.002217\n3.851733\n['not_food', 'food']\n[0.96337890625, 0.036834716796875]\nnot_food\n0.963379\n\n\n\n\n5 rows √ó 21 columns\nAnd another markdown cell here, can you tell code part from markdown?\n# How many rows contain \"menu\"\ndef contains_menu(text):\n  if \"menu\" in text:\n    return True\n  else:\n    return False\n\ndf[\"menu_in_text\"] = df[\"text\"].apply(lambda x: contains_menu(x))\ndf[\"menu_in_text\"].value_counts()\n\nmenu_in_text\nFalse    999637\nTrue        363\nName: count, dtype: int64\ndf[\"text_food_not_food_label\"].value_counts()\n\ntext_food_not_food_label\nnot_food    935125\nfood         64875\nName: count, dtype: int64\n# How many food samples have a score over X\nscores = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.97, 0.98, 0.99]\nfor score in scores:\n    print(f'Number of rows over {score}: {len(df[(df[\"text_food_not_food_label\"] == \"food\") & (df[\"text_food_not_food_score\"] &gt; score)])}')\n\nNumber of rows over 0.65: 60006\nNumber of rows over 0.7: 58333\nNumber of rows over 0.75: 56501\nNumber of rows over 0.8: 54560\nNumber of rows over 0.85: 52322\nNumber of rows over 0.9: 49524\nNumber of rows over 0.95: 44985\nNumber of rows over 0.97: 41839\nNumber of rows over 0.98: 39135\nNumber of rows over 0.99: 34326\n# Create a dataset of food rows over 0.85 + 100k random \"not_food\" rows\ndf_food_rows_sample = df[(df[\"text_food_not_food_label\"] == \"food\") & (df[\"text_food_not_food_score\"] &gt;= 0.85)]\nlen(df_food_rows_sample)\n\n52322\n# Create a sample of 100k \"not_food\" rows\ndf_not_food_rows_sample = df[df[\"text_food_not_food_label\"] == \"not_food\"].sample(100_000)\nlen(df_not_food_rows_sample)\n\n100000\ndf_subset = pd.concat([df_food_rows_sample, df_not_food_rows_sample]).reset_index(drop=True)\nlen(df_subset)\n\n152322\nfor row in df_subset.sample(5).iterrows():\n  row_item = row[1]\n  text = row_item[\"text\"]\n  food_label = row_item[\"text_food_not_food_label\"]\n  food_score = row_item[\"text_food_not_food_score\"]\n  print(f\"Text: {text}\")\n  print(f\"Food label: {food_label}\")\n  print(f\"Food score: {food_score}\")\n  print(f\"------\")\n  print()\n\nText: Deep Red - Audio commentary with Argento expert Thomas Rostock\nFood label: not_food\nFood score: 0.9609375\n------\n\nText: Young shaggy scalycap mushrooms (&lt;B&gt;Pholiota squarrosa&lt;/B&gt;, Russian name Cheshuychatka) in Lisiy Nos, west from Saint Petersburg. Russia, &lt;A HREF=\"../date-en/2016-09-12.htm\"&gt;September 12, 2016&lt;/A\nFood label: food\nFood score: 0.99609375\n------\n\nText: Picture of Belgian Chocolate Unicorn Lolly\nFood label: food\nFood score: 0.99267578125\n------\n\nText: Simply Shaker 7050 Baby Spinach\nFood label: food\nFood score: 0.998046875\n------\n\nText: A basket of lilies with teddy bear\nFood label: not_food\nFood score: 0.99365234375\n------\n# See here: https://huggingface.co/docs/datasets/en/loading#python-list-of-dictionaries\n# Make a list of dicts for HF Dataset\ndata_dict = df_subset[[\"text\", \"text_food_not_food_label\"]].to_dict(orient=\"records\")\ndata_dict_list = []\nfor item in data_dict:\n  item_text = item[\"text\"]\n  item_label = 0 if item[\"text_food_not_food_label\"] == \"not_food\" else 1\n  data_dict_list.append({item_label: item_text})\n\nimport random\nrandom.sample(data_dict_list, k=5)\n\n[{1: 'a bottle of Miracle Whip'},\n {1: 'Croquetas de Pollo Recipe (Chicken Croquettes): very popular as party food and street food, also common at Dominican Christmas dinner tables.'},\n {0: 'How to Build a Radiator Cover This easy DIY tutorial shows you how to make a radiator cover to cover those unsightly or unused radiators you might have in your home. Diy Radiator Cover, Radiator Screen, Radiator Ideas, Home Radiators, Baseboard Heater Covers, Diy Heater, Modern Birdhouses, Small Bedroom Storage, Small Bedrooms'},\n {0: 'Anthony and Olivia&apos;s wedding in Vancouver, British Columbia 7'},\n {0: 'Nissan Micra 2018 - Image #5'}]\ndata_dict = {col_name: df_subset[col_name].tolist() for col_name in [\"text\", \"text_food_not_food_label\"]}\n\ndataset = Dataset.from_dict(data_dict)\ndataset\n\nDataset({\n    features: ['text', 'text_food_not_food_label'],\n    num_rows: 152322\n})\n# Rename column, see; https://huggingface.co/docs/datasets/en/process#rename\ndataset = dataset.rename_column(\"text_food_not_food_label\", \"label\")\ndataset\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 152322\n})\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\"), see: https://huggingface.co/docs/datasets/en/process#map\ndef map_labels_to_number(example):\n  example[\"label\"] = 0 if example[\"label\"] == \"not_food\" else 1\n  return example\n\ndataset = dataset.map(map_labels_to_number)\ndataset[:5]\n\n\n\n\n{'text': ['The expression of copra farmers',\n  'Super easy cream biscuits made with just TWO ingredients from TheSuburbanSoapbox.com.',\n  'No Gluten Pizza - The Perfect Vegan Pie For All Occasions',\n  \":::FROSTY'S HOT COCO BAR SIGN:::\",\n  'Zucchini Escabeche @ A Moveable Feast Catering'],\n 'label': [1, 1, 1, 1, 1]}\ndataset.shuffle()[:5]\n\n{'text': [\"Beatles Sgt Pepper Apple Label Stereo Analog '69 3rd Press VG++ ULTRASONIC Clean\",\n  'GMC Savana Commercial Cutaway 3500 ** Cube 12 pieds Deck ** RAMPE ** COMME NEUF 2020',\n  'Image no. 1 for Solute Carrier Family 25 (Mitochondrial Carrier, Adenine Nucleotide Translocator), Member 4 (SLC25A4) ELISA Kit (ABIN5524005',\n  'BAPE x Coach SS20 Collection Revealed',\n  'Composition XIV by Conrad Marca-Relli at'],\n 'label': [0, 0, 0, 0, 0]}\n# Create train/test splits, see: https://huggingface.co/docs/datasets/en/process#split\ndataset = dataset.train_test_split(test_size=0.2)\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 121857\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 30465\n    })\n})\ndataset[\"train\"][0], dataset[\"test\"][0]\n\n({'text': 'Follow Your Dreams Floral LastingLite Sleeve', 'label': 0},\n {'text': 'Blue & Red Basketball TMAC Millennium Boost Tracy McGrady Sneakers',\n  'label': 0})",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#preprocess",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#preprocess",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 Preprocess",
    "text": "1 Preprocess\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#preprocess\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n  return tokenizer(examples[\"text\"], truncation=True)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\ntokenized_dataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 121857\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 30465\n    })\n})\n\n\n\ntokenized_dataset[\"train\"][0], tokenized_dataset[\"test\"][0]\n\n({'text': 'Follow Your Dreams Floral LastingLite Sleeve',\n  'label': 0,\n  'input_ids': [101, 3582, 2115, 5544, 18686, 9879, 22779, 10353, 102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'Blue & Red Basketball TMAC Millennium Boost Tracy McGrady Sneakers',\n  'label': 0,\n  'input_ids': [101,\n   2630,\n   1004,\n   2417,\n   3455,\n   1056,\n   22911,\n   10144,\n   12992,\n   10555,\n   11338,\n   16307,\n   2100,\n   28130,\n   102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n\n\n# Collate examples and pad them each batch\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ndata_collator\n\nDataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 Evaluation",
    "text": "2 Evaluation\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 Train",
    "text": "3 Train\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\n3 steps for training:\n\nDefine model\nDefine training arguments\nPass training arguments to Trainer\nCall train()\n\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\n\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id\n)\n\n\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"food_not_food_text_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    # push_to_hub=True\n)\n\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n\n# Batch size 16\n#  [ 391/15234 00:22 &lt; 14:27, 17.12 it/s, Epoch 0.05/2]\n\n# Batch size 32\n# [ 724/7618 01:08 &lt; 10:51, 10.58 it/s, Epoch 0.19/2]\n\n# Batch size 64\n#  [ 150/3810 00:31 &lt; 12:52, 4.74 it/s, Epoch 0.08/2]\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [7618/7618 13:51, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.111100\n0.095816\n0.963466\n\n\n2\n0.053100\n0.123158\n0.964484\n\n\n\n\n\n\nTrainOutput(global_step=7618, training_loss=0.08334020205012267, metrics={'train_runtime': 831.381, 'train_samples_per_second': 293.144, 'train_steps_per_second': 9.163, 'total_flos': 4317340976716644.0, 'train_loss': 0.08334020205012267, 'epoch': 2.0})\n\n\n\n# Optional: push the model to Hugging Face Hub for re-use later\n# Note: Requires Hugging Face login\n# trainer.push_to_hub()\n\n\n# Save model\n# See: https://discuss.huggingface.co/t/how-to-save-my-model-to-use-it-later/20568/4\ntrainer.save_model(\"food_not_food_text_model\")",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#inference",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#inference",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 Inference",
    "text": "4 Inference\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nsample_text = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n\n4.1 Pipeline mode\n\nfrom transformers import pipeline\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", model=\"./food_not_food_text_model\")\nfood_not_food_classifier(sample_text)\n\n[{'label': 'food', 'score': 0.9997596144676208}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.999407172203064}]\n\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"I'm feeling overwhelmed by this workload ‚Äì it's a real information buffet.\",\n    \"We're brainstorming new content ideas, hoping to cook up something innovative.\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9527214169502258},\n {'label': 'not_food', 'score': 0.952194094657898},\n {'label': 'not_food', 'score': 0.8402552008628845},\n {'label': 'food', 'score': 0.9794776439666748},\n {'label': 'not_food', 'score': 0.994716465473175},\n {'label': 'food', 'score': 0.6270619034767151},\n {'label': 'not_food', 'score': 0.9927234053611755},\n {'label': 'food', 'score': 0.779589831829071},\n {'label': 'not_food', 'score': 0.7383647561073303},\n {'label': 'food', 'score': 0.7064051628112793}]\n\n\n\n\n4.2 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"food_not_food_text_model\")\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"food_not_food_text_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nTODO:"
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]