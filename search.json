[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nTeaching style: A machine learning cooking show!\nMottos:\nTODO:"
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "extras/setup.html",
    "href": "extras/setup.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "We‚Äôll need to install the following libraries from the Hugging Face ecosystem:"
  },
  {
    "objectID": "extras/setup.html#tk---add-a-getting-started-section-for-mac",
    "href": "extras/setup.html#tk---add-a-getting-started-section-for-mac",
    "title": "Learn Hugging Face ü§ó",
    "section": "TK - Add a getting started section for Mac",
    "text": "TK - Add a getting started section for Mac\nI could make this into a YouTube video for running on your own Mac.\nTK - Getting setup for Hugging Face Transformers on a Mac."
  },
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\nTokenization = turning a series of data (text or image) into a series of tokens, where a token is a numerical representation of the input data, for example, in the case of text, tokenization could mean turning the words in a sentence into numbers (e.g.¬†‚Äúhello world‚Äù -&gt; [101, 102])\nTokens = a token is a letter, word or word-piece (word) that a model uses to represent input data, for example, in the case of text, a token could be a word (e.g.¬†‚Äúhello‚Äù) or a word-piece (e.g.¬†‚Äúhell‚Äù and ‚Äúo‚Äù), see: https://platform.openai.com/tokenizer for an example\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\nevaluate = A Python library by Hugging Face with premade evaluation functions for various tasks\ntorch = PyTorch, an open-source machine learning library\ntransfer learning = taking what one model has learned and applying it to another task (e.g.¬†a model which has learned across many millions of words of text from the internet and then adjusting it to work with your smaller dataset)\nfine-tuning = a type of transfer learning where you take the existing patterns of one model (usually trained on a very large dataset) and customize them to work for your smaller dataset\nhyperparameters = values you can set to adjust training settings, for example, learning rate is a hyperparameter that is adjustable\nHugging Face Hub = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared\nHugging Face Spaces = A platform to share and run machine learning apps/demos, usually built with Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit\nLearning rate = Often the most important hyperparameter to tune. It is proportional with the amount an optimizer will update a model‚Äôs parameters every update step. A higher amount means larger updates (though sometimes too large) a lower amount means smaller updates (though sometimes not enough). The most ideal learning rate is experimental. Common values include 0.001, 0.0001, 0.0005, 0.00001, 0.00005 (though the learning rate can be any value). Many optimizers have decent default learning rates. For example, the Adam optimizer (a common and generally well performing optimizer) in PyTorch (torch.optim.Adam) has a default learning rate of 0.001. For fine-tuning an already trained model a learning rate of 10x smaller than the default is a good rule of thumb (e.g.¬†if a model was trained with a learning rate of 0.001, fine-tuning with 0.0001 is common). The learning rate does not have to be static and can change dynamcially during training, this practice is referred to as learning rate scheduling.\nPrediction probability = the probability of a model‚Äôs prediction for a given input, is a score between 0 and 1 with 1 being the highest, for example, a model may have a prediction probability of 0.95, this would mean it‚Äôs quite confident with its prediction but it doesn‚Äôt mean it‚Äôs correct. A good way to inspect potential issues with a dataset is to show examples in the test set which have a high prediction probability but are wrong (e.g.¬†pred prob = 0.98 but the prediction was incorrect).\nHugging Face Pipeline (pipeline) = A high-level API for using model for various tasks (e.g.¬†text-classification, audio-classification, image-classification, object-detection and more), see the docs: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/pipelines#transformers.pipeline"
  },
  {
    "objectID": "extras/todo.html",
    "href": "extras/todo.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Add ‚Äúgetting setup‚Äù file to get started locally with the required dependencies\nAdd text classification dataset creation notebook (so people can see where the data comes from)\nAdd a Hugging Face ecosystem overview (transformers, datasets, tokenizers, torch, Hugging Face Hub, Hugging Face Spaces, etc.)\nMake code-only versions of notebooks? e.g.¬†text stripped away and only code is left"
  },
  {
    "objectID": "extras/todo.html#quarto-misc",
    "href": "extras/todo.html#quarto-misc",
    "title": "Learn Hugging Face ü§ó",
    "section": "Quarto misc",
    "text": "Quarto misc\n\nCreate share cards - https://quarto.org/docs/websites/website-tools.html#twitter-cards\n\nSee here for share image - https://quarto.org/docs/websites/website-tools.html#preview-images"
  },
  {
    "objectID": "extras/resources.html",
    "href": "extras/resources.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "See the Pytorch extra resources for some ideas: https://www.learnpytorch.io/pytorch_extra_resources/"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Next:\n# Add tools used in this overview (e.g. overview of the project)\n# Create a small dataset with text generation, e.g. 50x spam/not_spam emails and train a classifier on it ‚úÖ\n   # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n# Save the dataset to Hugging Face Datasets ‚úÖ\n   # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n# Train a classifier on it ‚úÖ\n# Save the model to the Hugging Face Model Hub ‚úÖ\n# Create a with Gradio and test the model in the wild ‚úÖ",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 TK - Overview",
    "text": "1 TK - Overview\n\n1.1 TK - What we‚Äôre going to build\nIn this project, we‚Äôre going to learn various aspects of the Hugging Face ecosystem whilst building a text classification model.\nTo keep things as practical as possible, we‚Äôre going to be bulding a food/not_food text classification model.\nGiven a piece of a text, our model will be able to predict if it‚Äôs about food or not.\nThis is the same kind of model I use in my own work on Nutrify (an app to help people learn about food).\nMore specifically, we‚Äôre going to follow the following steps:\n\nProblem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nFinding, training and evaluating a model - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\nCreating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others.\nTK image - see the finished product (demo)\n\n\n\n\n\n\nNote\n\n\n\nNote this is a hands-on project, so we‚Äôll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we‚Äôre doing, I‚Äôll leave links in the extra-curriculum section.\n\n\n\n\n1.2 TK - What is Hugging Face?\nTK - perhaps put this in the front of the website? e.g.¬†on the index page\nHugging Face is a platform that offers access to many different kinds of open-source machine learning models and datasets.\nThey‚Äôre also the creators of the popular transformers library which is a Python-based library for working with pre-trained models as well as custom models and datasets.\nIf you‚Äôre getting into the world of AI and machine learning, you‚Äôre going to come across Hugging Face.\n\n\n1.3 TK - Why Hugging Face?\nMany of the biggest companies in the world use Hugging Face for their open-source machine learning projects including Apple, Google, Facebook (Meta), Microsoft, OpenAI, ByteDance and more.\nTK image - image of people using Hugging Face\nNot only does Hugging Face make it so you can use state-of-the-art machine learning models such as Stable Diffusion (for image generation) and Whipser (for audio transcription) easily, it also makes it so you can share your own models, datasets and resources.\nConsider Hugging Face the homepage of your AI/machine learning profile.\n\n\n1.4 TK - What is text classification?\nText classification is the process of assigning a category to a piece of text.\nWhere a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\nTK image - example of text classification\nExample text classification problems include:\n\n\n\n\n\n\n\n\nProblem\nDescription\nProblem Type\n\n\n\n\nSpam email detection\nIs an email spam or not spam?\nBinary classification (one thing or another)\n\n\nSentiment analysis\nIs a piece of text positive, negative or neutral?\nMulti-class classification (one thing from many)\n\n\nLanguage detection\nWhat language is a piece of text written in?\nMulti-class classification (one thing from many)\n\n\nTopic classification\nWhat topic(s) does a news article belong to?\nMulti-label classification (one or more things from many)\n\n\nHate speech detection\nIs a comment hateful or not hateful?\nBinary classification (one thing or another)\n\n\nProduct categorization\nWhat categories does a product belong to?\nMulti-label classification (one or more things from many)\n\n\n\nThere are several different kinds of models you can use for text classification.\nAnd each will have its pros and cons depending on the problem you‚Äôre working on.\nExample text classification models include:\n\n\n\nModel\nDescription\nPros\nCons\n\n\n\n\nRule-based\nUses a set of rules to classify text (e.g.¬†if text contains ‚Äúsad‚Äù -&gt; sentiment = low)\nSimple, easy to understand\nRequires manual creation of rules\n\n\nBag of Words\nCounts the frequency of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nTF-IDF\nWeighs the importance of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nDeep learning-based models\nUses neural networks to learn patterns in text\nCan learn complex patterns at scale\nCan require large amounts of data/compute power to run, not as easy to understand (can be hard to debug)\n\n\n\nWe‚Äôre going to use a deep learning model our case.\nWhy?\nBecause Hugging Face helps us do so.\nAnd in most cases, with a large enough dataset, a deep learning model will often perform better than a rule-based or other model.\n\n\n1.5 TK - Why train your own text classification models?\nYou can use pre-trained models for text classification as well as API-powered models and LLMs such as GPT-4 or Gemini.\nHowever, it‚Äôs often a good idea to train your own text classification models for a few reasons:\n\nThey can be much faster than API-powered models (since they‚Äôre running on your own hardware, this can save on costs and time).\nThey‚Äôre customized to your own data.\nThey don‚Äôt require you to send your data elsewhere (privacy).\nIf a service goes down, you‚Äôll still have access to your model (reliability).\n\nTK image - example of training your own model vs using an API-powered model",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 TK - Importing necessary libraries",
    "text": "2 TK - Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide (tk - link to getting setup guide) to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport transformers\n\nfrom datasets import Dataset\n\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\n\nUsing transformers version: 4.41.2\nUsing datasets version: 2.19.1\nUsing torch version: 2.2.0+cu121\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 TK - Getting a dataset",
    "text": "3 TK - Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor a text classificaiton problem, your dataset will likely come in the form of text (e.g.¬†a paragraph, sentence or phrase) and a label (e.g.¬†what category the text belongs to).\n\nTK image - showcase what a supervised dataset looks like (e.g.¬†text and label, this can be the dataset we‚Äôve got on Hugging Face hub, showcase the different parts of the dataset as well including the name etc)\n\nIn our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\nThis is a dataset I‚Äôve created earlier to help us practice building a text classification model.\nYou can find it on Hugging Face under the name mrdbourke/learn_hf_food_not_food_image_captions.\n\n\n\n\n\n\nResource\n\n\n\nSee how the food/not_food image caption dataset was created in the (TK - add notebook link and title, make this available on the website)\n\nTK - see dataset creation:\n\nDone, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing\nDone, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n\n\n\n\n\n3.1 Where can you get more datasets?\nThe are many different places you can get datasets for text-based problems.\nOne of the best places is on the Hugging Face Hub, specifically huggingface.co/datasets.\nHere you can find many different kinds of problem specific data such as text classification.\nTK image - show example image of text classification datasets\n\n\n3.2 Loading the dataset\nOnce we‚Äôve found/prepared a dataset on the Hugging Face Hub, we can use the datasets library to load it.\nTo load a dataset we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/learn_hf_food_not_food_image_captions.\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n# Load the dataset from Hugging Face Hub\ndataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n\n# Inspect the dataset\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 250\n    })\n})\n\n\nDataset loaded!\nLooks like our dataset has two features, text and label.\nAnd 250 total rows (the number of examples in our dataset).\nWe can check the column names with dataset.column_names.\n\n# What features are there?\ndataset.column_names\n\n{'train': ['text', 'label']}\n\n\nLooks like our dataset comes with a train split already (the whole dataset).\nWe can access the train split with dataset[\"train\"] (some datasets also come with built-in \"test\" splits too).\n\n# Access the training split\ndataset[\"train\"]\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 250\n})\n\n\nHow about we check out a single sample?\nWe can do so with indexing.\n\ndataset[\"train\"][0]\n\n{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n 'label': 'food'}\n\n\nNice! We get back a dictionary with the keys text and label.\nThe text key contains the text of the image caption and the label key contains the label (food or not food).\n\n\n3.3 TK - Inspect random examples from the dataset\nAt 250 total samples, our dataset isn‚Äôt too large.\nSo we could sit there and explore the samples one by one.\nBut whenever I interact with a new dataset, I like to view a bunch of random examples and get a feel of the data.\nDoing so is inline with the data explorer‚Äôs motto: visualize, visualize, visualize!\nAs a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\nLet‚Äôs write some code to view 5 random indexes of our data and their corresponding text and labels at a time.\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: Luxurious coconut shrimp curry on a generous plate, featuring succulent shrimp in a rich coconut milk sauce, served with jasmine rice. | Label: food\nText: Set of golf clubs stored in a bag | Label: not_food\nText: A bowl of sliced pears with a sprinkle of ginger and a side of honey | Label: food\nText: Celery in a bowl, served with a side of peanut butter and a sprinkle of raisins for a classic, tasty snack. | Label: food\nText: Set of forks kept in a holder | Label: not_food\n\n\nBeautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\nWe can get the unique labels in our dataset with dataset[\"train\"].unique(\"label\").\n\n# Get unique label values\ndataset[\"train\"].unique(\"label\")\n\n['food', 'not_food']\n\n\nIf our dataset is small enough to fit into memory, we can count the number of different labels with Python‚Äôs collections.Counter (a method for counting objects in an iterable or mapping).\n\n# Check number of each label\nfrom collections import Counter\n\nCounter(dataset[\"train\"][\"label\"])\n\nCounter({'food': 125, 'not_food': 125})\n\n\nExcellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\nIn a binary classification case, this is ideal.\nIf the classes were dramatically unbalanced (e.g.¬†90% food and 10% not food) we might have to consider collecting/creating more data.\nBut best to train a model and see how it goes before making any drastic dataset changes.\nBecause our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets).\n\n# Turn our dataset into a DataFrame and get a random sample\nfood_not_food_df = pd.DataFrame(dataset[\"train\"])\nfood_not_food_df.sample(7)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n249\nTaking a nap on a hammock, a man has his dog s...\nnot_food\n\n\n7\nSet of muffin tins stacked together\nnot_food\n\n\n170\nColorful area rug brightening up a living room\nnot_food\n\n\n40\nWall clock ticking away in a living room\nnot_food\n\n\n188\nA basket of fresh strawberries with a sprinkle...\nfood\n\n\n154\nGluten-free sushi roll using tamari sauce inst...\nfood\n\n\n0\nCreamy cauliflower curry with garlic naan, fea...\nfood\n\n\n\n\n\n\n\n\n# Get the value counts of the label column\nfood_not_food_df[\"label\"].value_counts()\n\nlabel\nfood        125\nnot_food    125\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 TK - Preparing data for text classification",
    "text": "4 TK - Preparing data for text classification\nWe‚Äôve got our data ready but there are a few steps we‚Äôll need to take before we can model it.\nThe main two being:\n\nTokenization - turning our text into a numerical representation (machines prefer numbers rather than words), for example, {\"a\": 0, \"b\": 1, \"c\": 2...}.\nCreating a train/test split - right now our data is in a training split only but we‚Äôll create a test set to evaluate our model‚Äôs performance.\n\nThese don‚Äôt necessarily have to be in order either.\nBefore we get to them, let‚Äôs create a small mapping from our labels to numbers.\nIn the same way we need to tokenize our text into numerical representation, we also need to do the same for our labels.\n\n4.1 TK - Creating a mapping from labels to numbers\nOur machine learning model will want to see all numbers.\nThis goes for text as well as label input.\nSo let‚Äôs create a mapping from our labels to numbers.\nSince we‚Äôve only got a couple of labels (\"food\" and \"not_food\"), we can create a dictionary to map them to numbers, however, if you‚Äôve got a fair few labels, you may want to make this mapping programmatically.\nWe can use these dictionaries later on for our model training as well as evaluation.\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a binary classification task, the positive class, in our case \"food\", is usually given the label 1 and the negative class (\"not_food\") is given the label 0.\n\n\n\n# Create mappings programmatically from dataset\nid2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\nlabel2id = {label: idx for idx, label in id2label.items()}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\nWith our dictionary mappings created, we can update the labels of our dataset to be numeric.\nWe can do this using the datasets.Dataset.map method and passing it a function to apply to each example.\nLet‚Äôs create a small function which turns an example label into a number.\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\nexample_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n\n# Test the function\nmap_labels_to_number(example_sample)\n\n{'text': 'This is a sentence about my favourite food: honey.', 'label': 1}\n\n\nLooks like our function works!\nHow about we map it to the whole dataset?\n\n# Map our dataset labels to numbers\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\nNice! Looks like our labels are all numerical now.\nWe can check a few random samples using dataset.shuffle() and indexing for the first few.\n\n# Shuffle the dataset and view the first 5 samples (will return different results each time) \ndataset.shuffle()[:5]\n\n{'text': ['Treadmill available in a home gym',\n  'Potatoes, onions, garlic, cauliflower, and broccolini in boxes at the market, ready for a tasty, healthy meal.',\n  'A slice of pizza with a generous amount of shredded parmesan cheese on top',\n  'A bowl of sliced bell peppers with a sprinkle of paprika and a side of hummus',\n  'Set of forks kept in a holder'],\n 'label': [0, 1, 1, 1, 0]}\n\n\n\n\n4.2 TK - Split the dataset into training and test sets\nRight now our dataset only has a training split.\nHowever, we‚Äôd like to create a test split so we can evaluate our model.\nIn essence, our model will learn patterns (the relationship between text captions and their labels of food/not_food) on the training data.\nAnd we will evaluate those learned patterns on the test data.\nWe can split our data using the datasets.Dataset.train_test_split method.\nWe can use the test_size parameter to define the percentage of data we‚Äôd like to use in our test set (e.g.¬†test_size=0.2 would mean 20% of the data goes to the test set).\n\n# Create train/test splits\ndataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\n\n\nPerfect!\nOur dataset has been split into 200 training examples and 50 testing examples.\nLet‚Äôs visualize a few random examples to make sure they still look okay.\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: A couple enjoying a movie night on the couch with their pets snuggled close\nLabel: 0 (not_food)\n\n[INFO] Random sample from testing dataset:\nText: A bowl of sliced kiwi with a sprinkle of sugar and a side of yogurt\nLabel: 1 (food)\n\n\n\n\n4.3 TK - Tokenizing text data\nLabels numericalized, dataset split, time to turn our text into numbers.\nTokenization is the process of converting a non-numerical data source into numbers.\nWhy?\nBecause machines (especially machine learning models) prefer numbers to human-style data.\nIn the case of the text \"I love pizza\" a very simple method of tokenization might be to convert each word to a number.\nFor example, {\"I\": 0, \"love\": 1, \"pizza\": 2}.\nHowever, for most modern machine learning models, the tokenization process is a bit more nuanced.\nFor example, the text \"I love pizza\" might be tokenized into something more like [101, 1045, 2293, 10733, 102].\nTK image - showcase an example using OpenAI‚Äôs tokenization tool and what this looks like with ‚ÄúI love pizza‚Äù: https://platform.openai.com/tokenizer\n\n\n\n\n\n\nNote\n\n\n\nDepending on the model you use, the tokenization process could be different. For example, one model might turn \"I love pizza\" into [40, 3021, 23317], where as another model might turn it into [101, 1045, 2293, 10733, 102].\nTo deal with this, Hugging Face models often pair models with their own tokenizers by pairing a tokenizer configuration with a model‚Äôs weights.\nSuch is the case with distilbert/distilbert-base-uncased (there is a tokenizer.json file as well as a tokenizer_config.json file which contains all of the tokenizer implementation details).\nFor more examples of tokenization, you can see OpenAI‚Äôs tokenization visualizer tool as well as their open-source library tiktoken, Google also have an open-source tokenization library called sentencepiece, finally Hugging Face‚Äôs tokenizers library is also a great resource (this is what we‚Äôll be using behind the scenes).\n\n\nMany of the text-based models on Hugging Face come paired with their own tokenizer.\nFor example, the distilbert/distilbert-base-uncased model can be used with the distilbert/distilbert-base-uncased tokenizer.\nWe can load the tokenizer for a given model using the transformers.AutoTokenizer.from_pretrained method and passing it the name of the model we‚Äôd like to use.\nThe transformers.AutoTokenizer class is part of a series of Auto Classes (such as AutoConfig, AutoModel, AutoProcessor) which automatically loads the correct configuration settings for a given model.\nLet‚Äôs load the tokenizer for the distilbert/distilbert-base-uncased model and see how it works.\nUPTOHERE - ‚Äúwhy this model?‚Äù, add reasoning to why the distilbert model, Hugging Face has many models, often it takes a bit of practice to see which is best to use\n\n\n\n\n\n\nNote\n\n\n\nWhy use the distilbert/distilbert-base-uncased model?\nThe short answer is that I‚Äôve used it before and it works well (and fast) on various text classification tasks.\nThe longer answer is that Hugging Face has many available open-source models for many different problems available at https://huggingface.co/models.\nNavigating these models can take some practice.\nAnd several models may be suited for the same task (though with various tradeoffs such as size and speed).\nHowever, overtime and with adequate experimentation, you‚Äôll start to build an intuition on which models are good for which problems.\n\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n\ntokenizer\n\nDistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\nNice!\nThere‚Äôs our tokenizer!\nIt‚Äôs an instance of the transformers.DistilBertTokenizerFast class.\nYou can read more about it in the documentation.\nFor now, let‚Äôs try it out by passing it a string of text.\n\n# Test out tokenizer\ntokenizer(\"I love pizza\")\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\n\n# Try adding a \"!\" at the end\ntokenizer(\"I love pizza!\")\n\n{'input_ids': [101, 1045, 2293, 10733, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n\n\nWoohoo!\nOur text gets turned into numbers (or tokens).\nNotice how with even a slight change in the text, the tokenizer produces different results?\nThe input_ids are our tokens.\nAnd the attention_mask (in our case, all [1, 1, 1, 1, 1, 1]) is a mask which tells the model which tokens to use or not. Tokens with a mask value of 1 get used and tokens with a mask value of 0 get ignored.\nThere are several attributes of the tokenizer we can explore.\n\ntokenizer.vocab will return the vocabulary of the tokenizer or in other words, the unique words/word pieces the tokenizer is capable of converting into numbers.\ntokenizer.model_max_length will return the maximum length of a sequence the tokenizer can process, pass anything longer than this and the sequence will be truncated.\n\n\n# Get the length of the vocabulary \nlength_of_tokenizer_vocab = len(tokenizer.vocab)\nprint(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n\n# Get the maximum sequence length the tokenizer can handle\nmax_tokenizer_input_sequence_length = tokenizer.model_max_length\nprint(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")\n\nLength of tokenizer vocabulary: 30522\nMax tokenizer input sequence length: 512\n\n\nWoah, looks like our tokenizer has a vocabulary of 30,522 different words and word pieces.\nAnd it can handle a sequence length of up to 512 (any sequence longer than this will be automatically truncated from the end).\nLet‚Äôs check out some of the vocab.\nCan I find my own name?\n\n# Does \"daniel\" occur in the vocab?\ntokenizer.vocab[\"daniel\"]\n\n3817\n\n\nOooh, looks like my name is 3817 in the tokenizer‚Äôs vocab.\nCan you find your own name? (note: there may be an error if the token doesn‚Äôt exist, we‚Äôll get to this)\nHow about ‚Äúpizza‚Äù?\n\ntokenizer.vocab[\"pizza\"]\n\n10733\n\n\nWhat if a word doesn‚Äôt exist in the vocab?\n\ntokenizer.vocab[\"akash\"]\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 tokenizer.vocab[\"akash\"]\n\nKeyError: 'akash'\n\n\n\nDam, we get a KeyError.\nNot to worry, this is okay, since when calling the tokenizer on the word, it will automatically split the word into word pieces or subwords.\n\ntokenizer(\"akash\")\n\n{'input_ids': [101, 9875, 4095, 102], 'attention_mask': [1, 1, 1, 1]}\n\n\nIt works!\nWe can check what word pieces \"akash\" got broken into with tokenizer.convert_ids_to_tokens(input_ids).\n\ntokenizer.convert_ids_to_tokens(tokenizer(\"akash\").input_ids)\n\n['[CLS]', 'aka', '##sh', '[SEP]']\n\n\nAhhh, it seems \"akash\" was split into two tokens, [\"aka\", \"##sh\"].\nThe \"##\" at the start of \"##sh\" means that the sequence is part of a larger sequence.\nAnd the \"[CLS]\" and \"[SEP]\" tokens are special tokens indicating the start and end of a sequence.\nNow, since tokenizers can deal with any text, what if there was an unknown token?\nFor example, rather than \"pizza\" someone used the pizza emoji üçï?\nLet‚Äôs try!\n\n# Try to tokenize an emoji\ntokenizer.convert_ids_to_tokens(tokenizer(\"üçï\").input_ids)\n\n['[CLS]', '[UNK]', '[SEP]']\n\n\nAhh, we get the special \"[UNK]\" token.\nThis stands for ‚Äúunknown‚Äù.\nThe combination of word pieces and \"[UNK]\" special token means that our tokenizer will be able to turn almost any text into numbers for our model.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that just because one tokenizer uses an unknown special token for a particular word or emoji (üçï) doesn‚Äôt mean another will.\n\n\nSince the tokenizer.vocab is a Python dictionary, we can get a sample of the vocabulary using tokenizer.vocab.items().\nHow about we get the first 5?\n\n# Get the first 5 items in the tokenizer vocab\nsorted(tokenizer.vocab.items())[:5]\n\n[('!', 999), ('\"', 1000), ('#', 1001), ('##!', 29612), ('##\"', 29613)]\n\n\nThere‚Äôs our '!' from before! Looks like the first five items are all related to punctuation points.\nHow about a random sample of tokens?\n\nimport random\n\nrandom.sample(sorted(tokenizer.vocab.items()), k=5)\n\n[('macau', 16878),\n ('##nia', 6200),\n ('resurrection', 15218),\n ('drops', 9010),\n ('organizers', 18829)]\n\n\n\n\n4.4 TK - Making a preprocessing function to tokenize text\nRather than tokenizing our texts one by one, it‚Äôs best practice to define a preprocessing function which does it for us.\nThis process works regardless of whether you‚Äôre working with text data or other kinds of data such as images or audio.\n\n\n\n\n\n\nTurning data into numbers\n\n\n\nFor any kind of machine learning workflow, an important first step is turning your input data into numbers.\nAs machine learning models are algorithms which find patterns in numbers, before they can find patterns in your data (text, images, audio, tables) it must be numerically encoded first (e.g.¬†tokenizing text).\nTo help with this, transformers has an AutoProcessor class which can preprocess data in a specific format required for a paired model.\n\n\nTo prepare our text data, let‚Äôs create a preprocessing function to take in a dictionary which contains the key \"text\" which has the value of a target string (our data samples come in the form of dictionaries) and then returns the tokenized \"text\".\nWe‚Äôll set the following parameters in our tokenizer:\n\npadding=True - This will make all the sequences in a batch the same length by padding shorter sequences with 0‚Äôs until they equal the longest size in the batch. Why? If there are different size sequences in a batch, you can sometimes run into dimensionality issues.\ntruncation=True - This will shorten sequences longer than the model can handle to the model‚Äôs max input size (e.g.¬†if a sequence is 1000 long and the model can handle 512, it will be shortened to 512 via removing all tokens after 512).\n\nYou can see more parameters available for the tokenizer in the transformers.PreTrainedTokenizer documentation.\n\n\n\n\n\n\nNote\n\n\n\nFor more on padding and truncation (two important concepts in sequence processing), I‚Äôd recommend reading the Hugging Face documentation on Padding and Truncation.\n\n\n\ndef tokenize_text(examples):\n    \"\"\"\n    Tokenize given example text and return the tokenized text.\n    \"\"\"\n    return tokenizer(examples[\"text\"],\n                     padding=True, # pad short sequences to longest sequence in the batch\n                     truncation=True) # truncate long sequences to the maximum length the model can handle\n\nWonderful!\nNow let‚Äôs try it out on an example sample.\n\nexample_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n\n# Test the function\ntokenize_text(example_sample_2)\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\nLooking good!\nHow about we map our tokenize_text function to our whole dataset?\nWe can do so with the datasets.Dataset.map method.\nThe map method allows us to apply a given function to all examples in a dataset.\nBy setting batched=True we can apply the given function to batches of examples (many at a time) to speed up computation time.\nLet‚Äôs create a tokenized_dataset object by calling map on our dataset and passing it our tokenize_text function.\n\n# dataset.map() docs -&gt; https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.Dataset.map \ntokenized_dataset = dataset.map(function=tokenize_text, \n                                batched=True, # set batched=True to operate across batches of examples rather than only single examples\n                                batch_size=1000) # defaults to 1000, can be increased if you have a large dataset\n\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\nDataset tokenized!\nLet‚Äôs inspect a pair of samples.\n\n# Get two samples from the tokenized dataset\ntrain_tokenized_sample = tokenized_dataset[\"train\"][0]\ntest_tokenized_sample = tokenized_dataset[\"test\"][0]\n\nfor key in train_tokenized_sample.keys():\n    print(f\"[INFO] Key: {key}\")\n    print(f\"Train sample: {train_tokenized_sample[key]}\")\n    print(f\"Test sample: {test_tokenized_sample[key]}\")\n    print(\"\")\n\n[INFO] Key: text\nTrain sample: Set of headphones placed on a desk\nTest sample: A slice of pepperoni pizza with a layer of melted cheese\n\n[INFO] Key: label\nTrain sample: 0\nTest sample: 1\n\n[INFO] Key: input_ids\nTrain sample: [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [101, 1037, 14704, 1997, 11565, 10698, 10733, 2007, 1037, 6741, 1997, 12501, 8808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[INFO] Key: attention_mask\nTrain sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\nBeautiful! Our samples have been tokenized.\nNotice the zeroes on the end of the inpud_ids and attention_mask values.\nThese are padding tokens to ensure that each sample has the same length as the longest sequence in a given batch.\nWe can now use these tokenized samples later on in our model.\n\n\n4.5 Tokenization takeaways\nWe‚Äôve seen tokenizers in practice.\nA few takeaways before we start to build a model:\n\nTokenizers are used to turn text (or other forms of data such as images and audio) into a numerical representation ready to be used with a machine learning model.\nMany models reuse existing tokenizers and many models have their own specific tokenizer paired with them. Hugging Face‚Äôs transformers.AutoTokenizer, transformers.AutoProcessor and transformers.AutoModel classes make it easy to pair tokenizers and models based on their name (e.g.¬†distilbert/distilbert-base-uncased).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---setting-up-an-evaluation-metric",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---setting-up-an-evaluation-metric",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "5 TK - Setting up an evaluation metric",
    "text": "5 TK - Setting up an evaluation metric\nAside from training a model, one of the most important steps in machine learning is evaluating a model.\nTo do, we can use evaluation metrics.\nThere are many different kinds of evaluation metrics for various problems.\nBut since we‚Äôre focused on text classification, we‚Äôll use accuracy as our evaluation metric.\nA model which gets 99/100 predictions correct has an accuracy of 99%.\n\\[\n\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\n\\]\nFor some projects, you may have a minimum standard of a metric.\nFor example, when I worked on an insurance claim classification model, the clients required over 98% accuracy for it to be viable to use in production.\nWe can craft these evaluation metrics ourselves.\nHowever, Hugging Face has a library called evaluate which has various metrics built in ready to use.\nWe can load a metric using evaluate.load(\"METRIC_NAME\").\nLet‚Äôs load in \"accuracy\" and build a function to measure accuracy by comparing arrays of predictions and labels.\n\nimport evaluate\nimport numpy as np\nfrom typing import Tuple\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n  \"\"\"\n  Computes the accuracy of a model by comparing the predictions and labels.\n  \"\"\"\n  predictions, labels = predictions_and_labels\n\n  # Get highest prediction probability of each prediction if predictions are probabilities\n  if len(predictions.shape) &gt;= 2:\n    predictions = np.argmax(predictions, axis=1)\n\n  return accuracy_metric.compute(predictions=predictions, references=labels)\n\nAccuracy function created!\nNow let‚Äôs test it out.\n\n# Create example list of predictions and labels\nexample_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nexample_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\nexample_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# Test the function\nprint(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\nprint(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")\n\nAccuracy when all predictions are correct: {'accuracy': 1.0}\nAccuracy when one prediction is wrong: {'accuracy': 0.9}\n\n\nExcellent, our function works just as we‚Äôd like.\nWhen all predictions are correct, it scores 1.0 (or 100% accuracy) and when 9/10 predictions are correct, it returns 0.9 (or 90% accuracy).\nWe can use this function during training and evaluation of our model.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---setting-up-a-model-for-training",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---setting-up-a-model-for-training",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "6 TK - Setting up a model for training",
    "text": "6 TK - Setting up a model for training\nWe‚Äôve gone through the important steps of setting data up for training (and evaluation).\nNow let‚Äôs prepare a model.\nWe‚Äôll go through the following steps:\n\nCreate and preprocess data (done ‚úÖ).\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model.\nEvaluate results.\n\n\nTK image - steps for training in Hugging Face\n\nLet‚Äôs start by creating an instance of a model.\nSince we‚Äôre working on text classification, we‚Äôll do so with transformers.AutoModelForSequenceClassification (where sequence classification means a sequence of something, e.g.¬†our sequences of text).\nWe can use the from_pretrained() method to instatiate a pretrained model from the Hugging Face Hub.\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúpretrained‚Äù in transformers.AutoModelForSequenceClassification.from_pretrained means acquiring a model which has already been trained on a certain dataset.\nThis is common practice in many machine learning projects and is known as transfer learning.\nThe idea is to take an existing model which works well on a task similar to your target task and then fine-tune it to work even better on your target task.\nIn our case, we‚Äôre going to use the pretrained DistilBERT base model (distilbert/distilbert-base-uncased) which has been trained on many thousands of books as well as a version of the English Wikipedia (millions of words).\nThis training gives it a very good baseline representation of the patterns in language.\nWe‚Äôll take this baseline representation of the patterns in language and adjust it slightly to focus specifically on predicting whether an image caption is about food or not (based on the words it contains).\nThe main two benefits of using transfer learning are:\n\nAbility to get good results with smaller amounts of data (since the main representations are learned on a larger dataset, we only have to show the model a few examples of our specific problem).\nThis process can be repeated acorss various domains and tasks. For example, you can take a computer vision model trained on millions of images and customize it to your own use case. Or an audio model trained on many different nature sounds and customize it specifically for birds.\n\nSo when starting a new machine learning project, one of the first questions you should ask is: does an existing pretrained model similar to my task exist and can I fine-tune it for my own task?\nFor an end-to-end example of transfer learning in PyTorch (another popular deep learning framework), see PyTorch Transfer Learning.\n\n\nTime to setup our model instance.\nA few things to note: * We‚Äôll use transformers.AutoModelForSequenceClassification.from_pretrained, this will create the model architecture we specify with the pretrained_model_name_or_path parameter. * The AutoModelForSequenceClassification class comes with a classification head on top of our mdoel (so we can customize this to the number of classes we have with the num_labels parameter). * Using from_pretrained will also call the transformers.PretrainedConfig class which will enable us to set id2label and label2id parameters for our fine-tuning task.\nLet‚Äôs refresh what our id2label and label2id objects look like.\n\n# Get id and label mappings\nprint(f\"id2label: {id2label}\")\nprint(f\"label2id: {label2id}\")\n\nid2label: {0: 'not_food', 1: 'food'}\nlabel2id: {'not_food': 0, 'food': 1}\n\n\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Setup model for fine-tuning with classification head (top layers of network)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n    label2id=label2id\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel created!\nYou‚Äôll notice that a warning message gets displayed:\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: [‚Äòclassifier.bias‚Äô, ‚Äòclassifier.weight‚Äô, ‚Äòpre_classifier.bias‚Äô, ‚Äòpre_classifier.weight‚Äô] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nThis is essentially saying ‚Äúhey, some of the layers in this model are newly initialized (with random patterns) and you should probably customize them to your own dataset‚Äù.\nThis happens because we used the AutoModelForSequenceClassification class.\nWhilst the majority of the layers in our model have already learned patterns from a large corpus of text, the top layers (classifier layers) have been randomly setup so we can customize them on our own.\nLet‚Äôs try and make a prediction with our model and see what happens.\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][0])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[50], line 2\n      1 # Try and make a prediction with the loaded model (this will error)\n----&gt; 2 model(**tokenized_dataset[\"train\"][0])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\nOh no! We get an error.\nNot to worry, this is only because our model hasn‚Äôt been trained on our own dataset yet.\nLet‚Äôs take a look at the layers in our model.\n\n# Inspect the model \nmodel\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\nTK image - show what it looks like when fine-tuning a model for a specific task, e.g.¬†backbone is pre-trained layers, classification head is swapped out\nYou‚Äôll notice that the model comes in 3 main parts (data flows through these sequentially):\n\nembeddings - This part of the model turns the input tokens into a learned representation. So rather than just a list of integers, the values become a learned representation. This learned representation comes from the base model learning how different words and word pieces relate to eachother thanks to its training data. The size of (30522, 768) means the 30,522 words in the vocabulary are all represented by vectors of size 768 (one word gets represented by 768 numbers, these are often not human interpretable).\ntransformer - This is the main body of the model. There are several TransformerBlock layers stacked on top of each other. These layers attempt to learn a deeper representation of the data going through the model. A thorough breakdown of these layers is beyond the scope of this tutorial, however, for and in-depth guide on Transformer-based models, I‚Äôd recommend reading Transformers from scratch by Peter Bloem, going through Andrej Karpathy‚Äôs lecture on Transformers and their history or reading the original Attention is all you need paper (this is the paper that introduced the Transformer architecture).\nclassifier - This is what is going to take the representation of the data and compress it into our number of target classes (notice out_features=2, this means that we‚Äôll get two output numbers, one for each of our classes).\n\nFor more on the entire DistilBert architecture and its training setup, I‚Äôd recommend reading the DistilBert paper from the Hugging Face team.\nRather than breakdown the model itself, we‚Äôre focused on using it for a particular task (classifying text).\n\n6.1 TK - Counting the parameters of our model\nBefore we move into training, we can get another insight into our model by counting its number of parameters.\nLet‚Äôs create a small function to count the number of trainable (these will update during training) and total parameters in our model.\n\ndef count_params(model):\n    \"\"\"\n    Count the parameters of a PyTorch model.\n    \"\"\"\n    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_parameters = sum(p.numel() for p in model.parameters())\n\n    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n\n# Count the parameters of the model\ncount_params(model)\n\n{'trainable_parameters': 66955010, 'total_parameters': 66955010}\n\n\nNice!\nLooks like our model has a total of 66,955,010 parameters and all of them are trainable.\nA parameter is a numerical value in a model which is capable of being updated to better represent the input data.\nI like to think of them as a small opportunity to learn patterns in the data.\nIf a model has three parameters, it has three small opportunities to learn patterns in the data.\nWhereas, if a model has 60,000,000+ (60M) parameters (like our model), it has 60,000,000+ small opportunities to learn patterns in the data.\nSome models such as Large Language Models (LLMs) like Llama 3 70B have 70,000,000,000+ (70B) parameters (over 1000x our model).\nIn essence, the more parameters a model has, the more opportunities it has to learn (generally).\nMore parameters often results in more capabilities.\nHowever, more parameters also often results in a much larger model size (e.g.¬†multiple gigabytes versus hundreds of megabytes) as well as a much longer compute time (less samples per second).\nFor our use case, a binary text classification task, 60M parameters is more than enough.\n\n\n\n\n\n\nNote\n\n\n\nWhy count the parameters in a model?\nWhile it may be tempting to always go with a model that has the most parameters, there are many considerations to take into account before doing so.\n\nWhat hardware is the model going to run on?\n\nIf you need the model to run on cheap hardware, you‚Äôll likely want a smaller model.\n\nHow fast do you need the model to be?\n\nIf you need 100-1000s of predictions per second, you‚Äôll likely want a smaller model.\n\n‚ÄúI don‚Äôt mind about speed or cost, I just want quality.‚Äù\n\nGo with the biggest model you can.\nHowever, often times you can get really good results by training a small model to do a specific task using quality data than by just always using a large model.\n\n\n\n\n6.2 TK - Create a directory for saving models\nTraining a model can take a while.\nSo we‚Äôll want a place to save our models.\nLet‚Äôs create a directory called \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\" (it‚Äôs a bit verbose and you can change this if you like but I like to be specific).\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n\n6.3 TK - Setting up training arguments with TrainingArguments\nTK - update this table with actual parameters and what they do\nUPTOHERE\n\nTK - transformers.TrainingArguments has many parameters. Too many to explain here. However, the following table discusses the few that we will set. Some are the same as the defaults (this is on purpose as the defaults are often pretty good) some, such as learning_rate are different.\nfor more a comprehensive overview, see docs: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n\n\n\n\nParameter\nExplanation\n\n\n\n\noutput_dir\nName of output directory to save the model and checkpoints to. For example, ‚Äòlearn_hf_food_not_food_text_classifier_model‚Äô.\n\n\nlearning_rate\nValue of the initial learning rate to use during training. Passed to transformers.AdamW. Initial learning rate because the learning rate can be dynamic during training. The ideal learning is experimental in nature.\n\n\nper_device_train_batch_size\nSize of batches to place on target device during training. For example, a batch size of 32 means the model will look at 32 samples at a time. A batch size too large will result in out of memory issues (e.g.¬†your GPU can‚Äôt handle holding a large number of samples in memory at a time).\n\n\nper_device_eval_batch_size\nSize of batches to place on target device during evaluation. Can often be larger than during training because no gradients are being calculated. For example, training batch size could be 32 where as evaluation batch size may be able to be 128 (4x larger). Though these are only esitmates.\n\n\nnum_train_epochs\nNumber of times to pass over the data to try and learn patterns. For example, if num_train_epochs=10, the model will do 10 full passes of the training data.\n\n\nevaluation_strategy\nWhen to evaluate the model on the evaluation data. If evaluation_strategy=\"epoch\", the model will be evaluated every epoch. See the documentation for more options.\n\n\nsave_strategy\nUPTOHERE ‚Äúepoch‚Äù\n\n\nsave_total_limit\nLimit the total amount of save checkpoints (so we don‚Äôt save num_epochs checkpoints); e.g., 3\n\n\nuse_cpu\nSet to False by default, will use CUDA GPU or MPS device if available\n\n\nseed\nSet to 42 by default for reproducibility\n\n\nload_best_model_at_end\nLoad the best model when finished training; e.g., True\n\n\nlogging_strategy\nLog training results every epoch; e.g., ‚Äúepoch‚Äù\n\n\nreport_to\nLog experiments to Weights & Biases/other similar experimenting tracking services (we‚Äôll turn this off for now); e.g., ‚Äúnone‚Äù\n\n\npush_to_hub\nAutomatically upload the model to the Hub (we‚Äôll do this manually later on); e.g., True\n\n\nhub_token\nAdd your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login); e.g., ‚Äúyour_token_here‚Äù\n\n\n\n\nTK - add markdown table of different parameters and what they do (e.g.¬†most of the common ones but add a note that these may want to be changed depending on the problem + there are many more in the docs)\n\nNext:\n\nloading a model\ntrying to make a prediction (failing)\ninspecting the architecture of the model\ncounting the number of parameters (can use this model on smaller devices)\npreparing it for sequence classification\ncreating a save dir\nsetting up TrainingArguments (read the docs)\n\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\n\nTK - What kind of training are we doing? Supervised learning + fine-tuning an existing model\n\n\nfrom transformers import TrainingArguments\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n# TK - exercise: spend 15 minutes reading the TrainingArguments documentation\n# TODO: Turn off Weights & Biases logging? Or add it in?\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir, # TODO: change this path to model save path, e.g. 'learn_hf_food_not_food_text_classifier_model' \n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n    seed=42, # set to 42 by default for reproducibility\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\", # log training results every epoch\n    report_to=\"none\" # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n)\n\n\n\n6.4 TK - Setting up and instance of Trainer\nTK - Note: Trainer is designed to work best with Hugging Face Transformers models. It can work with torch.nn.Module models as long as they work in a similar way to Transformers models.\n\nfrom transformers import Trainer\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\n# TK - exercise: spend 10 minutes reading the Trainer documentation\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    # tokenizer=tokenizer, # Pass tokenizer to the Trainer for dynamic padding (padding as the training happens) (see \"data_collator\" in the Trainer docs)\n    compute_metrics=compute_accuracy\n)\n\n\n\n6.5 TK - Training our text classification model\n\nresults = trainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.615200\n0.450918\n1.000000\n\n\n2\n0.405600\n0.257541\n1.000000\n\n\n3\n0.219900\n0.123121\n1.000000\n\n\n4\n0.108100\n0.062602\n1.000000\n\n\n5\n0.056800\n0.036242\n1.000000\n\n\n6\n0.035900\n0.025235\n1.000000\n\n\n7\n0.026700\n0.019986\n1.000000\n\n\n8\n0.021900\n0.017336\n1.000000\n\n\n9\n0.019400\n0.016042\n1.000000\n\n\n10\n0.018200\n0.015633\n1.000000\n\n\n\n\n\n\n\n\n6.6 TK - Inspect the model results\n\n# TK - go through these\ntotal_train_time = results.metrics[\"train_runtime\"]\ntrain_samples_per_second = results.metrics[\"train_samples_per_second\"]\n\nprint(f\"Total training time: {total_train_time} seconds\")\nprint(f\"Training samples per second: {train_samples_per_second}\")\n\nTotal training time: 6.7168 seconds\nTraining samples per second: 297.761\n\n\n\n# TK - get loss curves\ntrainer_history = trainer.state.log_history[:-1]\ntrainer_training_time = trainer_history[-1]\ntrainer_history[:5]\n\n[{'loss': 0.6152,\n  'grad_norm': 3.3377952575683594,\n  'learning_rate': 1.8e-05,\n  'epoch': 1.0,\n  'step': 7},\n {'eval_loss': 0.45091766119003296,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0113,\n  'eval_samples_per_second': 4423.998,\n  'eval_steps_per_second': 176.96,\n  'epoch': 1.0,\n  'step': 7},\n {'loss': 0.4056,\n  'grad_norm': 2.4789676666259766,\n  'learning_rate': 1.6000000000000003e-05,\n  'epoch': 2.0,\n  'step': 14},\n {'eval_loss': 0.25754112005233765,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0124,\n  'eval_samples_per_second': 4023.931,\n  'eval_steps_per_second': 160.957,\n  'epoch': 2.0,\n  'step': 14},\n {'loss': 0.2199,\n  'grad_norm': 1.6385667324066162,\n  'learning_rate': 1.4e-05,\n  'epoch': 3.0,\n  'step': 21}]\n\n\n\n# Extract training and evaluation metrics\ntrainer_history_training_set = []\ntrainer_history_eval_set = []\n\nfor item in trainer_history[:-1]:\n    item_keys = list(item.keys())\n    if any(\"eval\" in item for item in item_keys):\n        trainer_history_eval_set.append(item)\n    else:\n        trainer_history_training_set.append(item)\n\n\ntrainer_history_training_df = pd.DataFrame(trainer_history_training_set)\ntrainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n\ntrainer_history_training_df\n\n\n\n\n\n\n\n\nloss\ngrad_norm\nlearning_rate\nepoch\nstep\n\n\n\n\n0\n0.6152\n3.337795\n0.000018\n1.0\n7\n\n\n1\n0.4056\n2.478968\n0.000016\n2.0\n14\n\n\n2\n0.2199\n1.638567\n0.000014\n3.0\n21\n\n\n3\n0.1081\n0.902428\n0.000012\n4.0\n28\n\n\n4\n0.0568\n0.546689\n0.000010\n5.0\n35\n\n\n5\n0.0359\n0.347724\n0.000008\n6.0\n42\n\n\n6\n0.0267\n0.309794\n0.000006\n7.0\n49\n\n\n7\n0.0219\n0.273363\n0.000004\n8.0\n56\n\n\n8\n0.0194\n0.244860\n0.000002\n9.0\n63\n\n\n9\n0.0182\n0.245236\n0.000000\n10.0\n70\n\n\n\n\n\n\n\n\ntrainer_history_eval_df\n\n\n\n\n\n\n\n\neval_loss\neval_accuracy\neval_runtime\neval_samples_per_second\neval_steps_per_second\nepoch\nstep\n\n\n\n\n0\n0.450918\n1.0\n0.0113\n4423.998\n176.960\n1.0\n7\n\n\n1\n0.257541\n1.0\n0.0124\n4023.931\n160.957\n2.0\n14\n\n\n2\n0.123121\n1.0\n0.0115\n4338.068\n173.523\n3.0\n21\n\n\n3\n0.062602\n1.0\n0.0115\n4349.855\n173.994\n4.0\n28\n\n\n4\n0.036242\n1.0\n0.0112\n4448.585\n177.943\n5.0\n35\n\n\n5\n0.025235\n1.0\n0.0122\n4100.485\n164.019\n6.0\n42\n\n\n6\n0.019986\n1.0\n0.0116\n4327.147\n173.086\n7.0\n49\n\n\n7\n0.017336\n1.0\n0.0113\n4406.522\n176.261\n8.0\n56\n\n\n8\n0.016042\n1.0\n0.0116\n4315.128\n172.605\n9.0\n63\n\n\n\n\n\n\n\n\n# Plot training and evaluation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\nplt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and evaluation loss over time\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.7 TK - Save the model for later use\n\n# Save model\n# See docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.save_model \ntrainer.save_model(model_save_dir)\n\n\n\n6.8 TK - Push the model to Hugging Face Hub\nTK - optional to share the model/use elsewhere\n\nsee here: https://huggingface.co/docs/transformers/en/model_sharing\nalso see here for how to setup huggingface-cli so you can write your model to your account\nTK - can use create_model_card on Trainer to create a model card that saves a model with information about how it was trained -&gt; https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.create_model_card\n\n\n# TK - have a note here for the errors\n# Note: you may see the following error\n# 403 Forbidden: You don't have the rights to create a model under the namespace \"mrdbourke\".\n# Cannot access content at: https://huggingface.co/api/repos/create.\n# If you are trying to create or update content,make sure you have a token with the `write` role.\n\n\n# TK - Push model to hub (for later re-use)\n# TODO: Push this model to the hub to be able to use it later\n# TK - this requires a \"write\" token from the Hugging Face Hub\n# TK - see docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub \n# TK - for example, on my local computer, my token is saved to: \"/home/daniel/.cache/huggingface/token\"\n\n# TK - Can create a model card with create_model_card()\n# see here: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.Trainer.create_model_card \n\ntrainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\" # set to False if you want the model to be public\n    # token=\"YOUR_HF_TOKEN_HERE\" # note: this will default to the token you have saved in your Hugging Face config\n)\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/7c9a4a6b17da981559f484538d51f6ff9a14c12d', commit_message='Uploading food not food text classifier model', commit_description='', oid='7c9a4a6b17da981559f484538d51f6ff9a14c12d', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nTK - note: this will make the model public, to make it private,\n\nSee the model here saved for later: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\n6.9 TK - Make and evaluate predictions on the test set\n\n# Perform predictions on the test set\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_metrics = predictions_all.metrics\nprediction_metrics\n\n\n\n\n{'test_loss': 0.015632618218660355,\n 'test_accuracy': 1.0,\n 'test_runtime': 0.0391,\n 'test_samples_per_second': 1280.07,\n 'test_steps_per_second': 51.203}\n\n\n\npredictions_all\n\nPredictionOutput(predictions=array([[-2.261428 ,  1.890655 ],\n       [ 1.8613493, -1.8532594],\n       [-2.2970695,  1.9171791],\n       [ 2.187019 , -2.1593657],\n       [ 2.1193414, -2.1615388],\n       [-2.2868803,  1.9454829],\n       [ 2.0827348, -2.1099336],\n       [ 2.154141 , -2.1266923],\n       [-2.279855 ,  1.9362432],\n       [-2.277952 ,  1.9518106],\n       [-2.2772808,  1.9423369],\n       [-1.9777709,  1.5732591],\n       [ 2.1512635, -2.0508409],\n       [-2.3032587,  1.9534686],\n       [-2.138177 ,  1.7531359],\n       [ 2.194142 , -2.1277084],\n       [-2.2709608,  1.9498663],\n       [ 1.9596925, -1.919577 ],\n       [-2.2827635,  1.9249418],\n       [-2.290854 ,  1.9592198],\n       [-2.2823153,  1.8799024],\n       [-2.3003585,  1.9387653],\n       [ 2.043029 , -2.0384376],\n       [ 2.0885575, -2.1244206],\n       [-2.2873669,  1.9443382],\n       [-2.2972584,  1.9009027],\n       [-2.2450745,  1.8596792],\n       [ 2.1050394, -2.040059 ],\n       [-2.2972147,  1.8946056],\n       [ 2.130832 , -2.133735 ],\n       [-2.2846339,  1.9422101],\n       [-2.2931519,  1.9279182],\n       [-2.3040657,  1.9485677],\n       [ 2.1816792, -2.141174 ],\n       [-2.3019922,  1.9271733],\n       [-2.2885954,  1.9124153],\n       [-2.2813184,  1.9542999],\n       [-2.304743 ,  1.8892938],\n       [ 2.1249578, -2.089177 ],\n       [ 2.043159 , -1.941504 ],\n       [-2.1469579,  1.8099191],\n       [-2.269732 ,  1.9235427],\n       [-2.0776005,  1.7352381],\n       [ 1.9634217, -2.0820174],\n       [-2.2788396,  1.9341636],\n       [-2.2946444,  1.9408271],\n       [-2.2920046,  1.9059081],\n       [-2.3030152,  1.9264866],\n       [ 2.1768198, -2.1458352],\n       [-2.301217 ,  1.9053475]], dtype=float32), label_ids=array([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 1, 0, 1]), metrics={'test_loss': 0.015632618218660355, 'test_accuracy': 1.0, 'test_runtime': 0.0391, 'test_samples_per_second': 1280.07, 'test_steps_per_second': 51.203})\n\n\n\npredictions_all._asdict().keys()\n\ndict_keys(['predictions', 'label_ids', 'metrics'])\n\n\n\nimport torch\npred_probs = torch.softmax(torch.tensor(predictions_all.predictions), dim=1)\npred_labels = np.argmax(predictions_all.predictions, axis=1)\ntrue_labels = dataset[\"test\"][\"label\"]\n\n# Calculate accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(true_labels, pred_labels)\naccuracy\n\n1.0\n\n\n\n# Make a DataFrame of test predictions\ntest_predictions_df = pd.DataFrame({\n    \"text\": dataset[\"test\"][\"text\"],\n    \"true_label\": true_labels,\n    \"pred_label\": pred_labels,\n    \"pred_prob\": torch.max(pred_probs, dim=1).values\n})\n\ntest_predictions_df.head()\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.984512\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.976215\n\n\n2\nA bowl of sliced bell peppers with a sprinkle ...\n1\n1\n0.985432\n\n\n3\nSet of mugs hanging on a hook\n0\n0\n0.987212\n\n\n4\nStanding floor lamp providing light next to an...\n0\n0\n0.986358\n\n\n\n\n\n\n\n\n# Show 10 examples with low prediction probability\n# TK - this is good to find samples where the model is unsure \ntest_predictions_df.sort_values(\"pred_prob\").head(10)\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n11\nA close-up shot of a cheesy pizza slice being ...\n1\n1\n0.972105\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.976215\n\n\n42\nBoxes of apples, pears, pineapple, manadrins a...\n1\n1\n0.978392\n\n\n17\nRelaxing on the porch, a couple enjoys the com...\n0\n0\n0.979753\n\n\n14\nTwo handfuls of bananas in a fruit bowl with g...\n1\n1\n0.979990\n\n\n40\nA bowl of cherries with a sprig of mint for ga...\n1\n1\n0.981236\n\n\n39\nA close-up of a woman practicing yoga in the l...\n0\n0\n0.981741\n\n\n43\nSet of muffin tins stacked together\n0\n0\n0.982799\n\n\n22\nTwo people sitting at a dining room table with...\n0\n0\n0.983398\n\n\n26\nA fruit platter with a variety of exotic fruit...\n1\n1\n0.983774",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-and-inspect-predictions-on-new-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-and-inspect-predictions-on-new-text-data",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "7 TK - Make and inspect predictions on new text data",
    "text": "7 TK - Make and inspect predictions on new text data\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n# Setup local model path\nlocal_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\nhuggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n\n7.1 TK - Pipeline mode\n\nTk - what is a pipeline?\n\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\ndef set_device():\n    \"\"\"\n    Set device to CUDA if available, else MPS (Mac), else CPU.\n\n    This defaults to using the best available device (usually).\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n\nDEVICE = set_device()\nprint(f\"[INFO] Using device: {DEVICE}\")\n\n[INFO] Using device: cuda\n\n\n\nimport torch\nfrom transformers import pipeline\n\n# Setup batch size for batched inference (can be adjusted depending on how much memory is available)\n# TK - why use batch size? -&gt; multiple samples at inference = faster\nBATCH_SIZE = 64\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=local_model_path,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE)\n\n\nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\nfood_not_food_classifier(sample_text_food)\n\n[{'label': 'food', 'score': 0.99871826171875}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9989410042762756}]\n\n\n\n# Pipeline also works with remote models (will have to laod the model locally first)\nfood_not_food_classifier_remote = pipeline(task=\"text-classification\", \n                                           model=huggingface_model_path,\n                                           batch_size=BATCH_SIZE,\n                                           device=DEVICE)\n\nfood_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")\n\n[{'label': 'food', 'score': 0.9981549382209778}]\n\n\n\n\n7.2 TK - Batch prediction\n\nTK - what is batch prediction?\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"Daniel Bourke is really cool :D\",\n    \"My favoruite food is biltong!\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9410305619239807},\n {'label': 'not_food', 'score': 0.9650871753692627},\n {'label': 'not_food', 'score': 0.9215793609619141},\n {'label': 'not_food', 'score': 0.9115400910377502},\n {'label': 'not_food', 'score': 0.9625208377838135},\n {'label': 'not_food', 'score': 0.9476941823959351},\n {'label': 'not_food', 'score': 0.9451109170913696},\n {'label': 'not_food', 'score': 0.9027702808380127},\n {'label': 'not_food', 'score': 0.9954429864883423},\n {'label': 'food', 'score': 0.7653573155403137}]\n\n\n\n\n7.3 TK - Time our model across larger sample sizes\n\nTK - our model is fast!\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n    print()\n\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n[INFO] Number of sentences: 100\n[INFO] Inference time for 100 sentences: 0.07726 seconds.\n[INFO] Avg inference time per sentence: 0.0007726 seconds.\n\n[INFO] Number of sentences: 1000\n[INFO] Inference time for 1000 sentences: 0.32344 seconds.\n[INFO] Avg inference time per sentence: 0.00032344 seconds.\n\n[INFO] Number of sentences: 10000\n[INFO] Inference time for 10000 sentences: 1.43834 seconds.\n[INFO] Avg inference time per sentence: 0.00014383 seconds.\n\n[INFO] Number of sentences: 100000\n[INFO] Inference time for 100000 sentences: 14.4585 seconds.\n[INFO] Avg inference time per sentence: 0.00014459 seconds.\n\nCPU times: user 15.8 s, sys: 552 ms, total: 16.3 s\nWall time: 16.3 s\n\n\n\n\n7.4 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text_food, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text_food}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "8 TK - Turning our model into a demo",
    "text": "8 TK - Turning our model into a demo\n\nTK - why build a demo?\n\n\ntry our model in the wild, see samples which don‚Äôt work properly, e.g.¬†use cases we didn‚Äôt think of‚Ä¶ ‚Äúpie‚Äù/‚Äútea‚Äù (short words), ‚Äúhjflasdjhfhwerr‚Äù (gibberish)\n\n\nTK - build a demo with Gradio, see it here: https://www.gradio.app/guides/quickstart\nTK - requires pip install gradio\n\n\n# Set top_k=2 to get top 2 predictions (in our case, food and not_food)\nfood_not_food_classifier(\"Testing the pipeline\", top_k=2)\n\n[{'label': 'not_food', 'score': 0.9977033734321594},\n {'label': 'food', 'score': 0.002296620048582554}]\n\n\n\n8.1 TK - Creating a simple function to perform inference\n\nTK - this is required for gradio -&gt; output a dict of {‚Äúlabel_1‚Äù: probability_1, ‚Äúlabel_2‚Äù: probability_2‚Ä¶}\n2 options:\n\nLocal demo (for our own inspection)\nHosted demo on Hugging Face Spaces (for sharing with others)\n\n\n\nimport gradio as gr\n\ndef food_not_food_classifier(text):\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=local_model_path,\n                                        batch_size=64,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\nfood_not_food_classifier(\"My lunch today was bacon and eggs\")\n\n{'food': 0.7966588139533997, 'not_food': 0.20334114134311676}\n\n\n\ndemo = gr.Interface(\n    fn=food_not_food_classifier, \n    inputs=\"text\", \n    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n    title=\"Food or Not Food Classifier\",\n    description=\"A text classifier to determine if a sentence is about food or not food.\",\n    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7863\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n\n\n8.2 TK - Uploading/running the demo\nOptions: * Uploading manually to Hugging Face Spaces - hf.co/new-space * Uploading programmatically to Hugging Face Spaces - https://www.gradio.app/guides/using-hugging-face-integrations#hosting-your-gradio-demos-on-spaces * Running the demo locally - Interface.launch() (only works if you have Gradio installed)\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\nimport torch\nimport gradio as gr\n\nfrom transformers import pipeline\n\ndef food_not_food_classifier(text):\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food.\n\nTK - See source code:\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\nTK - note: you will often need a requirements.txt file\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nCreate a README.md file with metadata instructions (these are specific to Hugging Face Spaces).\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\nsdk_version: 4.36.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nTK - see the demo notebook on how to create this\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\n\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file\n    upload_folder # for uploading multiple files (in a folder)\n)\n\npath_to_demo_folder = \"../demos/food_not_food_text_classifier\"\nrepo_type = \"space\" # we're creating a Hugging Face Space\n\n# Create a repo on Hugging Face\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_repo\ntarget_space_name = \"learn_hf_food_not_food_text_classifier_demo\"\nprint(f\"[INFO] Creating repo: {target_space_name}\")\ncreate_repo(\n    repo_id=target_space_name,\n    #token=\"YOUR_HF_TOKEN\"\n    private=False, # set to True if you want the repo to be private\n    repo_type=repo_type, # create a Hugging Face Space\n    space_sdk=\"gradio\", # we're using Gradio to build our demo \n    exist_ok=True, # set to False if you want to create the repo even if it already exists            \n)\n\n# Get the full repo name (e.g. \"mrdbourke/learn_hf_food_not_food_text_classifier_demo\")\nfull_repo_name = get_full_repo_name(model_id=target_space_name)\nprint(f\"[INFO] Full repo name: {full_repo_name}\")\n\n# Upload a file\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file \nprint(f\"[INFO] Uploading {path_to_demo_folder} to repo: {full_repo_name}\")\nfile_url = upload_folder(\n    folder_path=path_to_demo_folder,\n    path_in_repo=\".\", # save to the root of the repo\n    repo_id=full_repo_name,\n    repo_type=repo_type,\n    #token=\"YOUR_HF_TOKEN\"\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\n\n[INFO] Creating repo: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\n\nTK - see the demo link here: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\n\n8.3 TK - Testing the live demo\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML('''\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n&gt;&lt;/iframe&gt;     \n''')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---exercises-and-extensions",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---exercises-and-extensions",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 TK - Exercises and Extensions",
    "text": "9 TK - Exercises and Extensions\n\nReading:\n\nSpend 15 minutes reading the TrainingArguments documentation - https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\nSpend 10 minutes reading the Trainer documentation - https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer\n\nWhere does our model fail? E.g. what kind of sentences does it struggle with? How could you fix this?\n\nMake an extra 10-50 examples of these and add them to the dataset and then retrain the model\nSee here: https://discuss.huggingface.co/t/how-do-i-add-things-rows-to-an-already-saved-dataset/27423\n\nBuild your own text classifier on a different dataset/your own custom dataset\nHow might we make our dataset multi-class? (e.g.¬†more than 2 classes)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---extra-resources",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---extra-resources",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "10 TK - Extra resources",
    "text": "10 TK - Extra resources\n\nHugging Face guide on text classification: https://huggingface.co/docs/transformers/en/tasks/sequence_classification\nHugging Face documentation on padding and truncation - https://huggingface.co/docs/transformers/en/pad_truncation\nFor more on Transformers (the architecture) as well as the DistilBert model:\n\nRead Transformers from scratch by Peter Bloem.\nWatch Andrej Karpathy‚Äôs lecture on Transformers and their history.\nRead the original Attention is all you need paper (the paper that introduced the Transformer architecture).\nRead the DistilBert paper from the Hugging Face team (paper that introduced the DistilBert architecture and training setup).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  }
]