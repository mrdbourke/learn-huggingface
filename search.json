[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nEach example will include an end-to-end approach of starting with a dataset (custom or existing), building and evaluating a model and creating a demo to share.\nTeaching style:\nA machine learning cooking show! üë®‚Äçüç≥\nMottos:\nProject style:\nPrerequisites:\nTODO:"
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "FAQ",
    "text": "FAQ\n\nIs this an official Hugging Face website?\n\nNo, it‚Äôs a personal project by myself (Daniel Bourke) to learn and help others learn the Hugging Face ecosystem."
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "extras/setup.html",
    "href": "extras/setup.html",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Create a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens.\n\n\n\n\n\n\nFollow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nTK image - show image for loading a secret in Google Colab\nIf you need to force relogin for a notebook session, you can run:\nimport huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates).\n\n\n\n\nFollow the steps in Start here.\nInstall the Hugging Face CLI with pip install -U \"huggingface_hub[cli]\".\nFollow the setup steps mentioned in https://huggingface.co/docs/huggingface_hub/en/guides/cli.\n\n\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio."
  },
  {
    "objectID": "extras/setup.html#tk---start-here-universal-steps",
    "href": "extras/setup.html#tk---start-here-universal-steps",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Create a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens."
  },
  {
    "objectID": "extras/setup.html#tk---getting-setup-on-google-colab",
    "href": "extras/setup.html#tk---getting-setup-on-google-colab",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Follow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nTK image - show image for loading a secret in Google Colab\nIf you need to force relogin for a notebook session, you can run:\nimport huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates)."
  },
  {
    "objectID": "extras/setup.html#tk---getting-started-locally",
    "href": "extras/setup.html#tk---getting-started-locally",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Follow the steps in Start here.\nInstall the Hugging Face CLI with pip install -U \"huggingface_hub[cli]\".\nFollow the setup steps mentioned in https://huggingface.co/docs/huggingface_hub/en/guides/cli."
  },
  {
    "objectID": "extras/setup.html#installing-hugging-face-libraries",
    "href": "extras/setup.html#installing-hugging-face-libraries",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "We‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio."
  },
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\nTokenization = turning a series of data (text or image) into a series of tokens, where a token is a numerical representation of the input data, for example, in the case of text, tokenization could mean turning the words in a sentence into numbers (e.g.¬†‚Äúhello world‚Äù -&gt; [101, 102])\nTokens = a token is a letter, word or word-piece (word) that a model uses to represent input data, for example, in the case of text, a token could be a word (e.g.¬†‚Äúhello‚Äù) or a word-piece (e.g.¬†‚Äúhell‚Äù and ‚Äúo‚Äù), see: https://platform.openai.com/tokenizer for an example\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\nevaluate = A Python library by Hugging Face with premade evaluation functions for various tasks\ntorch = PyTorch, an open-source machine learning library\ntransformers.pipeline = an abstraction to get a machine learning pipeline up and running in a few lines of code, handles data preprocessing and device placement behind the scences. For example, transformers.pipeline(\"text-classification\") can be used to tokenize input text and classify it.\ntransfer learning = taking what one model has learned and applying it to another task (e.g.¬†a model which has learned across many millions of words of text from the internet and then adjusting it to work with your smaller dataset)\nfine-tuning = a type of transfer learning where you take the existing patterns of one model (usually trained on a very large dataset) and customize them to work for your smaller dataset\nhyperparameters = values you can set to adjust training settings, for example, learning rate is a hyperparameter that is adjustable\nHugging Face Hub (or Hub for short) = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared. If you are familiar with GitHub, Hugging Face is like the GitHub of machine learning.\nHugging Face Spaces = A platform to share and run machine learning apps/demos, usually built with Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit\nLearning rate = Often the most important hyperparameter to tune. It is proportional with the amount an optimizer will update a model‚Äôs parameters every update step. A higher amount means larger updates (though sometimes too large) a lower amount means smaller updates (though sometimes not enough). The most ideal learning rate is experimental. Common values include 0.001, 0.0001, 0.0005, 0.00001, 0.00005 (though the learning rate can be any value). Many optimizers have decent default learning rates. For example, the Adam optimizer (a common and generally well performing optimizer) in PyTorch (torch.optim.Adam) has a default learning rate of 0.001. For fine-tuning an already trained model a learning rate of 10x smaller than the default is a good rule of thumb (e.g.¬†if a model was trained with a learning rate of 0.001, fine-tuning with 0.0001 is common). The learning rate does not have to be static and can change dynamcially during training, this practice is referred to as learning rate scheduling.\nInference = using a trained (or untrained) model to make predictions on a given piece of data. The model infers what the output should be based on the inputs. Inference is often much faster than training on a sample per sample basis because no weights get updated during inference. Though, when compared to training, inference can often take more compute in the long run. Because a model can be trained once but then used for inference millions of times (or more) over the next several months (or longer).\nPrediction probability = the probability of a model‚Äôs prediction for a given input, is a score between 0 and 1 with 1 being the highest, for example, a model may have a prediction probability of 0.95, this would mean it‚Äôs quite confident with its prediction but it doesn‚Äôt mean it‚Äôs correct. A good way to inspect potential issues with a dataset is to show examples in the test set which have a high prediction probability but are wrong (e.g.¬†pred prob = 0.98 but the prediction was incorrect).\nHugging Face Pipeline (pipeline) = A high-level API for using model for various tasks (e.g.¬†text-classification, audio-classification, image-classification, object-detection and more), see the docs: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/pipelines#transformers.pipeline\nloss value = Measure of how wrong your model is by a given metric. A perfect model will have a loss value of 0 (it is able to predict the data perfectly), though this is highly unlikely in practice (there are no perfect models). Ideally, the loss value will go down (towards 0) as training goes on. If the loss value on the training set is lower than the loss value on the test set, the model is likely overfitting (memorizing the training set too well rather than learning generalizable patterns to unseen data). To fix overfitting, introduce more regularization. To fix underfitting (loss not going down), introduce more learning capacity (more data, more parameters in the model, longer training). Machine learning is a constant battle between overfitting and underfitting.\nRandom seed = Value to flavour the randomness of an operation. For example, if you set a random seed to 42 the numbers produced by a random generator will be random but flavoured by the seed. This means if the seed stays at 42, subsequent calls of the same operation will return the same values. Not setting a random seed will result in different random values each time. Setting a random seed is done to ensure reproducibility of an operation. This is helpful when performing experiments and you do not want the outputs to be random each time.\nSynthetic data generation = using a model such as a generative Large Language Model (LLM) to generate synthetic pieces of data for a specific problem. For example, getting an LLM to generate food and not food image captions to create a binary text classification model. Synthetic data is very helpful when bootstrapping a machine learning problem. Though it is advised to only train on synthetic data and to evaluate on real data whenever possible.\nPre-trained models = models which have already been trained on a large dataset, for example, text-based models which have gone through many millions of words of text (e.g.¬†all of Wikipedia and 1000s of books) or image-based models which have seen millions of images (e.g.¬†models trained on ImageNet). In essence, any model which has already spent a large amount of time learning patterns in data. These patterns can then be adjusted for your own sample problems, often with much much smaller amounts of data for excellent results. The process of customizing a pre-trained model for a specific problem is called transfer learning (transferring what an existing model knows to your own problem).\nTraining/test split = One of the most important concepts in machine learning. Train models on the training data and evaluate them on the test data. The test data should never be seen by a model during training. Think of the test data as the final exam in a university course. A model should be able to learn enough patterns in the training set to perform well on the test set. Just like a student should be able to learn enough on course materials to do well on the final exam. If a model performs well on the training set but not well on the test set, this is known as overfitting, as in, the model memorizes the training set rather than learning generalizable patterns to unseen data. If a model performs poorly on both the training set and the test set, this is known as underfitting.\n\nPrediction probabilities = a value assigned to a model‚Äôs prediction on a certain sample after its output logits have passed through an activation function such as Softmax or Sigmoid. For example, in a binary classification problem of whether an image is of food or not of food, a model could assign a prediction probability of the image being 0.98 food and 0.02 not food. Prediction probabilities do not indicate how right a prediction is, more so, how confident a model is in that prediction. The closer a prediction probability to 1, the higher the model‚Äôs confidence in the prediction. A good evaluation step is to inspect samples with low prediction probabilities (the model seems to get confused on them) or inspect test samples where the model has a high prediction probability but the prediction is wrong (these predictions are often referred to as most wrong predictions)."
  },
  {
    "objectID": "extras/todo.html",
    "href": "extras/todo.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Move todo.md into index.md for easier navigation (one file is enough)\nAdd ‚Äúgetting setup‚Äù file to get started locally with the required dependencies\nAdd text classification dataset creation notebook (so people can see where the data comes from)\nAdd a Hugging Face ecosystem overview (transformers, datasets, tokenizers, torch, Hugging Face Hub, Hugging Face Spaces, etc.)\nAdd a fav icon\nMake code-only versions of notebooks? e.g.¬†text stripped away and only code is left"
  },
  {
    "objectID": "extras/todo.html#quarto-misc",
    "href": "extras/todo.html#quarto-misc",
    "title": "Learn Hugging Face ü§ó",
    "section": "Quarto misc",
    "text": "Quarto misc\n\nCreate share cards - https://quarto.org/docs/websites/website-tools.html#twitter-cards\n\nSee here for share image - https://quarto.org/docs/websites/website-tools.html#preview-images"
  },
  {
    "objectID": "extras/resources.html",
    "href": "extras/resources.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "See the Pytorch extra resources for some ideas: https://www.learnpytorch.io/pytorch_extra_resources/\nHugging Face NLP course: https://huggingface.co/learn/nlp-course/chapter0/1"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Next:\n# Add tools used in this overview (e.g. overview of the project)\n# Create a small dataset with text generation, e.g. 50x spam/not_spam emails and train a classifier on it ‚úÖ\n   # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n# Save the dataset to Hugging Face Datasets ‚úÖ\n   # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n# Train a classifier on it ‚úÖ\n# Save the model to the Hugging Face Model Hub ‚úÖ\n# Create a with Gradio and test the model in the wild ‚úÖ \n\n# TODO:\n# ‚úÖ Make sure notebook runs in Google Colab \n   # ‚úÖ Can the notebook run with a demo account? E.g. not my own Hugging Face token? ‚úÖ\n# ‚úÖ Add a link to the dataset notebook - does this need a walkthrough of how it was created? Could do this in one video? ‚úÖ\n# UPTOHERE: Go back and edit the TK's through the notebook\n   # count: 57\n   # count: 25\n   # ‚úÖ Can the introduction to Hugging Face be added to the front page? \n      # This is universal for projects and won't need to be repeated each project... \n   # Add images and diagrams throughout\n    # Look into Quarto figures and how these work\n# Make sure the online book version looks good\n# Make a code only version of the notebook so there's less fluff",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 TK - Overview",
    "text": "1 TK - Overview\n\nTK - add a note that this notebook can be run in end-to-end in Google Colab, however, it‚Äôs best viewed at learnhuggingface.com for formatting purposes\nTK image - add an image of the project we‚Äôre building\n\n\n1.1 What we‚Äôre going to build\nIn this project, we‚Äôre going to learn various aspects of the Hugging Face ecosystem whilst building a text classification model.\nTo keep things as practical as possible, we‚Äôre going to be bulding a food/not_food text classification model.\nGiven a piece of a text (such as an image caption), our model will be able to predict if it‚Äôs about food or not.\nThis is the same kind of model I use in my own work on Nutrify (an app to help people learn about food).\nMore specifically, we‚Äôre going to follow the following steps:\n\nProblem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nFinding, training and evaluating a model - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\nCreating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others:\n\nfrom IPython.display import HTML \n\nHTML(\"\"\"\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"650\"\n&gt;&lt;/iframe&gt;\n\"\"\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote this is a hands-on project, so we‚Äôll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we‚Äôre doing, I‚Äôll leave links in the extra-curriculum section.\n\n\n\n\n1.2 TK - What is text classification?\nText classification is the process of assigning a category to a piece of text.\nWhere a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\nTK image - example of text classification\nExample text classification problems include:\n\n\n\n\n\n\n\n\nProblem\nDescription\nProblem Type\n\n\n\n\nSpam/phishing email detection\nIs an email spam or not spam? Or is it a phishing email or not?\nBinary classification (one thing or another)\n\n\nSentiment analysis\nIs a piece of text positive, negative or neutral? Such as classifying product reviews into good/bad/neutral.\nMulti-class classification (one thing from many)\n\n\nLanguage detection\nWhat language is a piece of text written in?\nMulti-class classification (one thing from many)\n\n\nTopic classification\nWhat topic(s) does a news article belong to?\nMulti-label classification (one or more things from many)\n\n\nHate speech detection\nIs a comment hateful or not hateful?\nBinary classification (one thing or another)\n\n\nProduct categorization\nWhat categories does a product belong to?\nMulti-label classification (one or more things from many)\n\n\n\nText classification is a very common problem in many business settings.\nFor example, a project I‚Äôve worked on previously as a machine learning engineer was building a text classification model to classify different insurance claims into claimant_at_fault/claimant_not_at_fault for a large insurance company.\nIt turns out the deep learning-based model we built was very good (98%+ accuracy on the test dataset).\nSpeaking of models, there are several different kinds of models you can use for text classification.\nAnd each will have its pros and cons depending on the problem you‚Äôre working on.\nExample text classification models include:\n\n\n\nModel\nDescription\nPros\nCons\n\n\n\n\nRule-based\nUses a set of rules to classify text (e.g.¬†if text contains ‚Äúsad‚Äù -&gt; sentiment = low)\nSimple, easy to understand\nRequires manual creation of rules\n\n\nBag of Words\nCounts the frequency of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nTF-IDF\nWeighs the importance of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nDeep learning-based models\nUses neural networks to learn patterns in text\nCan learn complex patterns at scale\nCan require large amounts of data/compute power to run, not as easy to understand (can be hard to debug)\n\n\n\nFor our project, we‚Äôre going to go with a deep learning model.\nWhy?\nBecause Hugging Face helps us do so.\nAnd in most cases, with a quality dataset, a deep learning model will often perform better than a rule-based or other model.\n\n\n1.3 TK - Why train your own text classification models?\nYou can use pre-trained models for text classification as well as API-powered models and LLMs such as GPT-4 or Gemini.\nHowever, it‚Äôs often a good idea to train your own text classification models for a few reasons:\n\nThey can be much faster than API-powered models (since they‚Äôre running on your own hardware, this can save on costs and time).\nThey‚Äôre customized to your own data.\nThey don‚Äôt require you to send your data elsewhere (privacy).\nIf a service goes down, you‚Äôll still have access to your model (reliability).\nYou own the model at the end.\n\nTK image - example of training your own model vs using an API-powered model\n\n\n1.4 TK - Workflow we‚Äôre going to follow\nTK - explain the workflow\n\nCreate and preprocess data.\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 TK - Importing necessary libraries",
    "text": "2 TK - Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide (tk - link to getting setup guide) to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport transformers\n\nfrom datasets import Dataset\n\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\n\nUsing transformers version: 4.41.2\nUsing datasets version: 2.19.1\nUsing torch version: 2.2.0+cu121\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 Getting a dataset",
    "text": "3 Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor a text classificaiton problem, your dataset will likely come in the form of text (e.g.¬†a paragraph, sentence or phrase) and a label (e.g.¬†what category the text belongs to).\n\nTK image - showcase what a supervised dataset looks like (e.g.¬†text and label, this can be the dataset we‚Äôve got on Hugging Face hub, showcase the different parts of the dataset as well including the name etc)\n\nIn our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\nThis is a dataset I‚Äôve created earlier to help us practice building a text classification model.\nYou can find it on Hugging Face under the name mrdbourke/learn_hf_food_not_food_image_captions.\n\n\n\n\n\n\nFood Not Food Image Caption Dataset Creation\n\n\n\nYou can see how the Food Not Food image caption dataset was created in the example Google Colab notebook.\nA Large Language Model (LLM) was asked to generate various image caption texts about food and not food.\nGetting another model to create data for a problem is known as synthetic data generation and is a very good way of bootstrapping towards creating a model.\nOne workflow would be to use real data wherever possible and use synthetic data to boost when needed.\nNote that it‚Äôs always advised to evaluate/test models on real-life data as opposed to synthetic data.\n\n\n\n3.1 Where can you get more datasets?\nThe are many different places you can get datasets for text-based problems.\nOne of the best places is on the Hugging Face Hub, specifically huggingface.co/datasets.\nHere you can find many different kinds of problem specific data such as text classification.\nTK image - show example image of text classification datasets\n\n\n3.2 Loading the dataset\nOnce we‚Äôve found/prepared a dataset on the Hugging Face Hub, we can use the Hugging Face datasets library to load it.\nTo load a dataset we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/learn_hf_food_not_food_image_captions.\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n# Load the dataset from Hugging Face Hub\ndataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n\n# Inspect the dataset\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 250\n    })\n})\n\n\nDataset loaded!\nLooks like our dataset has two features, text and label.\nAnd 250 total rows (the number of examples in our dataset).\nWe can check the column names with dataset.column_names.\n\n# What features are there?\ndataset.column_names\n\n{'train': ['text', 'label']}\n\n\nLooks like our dataset comes with a train split already (the whole dataset).\nWe can access the train split with dataset[\"train\"] (some datasets also come with built-in \"test\" splits too).\n\n# Access the training split\ndataset[\"train\"]\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 250\n})\n\n\nHow about we check out a single sample?\nWe can do so with indexing.\n\ndataset[\"train\"][0]\n\n{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n 'label': 'food'}\n\n\nNice! We get back a dictionary with the keys text and label.\nThe text key contains the text of the image caption and the label key contains the label (food or not food).\n\n\n3.3 Inspect random examples from the dataset\nAt 250 total samples, our dataset isn‚Äôt too large.\nSo we could sit here and explore the samples one by one.\nBut whenever I interact with a new dataset, I like to view a bunch of random examples and get a feel of the data.\nDoing so is inline with the data explorer‚Äôs motto: visualize, visualize, visualize!\nAs a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\nLet‚Äôs write some code to view 5 random indexes of our data and their corresponding text and labels at a time.\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: Pizza with a stuffed crust, oozing with cheese | Label: food\nText: Brushing her cat's fur in her bedroom, a young girl concentrates | Label: not_food\nText: Set of muffin tins stacked together | Label: not_food\nText: Garden hose rolled up and ready in a yard | Label: not_food\nText: A cat and a dog sitting on a couch | Label: not_food\n\n\nBeautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\nWe can get the unique labels in our dataset with dataset[\"train\"].unique(\"label\").\n\n# Get unique label values\ndataset[\"train\"].unique(\"label\")\n\n['food', 'not_food']\n\n\nIf our dataset is small enough to fit into memory, we can count the number of different labels with Python‚Äôs collections.Counter (a method for counting objects in an iterable or mapping).\n\n# Check number of each label\nfrom collections import Counter\n\nCounter(dataset[\"train\"][\"label\"])\n\nCounter({'food': 125, 'not_food': 125})\n\n\nExcellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\nIn a binary classification case, this is ideal.\nIf the classes were dramatically unbalanced (e.g.¬†90% food and 10% not food) we might have to consider collecting/creating more data.\nBut best to train a model and see how it goes before making any drastic dataset changes.\nBecause our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets).\n\n# Turn our dataset into a DataFrame and get a random sample\nfood_not_food_df = pd.DataFrame(dataset[\"train\"])\nfood_not_food_df.sample(7)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n46\nSet of mugs hanging on a hook\nnot_food\n\n\n125\nA bowl of sliced cantaloupe with a sprinkle of...\nfood\n\n\n30\nSet of test tubes arranged in a rack\nnot_food\n\n\n86\nA fruit kabob with a variety of fruits, such a...\nfood\n\n\n31\nPotatoes, onions, garlic, cauliflower, and bro...\nfood\n\n\n227\nA plate of sliced pineapple with a side of whi...\nfood\n\n\n127\nZucchini in a bowl, sprinkled with basil and s...\nfood\n\n\n\n\n\n\n\n\n# Get the value counts of the label column\nfood_not_food_df[\"label\"].value_counts()\n\nlabel\nfood        125\nnot_food    125\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 Preparing data for text classification",
    "text": "4 Preparing data for text classification\nWe‚Äôve got our data ready but there are a few steps we‚Äôll need to take before we can model it.\nThe main two being:\n\nTokenization - turning our text into a numerical representation (machines prefer numbers rather than words), for example, {\"a\": 0, \"b\": 1, \"c\": 2...}.\nCreating a train/test split - right now our data is in a training split only but we‚Äôll create a test set to evaluate our model‚Äôs performance.\n\nThese don‚Äôt necessarily have to be in order either.\nBefore we get to them, let‚Äôs create a small mapping from our labels to numbers.\nIn the same way we need to tokenize our text into numerical representation, we also need to do the same for our labels.\n\n4.1 Creating a mapping from labels to numbers\nOur machine learning model will want to see all numbers (people do well with text, computers do well with numbers).\nThis goes for text as well as label input.\nSo let‚Äôs create a mapping from our labels to numbers.\nSince we‚Äôve only got a couple of labels (\"food\" and \"not_food\"), we can create a dictionary to map them to numbers, however, if you‚Äôve got a fair few labels, you may want to make this mapping programmatically.\nWe can use these dictionaries later on for our model training as well as evaluation.\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a binary classification task (such as what we‚Äôre working on), the positive class, in our case \"food\", is usually given the label 1 and the negative class (\"not_food\") is given the label 0.\n\n\n\n# Create mappings programmatically from dataset\nid2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\nlabel2id = {label: idx for idx, label in id2label.items()}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\nWith our dictionary mappings created, we can update the labels of our dataset to be numeric.\nWe can do this using the datasets.Dataset.map method and passing it a function to apply to each example.\nLet‚Äôs create a small function which turns an example label into a number.\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\nexample_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n\n# Test the function\nmap_labels_to_number(example_sample)\n\n{'text': 'This is a sentence about my favourite food: honey.', 'label': 1}\n\n\nLooks like our function works!\nHow about we map it to the whole dataset?\n\n# Map our dataset labels to numbers\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\nNice! Looks like our labels are all numerical now.\nWe can check a few random samples using dataset.shuffle() and indexing for the first few.\n\n# Shuffle the dataset and view the first 5 samples (will return different results each time) \ndataset.shuffle()[:5]\n\n{'text': ['Set of measuring cups nested in a drawer',\n  'Fennel in a bowl, sprinkled with lemon zest and served with a side of olive oil for a light, refreshing dish.',\n  'Creamy spinach and potato curry, featuring fluffy potatoes and nutritious spinach in a rich sauce with cream and garam masala.',\n  'A slice of pizza with a spicy kick, featuring jalapeno peppers',\n  'A kabob of grilled vegetables, including zucchini, squash, and onion, perfect for a summer barbecue.'],\n 'label': [0, 1, 1, 1, 1]}\n\n\n\n\n4.2 Split the dataset into training and test sets\nRight now our dataset only has a training split.\nHowever, we‚Äôd like to create a test split so we can evaluate our model.\nIn essence, our model will learn patterns (the relationship between text captions and their labels of food/not_food) on the training data.\nAnd we will evaluate those learned patterns on the test data.\nWe can split our data using the datasets.Dataset.train_test_split method.\nWe can use the test_size parameter to define the percentage of data we‚Äôd like to use in our test set (e.g.¬†test_size=0.2 would mean 20% of the data goes to the test set).\n\n# Create train/test splits\ndataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\ndataset\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create train/test splits\n----&gt; 2 dataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\n      3 dataset\n\nNameError: name 'dataset' is not defined\n\n\n\nPerfect!\nOur dataset has been split into 200 training examples and 50 testing examples.\nLet‚Äôs visualize a few random examples to make sure they still look okay.\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: Turnips in a bowl, sprinkled with pepper and served with a side of mustard sauce for a hearty, flavorful dish.\nLabel: 1 (food)\n\n[INFO] Random sample from testing dataset:\nText: Set of stainless steel utensils arranged on a kitchen table\nLabel: 0 (not_food)\n\n\n\n\n4.3 TK - Tokenizing text data\nLabels numericalized, dataset split, time to turn our text into numbers.\nHow?\nTokenization.\nWhat‚Äôs tokenization?\nTokenization is the process of converting a non-numerical data source into numbers.\nWhy?\nBecause machines (especially machine learning models) prefer numbers to human-style data.\nIn the case of the text \"I love pizza\" a very simple method of tokenization might be to convert each word to a number.\nFor example, {\"I\": 0, \"love\": 1, \"pizza\": 2}.\nHowever, for most modern machine learning models, the tokenization process is a bit more nuanced.\nFor example, the text \"I love pizza\" might be tokenized into something more like [101, 1045, 2293, 10733, 102].\nTK image - showcase an example using OpenAI‚Äôs tokenization tool and what this looks like with ‚ÄúI love pizza‚Äù: https://platform.openai.com/tokenizer\n\n\n\n\n\n\nNote\n\n\n\nDepending on the model you use, the tokenization process could be different.\nFor example, one model might turn \"I love pizza\" into [40, 3021, 23317], where as another model might turn it into [101, 1045, 2293, 10733, 102].\nTo deal with this, Hugging Face models often pair models and tokenizers together by name.\nSuch is the case with distilbert/distilbert-base-uncased (there is a tokenizer.json file as well as a tokenizer_config.json file which contains all of the tokenizer implementation details).\nFor more examples of tokenization, you can see OpenAI‚Äôs tokenization visualizer tool as well as their open-source library tiktoken, Google also have an open-source tokenization library called sentencepiece, finally Hugging Face‚Äôs tokenizers library is also a great resource (this is what we‚Äôll be using behind the scenes).\n\n\nMany of the text-based models on Hugging Face come paired with their own tokenizer.\nFor example, the distilbert/distilbert-base-uncased model is paired with the distilbert/distilbert-base-uncased tokenizer.\nWe can load the tokenizer for a given model using the transformers.AutoTokenizer.from_pretrained method and passing it the name of the model we‚Äôd like to use.\nThe transformers.AutoTokenizer class is part of a series of Auto Classes (such as AutoConfig, AutoModel, AutoProcessor) which automatically loads the correct configuration settings for a given model ID.\nLet‚Äôs load the tokenizer for the distilbert/distilbert-base-uncased model and see how it works.\n\n\n\n\n\n\nNote\n\n\n\nWhy use the distilbert/distilbert-base-uncased model?\nThe short answer is that I‚Äôve used it before and it works well (and fast) on various text classification tasks.\nIt also performed well in the original research paper which introduced it.\nThe longer answer is that Hugging Face has many available open-source models for many different problems available at https://huggingface.co/models.\nNavigating these models can take some practice.\nAnd several models may be suited for the same task (though with various tradeoffs such as size and speed).\nHowever, overtime and with adequate experimentation, you‚Äôll start to build an intuition on which models are good for which problems.\n\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n\ntokenizer\n\nDistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\nNice!\nThere‚Äôs our tokenizer!\nIt‚Äôs an instance of the transformers.DistilBertTokenizerFast class.\nYou can read more about it in the documentation.\nFor now, let‚Äôs try it out by passing it a string of text.\n\n# Test out tokenizer\ntokenizer(\"I love pizza\")\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\n\n# Try adding a \"!\" at the end\ntokenizer(\"I love pizza!\")\n\n{'input_ids': [101, 1045, 2293, 10733, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n\n\nWoohoo!\nOur text gets turned into numbers (or tokens).\nNotice how with even a slight change in the text, the tokenizer produces different results?\nThe input_ids are our tokens.\nAnd the attention_mask (in our case, all [1, 1, 1, 1, 1, 1]) is a mask which tells the model which tokens to use or not.\nTokens with a mask value of 1 get used and tokens with a mask value of 0 get ignored.\nThere are several attributes of the tokenizer we can explore.\n\ntokenizer.vocab will return the vocabulary of the tokenizer or in other words, the unique words/word pieces the tokenizer is capable of converting into numbers.\ntokenizer.model_max_length will return the maximum length of a sequence the tokenizer can process, pass anything longer than this and the sequence will be truncated.\n\n\n# Get the length of the vocabulary \nlength_of_tokenizer_vocab = len(tokenizer.vocab)\nprint(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n\n# Get the maximum sequence length the tokenizer can handle\nmax_tokenizer_input_sequence_length = tokenizer.model_max_length\nprint(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")\n\nLength of tokenizer vocabulary: 30522\nMax tokenizer input sequence length: 512\n\n\nWoah, looks like our tokenizer has a vocabulary of 30,522 different words and word pieces.\nAnd it can handle a sequence length of up to 512 (any sequence longer than this will be automatically truncated from the end).\nLet‚Äôs check out some of the vocab.\nCan I find my own name?\n\n# Does \"daniel\" occur in the vocab?\ntokenizer.vocab[\"daniel\"]\n\n3817\n\n\nOooh, looks like my name is 3817 in the tokenizer‚Äôs vocab.\nCan you find your own name? (note: there may be an error if the token doesn‚Äôt exist, we‚Äôll get to this)\nHow about ‚Äúpizza‚Äù?\n\ntokenizer.vocab[\"pizza\"]\n\n10733\n\n\nWhat if a word doesn‚Äôt exist in the vocab?\n\ntokenizer.vocab[\"akash\"]\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 tokenizer.vocab[\"akash\"]\n\nKeyError: 'akash'\n\n\n\nDam, we get a KeyError.\nNot to worry, this is okay, since when calling the tokenizer on the word, it will automatically split the word into word pieces or subwords.\n\ntokenizer(\"akash\")\n\n{'input_ids': [101, 9875, 4095, 102], 'attention_mask': [1, 1, 1, 1]}\n\n\nIt works!\nWe can check what word pieces \"akash\" got broken into with tokenizer.convert_ids_to_tokens(input_ids).\n\ntokenizer.convert_ids_to_tokens(tokenizer(\"akash\").input_ids)\n\n['[CLS]', 'aka', '##sh', '[SEP]']\n\n\nAhhh, it seems \"akash\" was split into two tokens, [\"aka\", \"##sh\"].\nThe \"##\" at the start of \"##sh\" means that the sequence is part of a larger sequence.\nAnd the \"[CLS]\" and \"[SEP]\" tokens are special tokens indicating the start and end of a sequence.\nNow, since tokenizers can deal with any text, what if there was an unknown token?\nFor example, rather than \"pizza\" someone used the pizza emoji üçï?\nLet‚Äôs try!\n\n# Try to tokenize an emoji\ntokenizer.convert_ids_to_tokens(tokenizer(\"üçï\").input_ids)\n\n['[CLS]', '[UNK]', '[SEP]']\n\n\nAhh, we get the special \"[UNK]\" token.\nThis stands for ‚Äúunknown‚Äù.\nThe combination of word pieces and \"[UNK]\" special token means that our tokenizer will be able to turn almost any text into numbers for our model.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that just because one tokenizer uses an unknown special token for a particular word or emoji (üçï) doesn‚Äôt mean another will.\n\n\nSince the tokenizer.vocab is a Python dictionary, we can get a sample of the vocabulary using tokenizer.vocab.items().\nHow about we get the first 5?\n\n# Get the first 5 items in the tokenizer vocab\nsorted(tokenizer.vocab.items())[:5]\n\n[('!', 999), ('\"', 1000), ('#', 1001), ('##!', 29612), ('##\"', 29613)]\n\n\nThere‚Äôs our '!' from before! Looks like the first five items are all related to punctuation points.\nHow about a random sample of tokens?\n\nimport random\n\nrandom.sample(sorted(tokenizer.vocab.items()), k=5)\n\n[('chased', 13303),\n ('luther', 9678),\n ('fossil', 10725),\n ('sphinx', 27311),\n ('murderous', 25303)]\n\n\n\n\n4.4 Making a preprocessing function to tokenize text\nRather than tokenizing our texts one by one, it‚Äôs best practice to define a preprocessing function which does it for us.\nThis process works regardless of whether you‚Äôre working with text data or other kinds of data such as images or audio.\n\n\n\n\n\n\nTurning data into numbers\n\n\n\nFor any kind of machine learning workflow, an important first step is turning your input data into numbers.\nAs machine learning models are algorithms which find patterns in numbers, before they can find patterns in your data (text, images, audio, tables) it must be numerically encoded first (e.g.¬†tokenizing text).\nTo help with this, transformers has an AutoProcessor class which can preprocess data in a specific format required for a paired model.\n\n\nTo prepare our text data, let‚Äôs create a preprocessing function to take in a dictionary which contains the key \"text\" which has the value of a target string (our data samples come in the form of dictionaries) and then returns the tokenized \"text\".\nWe‚Äôll set the following parameters in our tokenizer:\n\npadding=True - This will make all the sequences in a batch the same length by padding shorter sequences with 0‚Äôs until they equal the longest size in the batch. Why? If there are different size sequences in a batch, you can sometimes run into dimensionality issues.\ntruncation=True - This will shorten sequences longer than the model can handle to the model‚Äôs max input size (e.g.¬†if a sequence is 1000 long and the model can handle 512, it will be shortened to 512 via removing all tokens after 512).\n\nYou can see more parameters available for the tokenizer in the transformers.PreTrainedTokenizer documentation.\n\n\n\n\n\n\nNote\n\n\n\nFor more on padding and truncation (two important concepts in sequence processing), I‚Äôd recommend reading the Hugging Face documentation on Padding and Truncation.\n\n\n\ndef tokenize_text(examples):\n    \"\"\"\n    Tokenize given example text and return the tokenized text.\n    \"\"\"\n    return tokenizer(examples[\"text\"],\n                     padding=True, # pad short sequences to longest sequence in the batch\n                     truncation=True) # truncate long sequences to the maximum length the model can handle\n\nWonderful!\nNow let‚Äôs try it out on an example sample.\n\nexample_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n\n# Test the function\ntokenize_text(example_sample_2)\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\nLooking good!\nHow about we map our tokenize_text function to our whole dataset?\nWe can do so with the datasets.Dataset.map method.\nThe map method allows us to apply a given function to all examples in a dataset.\nBy setting batched=True we can apply the given function to batches of examples (many at a time) to speed up computation time.\nLet‚Äôs create a tokenized_dataset object by calling map on our dataset and passing it our tokenize_text function.\n\n# Map our tokenize_text function to the dataset\ntokenized_dataset = dataset.map(function=tokenize_text, \n                                batched=True, # set batched=True to operate across batches of examples rather than only single examples\n                                batch_size=1000) # defaults to 1000, can be increased if you have a large dataset\n\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\nDataset tokenized!\nLet‚Äôs inspect a pair of samples.\n\n# Get two samples from the tokenized dataset\ntrain_tokenized_sample = tokenized_dataset[\"train\"][0]\ntest_tokenized_sample = tokenized_dataset[\"test\"][0]\n\nfor key in train_tokenized_sample.keys():\n    print(f\"[INFO] Key: {key}\")\n    print(f\"Train sample: {train_tokenized_sample[key]}\")\n    print(f\"Test sample: {test_tokenized_sample[key]}\")\n    print(\"\")\n\n[INFO] Key: text\nTrain sample: Set of headphones placed on a desk\nTest sample: A slice of pepperoni pizza with a layer of melted cheese\n\n[INFO] Key: label\nTrain sample: 0\nTest sample: 1\n\n[INFO] Key: input_ids\nTrain sample: [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [101, 1037, 14704, 1997, 11565, 10698, 10733, 2007, 1037, 6741, 1997, 12501, 8808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[INFO] Key: attention_mask\nTrain sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\nBeautiful! Our samples have been tokenized.\nNotice the zeroes on the end of the inpud_ids and attention_mask values.\nThese are padding tokens to ensure that each sample has the same length as the longest sequence in a given batch.\nWe can now use these tokenized samples later on in our model.\n\n\n4.5 Tokenization takeaways\nWe‚Äôve seen tokenizers in practice.\nA few takeaways before we start to build a model:\n\nTokenizers are used to turn text (or other forms of data such as images and audio) into a numerical representation ready to be used with a machine learning model.\nMany models reuse existing tokenizers and many models have their own specific tokenizer paired with them. Hugging Face‚Äôs transformers.AutoTokenizer, transformers.AutoProcessor and transformers.AutoModel classes make it easy to pair tokenizers and models based on their name (e.g.¬†distilbert/distilbert-base-uncased).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---setting-up-an-evaluation-metric",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---setting-up-an-evaluation-metric",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "5 TK - Setting up an evaluation metric",
    "text": "5 TK - Setting up an evaluation metric\nAside from training a model, one of the most important steps in machine learning is evaluating a model.\nTo do, we can use evaluation metrics.\nAn evaluation metric attempts to represent a model‚Äôs performance in a single (or series) of numbers (note, I say ‚Äúattempts‚Äù here because evaluation metrics are useful to guage performance but the real test of a machine learning model is in the real world).\nThere are many different kinds of evaluation metrics for various problems.\nBut since we‚Äôre focused on text classification, we‚Äôll use accuracy as our evaluation metric.\nA model which gets 99/100 predictions correct has an accuracy of 99%.\n\\[\n\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\n\\]\nFor some projects, you may have a minimum standard of a metric.\nFor example, when I worked on an insurance claim classification model, the clients required over 98% accuracy on the test dataset for it to be viable to use in production.\nIf needed, we can craft these evaluation metrics ourselves.\nHowever, Hugging Face has a library called evaluate which has various metrics built in ready to use.\nWe can load a metric using evaluate.load(\"METRIC_NAME\").\nLet‚Äôs load in \"accuracy\" and build a function to measure accuracy by comparing arrays of predictions and labels.\n\nimport evaluate\nimport numpy as np\nfrom typing import Tuple\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n  \"\"\"\n  Computes the accuracy of a model by comparing the predictions and labels.\n  \"\"\"\n  predictions, labels = predictions_and_labels\n\n  # Get highest prediction probability of each prediction if predictions are probabilities\n  if len(predictions.shape) &gt;= 2:\n    predictions = np.argmax(predictions, axis=1)\n\n  return accuracy_metric.compute(predictions=predictions, references=labels)\n\nAccuracy function created!\nNow let‚Äôs test it out.\n\n# Create example list of predictions and labels\nexample_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nexample_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\nexample_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# Test the function\nprint(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\nprint(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")\n\nAccuracy when all predictions are correct: {'accuracy': 1.0}\nAccuracy when one prediction is wrong: {'accuracy': 0.9}\n\n\nExcellent, our function works just as we‚Äôd like.\nWhen all predictions are correct, it scores 1.0 (or 100% accuracy) and when 9/10 predictions are correct, it returns 0.9 (or 90% accuracy).\nWe can use this function during training and evaluation of our model.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-a-model-for-training",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-a-model-for-training",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "6 Setting up a model for training",
    "text": "6 Setting up a model for training\nWe‚Äôve gone through the important steps of setting data up for training (and evaluation).\nNow let‚Äôs prepare a model.\nWe‚Äôll keep going through the following steps:\n\n‚úÖ Create and preprocess data.\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\n\nTK image - steps for training in Hugging Face\n\nLet‚Äôs start by creating an instance of a model.\nSince we‚Äôre working on text classification, we‚Äôll do so with transformers.AutoModelForSequenceClassification (where sequence classification means a sequence of something, e.g.¬†our sequences of text).\nWe can use the from_pretrained() method to instatiate a pretrained model from the Hugging Face Hub.\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúpretrained‚Äù in transformers.AutoModelForSequenceClassification.from_pretrained means acquiring a model which has already been trained on a certain dataset.\nThis is common practice in many machine learning projects and is known as transfer learning.\nThe idea is to take an existing model which works well on a task similar to your target task and then fine-tune it to work even better on your target task.\nIn our case, we‚Äôre going to use the pretrained DistilBERT base model (distilbert/distilbert-base-uncased) which has been trained on many thousands of books as well as a version of the English Wikipedia (millions of words).\nThis training gives it a very good baseline representation of the patterns in language.\nWe‚Äôll take this baseline representation of the patterns in language and adjust it slightly to focus specifically on predicting whether an image caption is about food or not (based on the words it contains).\nThe main two benefits of using transfer learning are:\n\nAbility to get good results with smaller amounts of data (since the main representations are learned on a larger dataset, we only have to show the model a few examples of our specific problem).\nThis process can be repeated acorss various domains and tasks. For example, you can take a computer vision model trained on millions of images and customize it to your own use case. Or an audio model trained on many different nature sounds and customize it specifically for birds.\n\nSo when starting a new machine learning project, one of the first questions you should ask is: does an existing pretrained model similar to my task exist and can I fine-tune it for my own task?\nFor an end-to-end example of transfer learning in PyTorch (another popular deep learning framework), see PyTorch Transfer Learning.\n\n\nTime to setup our model instance.\nA few things to note:\n\nWe‚Äôll use transformers.AutoModelForSequenceClassification.from_pretrained, this will create the model architecture we specify with the pretrained_model_name_or_path parameter.\nThe AutoModelForSequenceClassification class comes with a classification head on top of our mdoel (so we can customize this to the number of classes we have with the num_labels parameter).\nUsing from_pretrained will also call the transformers.PretrainedConfig class which will enable us to set id2label and label2id parameters for our fine-tuning task.\n\nLet‚Äôs refresh what our id2label and label2id objects look like.\n\n# Get id and label mappings\nprint(f\"id2label: {id2label}\")\nprint(f\"label2id: {label2id}\")\n\nid2label: {0: 'not_food', 1: 'food'}\nlabel2id: {'not_food': 0, 'food': 1}\n\n\nBeautiful, we can pass these mappings to transformers.AutoModelForSequenceClassification.from_pretrained.\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Setup model for fine-tuning with classification head (top layers of network)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n    label2id=label2id\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel created!\nYou‚Äôll notice that a warning message gets displayed:\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: [‚Äòclassifier.bias‚Äô, ‚Äòclassifier.weight‚Äô, ‚Äòpre_classifier.bias‚Äô, ‚Äòpre_classifier.weight‚Äô] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nThis is essentially saying ‚Äúhey, some of the layers in this model are newly initialized (with random patterns) and you should probably customize them to your own dataset‚Äù.\nThis happens because we used the AutoModelForSequenceClassification class.\nWhilst the majority of the layers in our model have already learned patterns from a large corpus of text, the top layers (classifier layers) have been randomly setup so we can customize them on our own.\nLet‚Äôs try and make a prediction with our model and see what happens.\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][0])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 2\n      1 # Try and make a prediction with the loaded model (this will error)\n----&gt; 2 model(**tokenized_dataset[\"train\"][0])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\nOh no! We get an error.\nNot to worry, this is only because our model hasn‚Äôt been trained on our own dataset yet.\nLet‚Äôs take a look at the layers in our model.\n\n# Inspect the model \nmodel\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\n\nTK image - show what it looks like when fine-tuning a model for a specific task, e.g.¬†backbone is pre-trained layers, classification head is swapped out\n\nYou‚Äôll notice that the model comes in 3 main parts (data flows through these sequentially):\n\nembeddings - This part of the model turns the input tokens into a learned representation. So rather than just a list of integers, the values become a learned representation. This learned representation comes from the base model learning how different words and word pieces relate to eachother thanks to its training data. The size of (30522, 768) means the 30,522 words in the vocabulary are all represented by vectors of size 768 (one word gets represented by 768 numbers, these are often not human interpretable).\ntransformer - This is the main body of the model. There are several TransformerBlock layers stacked on top of each other. These layers attempt to learn a deeper representation of the data going through the model. A thorough breakdown of these layers is beyond the scope of this tutorial, however, for and in-depth guide on Transformer-based models, I‚Äôd recommend reading Transformers from scratch by Peter Bloem, going through Andrej Karpathy‚Äôs lecture on Transformers and their history or reading the original Attention is all you need paper (this is the paper that introduced the Transformer architecture).\nclassifier - This is what is going to take the representation of the data and compress it into our number of target classes (notice out_features=2, this means that we‚Äôll get two output numbers, one for each of our classes).\n\nFor more on the entire DistilBert architecture and its training setup, I‚Äôd recommend reading the DistilBert paper from the Hugging Face team.\nRather than breakdown the model itself, we‚Äôre focused on using it for a particular task (classifying text).\n\n6.1 Counting the parameters of our model\nBefore we move into training, we can get another insight into our model by counting its number of parameters.\nLet‚Äôs create a small function to count the number of trainable (these will update during training) and total parameters in our model.\n\ndef count_params(model):\n    \"\"\"\n    Count the parameters of a PyTorch model.\n    \"\"\"\n    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_parameters = sum(p.numel() for p in model.parameters())\n\n    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n\n# Count the parameters of the model\ncount_params(model)\n\n{'trainable_parameters': 66955010, 'total_parameters': 66955010}\n\n\nNice!\nLooks like our model has a total of 66,955,010 parameters and all of them are trainable.\nA parameter is a numerical value in a model which is capable of being updated to better represent the input data.\nI like to think of them as a small opportunity to learn patterns in the data.\nIf a model has three parameters, it has three small opportunities to learn patterns in the data.\nWhereas, if a model has 60,000,000+ (60M) parameters (like our model), it has 60,000,000+ small opportunities to learn patterns in the data.\nSome models such as Large Language Models (LLMs) like Llama 3 70B have 70,000,000,000+ (70B) parameters (over 1000x our model).\nIn essence, the more parameters a model has, the more opportunities it has to learn (generally).\nMore parameters often results in more capabilities.\nHowever, more parameters also often results in a much larger model size (e.g.¬†many gigabytes versus hundreds of megabytes) as well as a much longer compute time (fewer samples per second).\nFor our use case, a binary text classification task, 60M parameters is more than enough.\n\n\n\n\n\n\nNote\n\n\n\nWhy count the parameters in a model?\nWhile it may be tempting to always go with a model that has the most parameters, there are many considerations to take into account before doing so.\n\nWhat hardware is the model going to run on?\n\nIf you need the model to run on cheap hardware, you‚Äôll likely want a smaller model.\n\nHow fast do you need the model to be?\n\nIf you need 100-1000s of predictions per second, you‚Äôll likely want a smaller model.\n\n‚ÄúI don‚Äôt mind about speed or cost, I just want quality.‚Äù\n\nGo with the biggest model you can.\nHowever, often times you can get really good results by training a small model to do a specific task using quality data than by just always using a large model.\n\n\n\n\n6.2 Create a directory for saving models\nTraining a model can take a while.\nSo we‚Äôll want a place to save our models.\nLet‚Äôs create a directory called \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\" (it‚Äôs a bit verbose and you can change this if you like but I like to be specific).\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n\n6.3 Setting up training arguments with TrainingArguments\nTime to get our model ready for training!\nWe‚Äôre up to step 3 of our process:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nThe transformers.TrainingArguments class contains a series of helpful items, including hyperparameter settings and model saving strategies to use throughout training.\nIt has many parameters, too many to explain here.\nHowever, the following table breaks down a helpful handful.\nSome of the parameters we‚Äôll set are the same as the defaults (this is on purpose as the defaults are often pretty good), some such as learning_rate are different.\n\n\n\nParameter\nExplanation\n\n\n\n\noutput_dir\nName of output directory to save the model and checkpoints to. For example, learn_hf_food_not_food_text_classifier_model.\n\n\nlearning_rate\nValue of the initial learning rate to use during training. Passed to transformers.AdamW. Initial learning rate because the learning rate can be dynamic during training. The ideal learning is experimental in nature. Defaults to 5e-5 or 0.00001 but we‚Äôll use 0.0001.\n\n\nper_device_train_batch_size\nSize of batches to place on target device during training. For example, a batch size of 32 means the model will look at 32 samples at a time. A batch size too large will result in out of memory issues (e.g.¬†your GPU can‚Äôt handle holding a large number of samples in memory at a time).\n\n\nper_device_eval_batch_size\nSize of batches to place on target device during evaluation. Can often be larger than during training because no gradients are being calculated. For example, training batch size could be 32 where as evaluation batch size may be able to be 128 (4x larger). Though these are only esitmates.\n\n\nnum_train_epochs\nNumber of times to pass over the data to try and learn patterns. For example, if num_train_epochs=10, the model will do 10 full passes of the training data. Because we‚Äôre working with a small dataset, 10 epochs should be fine to begin with. However, if you had a larger dataset, you may want to do a few experiments using less data (e.g.¬†10% of the data) for a smaller number of epochs to make sure things work.\n\n\neval_strategy\nWhen to evaluate the model on the evaluation data. If eval_strategy=\"epoch\", the model will be evaluated every epoch. See the documentation for more options. Note: This was previously called evaluation_strategy but was shortened in transformers==4.46.\n\n\nsave_strategy\nWhen to save a model checkpoint. If save_strategy=\"epoch\", a checkpoint will be saved every epoch. See the documentation for more save options.\n\n\nsave_total_limit\nNumber of total amount of checkpoints to save (so we don‚Äôt save num_train_epochs checkpoints). For example, can limit to 3 saves so the total number of saves are the 3 most recent as well as the best performing checkpoint (as per load_best_model_at_end).\n\n\nuse_cpu\nSet to False by default, will use CUDA GPU (torch.device(\"cuda\")) or MPS device (torch.device(\"mps\"), for Mac) if available. This is because training is generally faster on an accelerator device.\n\n\nseed\nSet to 42 by default for reproducibility. Meaning that subsequent runs with the same setup should achieve the same results.\n\n\nload_best_model_at_end\nWhen set to True, makes sure that the best model found during training is loaded when training finishes. This will mean the best model checkpoint gets saved regardless of what epoch it happened on. This is set to False by default.\n\n\nlogging_strategy\nWhen to log the training results and metrics. For example, if logging_strategy=\"epoch\", results will be logged as outputs every epoch. See the documentation for more logging options.\n\n\nreport_to\nLog experiments to various experiment tracking services. For example, you can log to Weights & Biases using report_to=\"wandb\". We‚Äôll turn this off for now and keep logging to a local directory by setting report_to=\"none\".\n\n\npush_to_hub\nAutomatically upload the model to the Hugging Face Hub every time the model is saved. We‚Äôll set push_to_hub=False as we‚Äôll see how to do this manually later on. See the documentation for more options on saving models to the Hugging Face Hub.\n\n\nhub_token\nAdd your Hugging Face Hub token to push a model to the Hugging Face Hub with push_to_hub (will default to huggingface-cli login details).\n\n\nhub_private_repo\nWhether or not to make the Hugging Face Hub repository private or public, defaults to False (e.g.¬†set to True if you want the repository to be private).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo get more familiar with the transformers.TrainingArguments class, I‚Äôd highly recommend reading the documentation for 15-20 minutes. Perhaps over a couple of sessions. There are quite a large number of parameters which will be helpful to be aware of.\n\n\nPhew!\nThat was a lot to take in.\nBut let‚Äôs now practice setting up our own instance of transformers.TrainingArguments.\n\nfrom transformers import TrainingArguments\n\nprint(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n\n# Create training arguments\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir,\n    learning_rate=0.0001,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\", # was previously \"evaluation_strategy\"\n    save_strategy=\"epoch\",\n    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n    seed=42, # set to 42 by default for reproducibility\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\", # log training results every epoch\n    report_to=\"none\", # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n    hub_private_repo=False # optional: make the uploaded model private (defaults to False)\n)\n\n# Optional: Print out training_args to inspect (warning, it is quite a long output)\n# training_args\n\n[INFO] Saving model checkpoints to: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\nTraining arguments created!\nLet‚Äôs put them to work in an instance of transformers.Trainer.\n\n\n6.4 Setting up an instance of Trainer\nTime for step 4!\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nThe transformers.Trainer class allows you to train models.\nIt‚Äôs built on PyTorch so it gets to leverage all of the powerful PyTorch toolkit.\nBut since it also works closely with the transformers.TrainingArguments class, it offers many helpful features.\n\n\n\n\n\n\nNote\n\n\n\ntransformers.Trainer can work with torch.nn.Module models, however, it is designed to work best with transformers.PreTrainedModel‚Äôs from the transformers library.\nThis is not a problem for us as we‚Äôre using transformers.AutoModelForSequenceClassification.from_pretrained which loads a transformers.PreTrainedModel.\nSee the transformers.Trainer documentation for tips on how to make sure your model is compatible.\n\n\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nmodel\nThe model we‚Äôd like to train. Works best with an instance of transformers.PreTrainedModel. Most models loaded using from_pretrained will be of this type.\n\n\nargs\nInstance of transformers.TrainingArguments. We‚Äôll use the training_args object we defined earlier. But if this is not set, it will default to the default settings for transformers.TrainingArguments.\n\n\ntrain_dataset\nDataset to use during training. We can use our tokenized_dataset[\"train\"] as it has already been preprocessed.\n\n\neval_dataset\nDataset to use during evaluation (our model will not see this data during training). We can use our tokenized_dataset[\"test\"] as it has already been preprocessed.\n\n\ntokenizer\nThe tokenizer which was used to preprocess the data. Passing a tokenizer will also pad the inputs to maximum length when batching them. It will also be saved with the model so future re-runs are easier.\n\n\ncompute_metrics\nAn evaluation function to evaluate a model during training and evaluation steps. In our case, we‚Äôll use the compute_accuracy function we defined earlier.\n\n\n\nWith all this being said, let‚Äôs build our Trainer!\n\nfrom transformers import Trainer\n\n# Setup Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer, # Pass tokenizer to the Trainer for dynamic padding (padding as the training happens) (see \"data_collator\" in the Trainer docs)\n    compute_metrics=compute_accuracy\n)\n\nWoohoo! We‚Äôve created our own trainer.\nWe‚Äôre one step closer to training!\n\n\n6.5 Training our text classification model\nWe‚Äôve done most of the hard word setting up our transformers.TrainingArguments as well as our transformers.Trainer.\nNow how about we train a model?\nFollowing our steps:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nLooks like all we have to do is call transformers.Trainer.train().\nWe‚Äôll be sure to save the results of the training to a variable results so we can inspect them later.\nLet‚Äôs try!\n\n# Train a text classification model\nresults = trainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.327000\n0.034850\n1.000000\n\n\n2\n0.018300\n0.004165\n1.000000\n\n\n3\n0.003300\n0.001551\n1.000000\n\n\n4\n0.001500\n0.000902\n1.000000\n\n\n5\n0.000900\n0.000657\n1.000000\n\n\n6\n0.000800\n0.000544\n1.000000\n\n\n7\n0.000600\n0.000483\n1.000000\n\n\n8\n0.000600\n0.000450\n1.000000\n\n\n9\n0.000600\n0.000432\n1.000000\n\n\n10\n0.000600\n0.000426\n1.000000\n\n\n\n\n\n\nWoahhhh!!!\nHow cool is that!\nWe just trained a text classification model!\nAnd it looks like the training went pretty quick (thanks to our smaller dataset and relatively small model, for larger datasets, training would likely take longer).\nHow about we check some of the metrics?\nWe can do so using the results.metrics attribute (this returns a Python dictionary with stats from our training run).\n\n# Inspect training metrics\nfor key, value in results.metrics.items():\n    print(f\"{key}: {value}\")\n\ntrain_runtime: 6.9675\ntrain_samples_per_second: 287.048\ntrain_steps_per_second: 10.047\ntotal_flos: 18110777160000.0\ntrain_loss: 0.03540716338570097\nepoch: 10.0\n\n\nNice!\nLooks like our overall training runtime is low because of our small dataset.\nAnd looks like our trainer was able to process a fair few samples per second.\nIf we were to 1000x the size of our dataset (e.g.¬†~250 samples -&gt; ~250,000 samples which is quite a substantial dataset), it seems our training time still wouldn‚Äôt take too long.\nThe total_flos stands for ‚Äúfloating point operations‚Äù (also referred to as FLOPS), this is the total number of calculations our model has performed to find patterns in the data. And as you can see, it‚Äôs quite a large number!\n\n\n\n\n\n\nNote\n\n\n\nDepending on the hardware you‚Äôre using, the results with respect to train_runtime, train_samples_per_second and train_steps_per_second will likely be different.\nThe faster your accelerator hardware (e.g.¬†NVIDIA GPU or Mac GPU), the lower your runtime and higher your samples/steps per second will be.\nFor reference, on my local NVIDIA RTX 4090, I get a train_runtime of 8-9 seconds, train_samples_per_second of 230-250 and train_steps_per_second of 8.565.\n\n\n\n\n6.6 Save the model for later use\nNow our model has been trained, let‚Äôs save it for later use.\nWe‚Äôll save it locally first and push it to the Hugging Face Hub later.\nWe can save our model using the transformers.Trainer.save_model method.\n\n# Save model\nprint(f\"[INFO] Saving model to {model_save_dir}\")\ntrainer.save_model(output_dir=model_save_dir)\n\n[INFO] Saving model to models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\nModel saved locally! Before we save it to the Hugging Face Hub, let‚Äôs check out its metrics.\n\n\n6.7 Inspecting the model training metrics\nWe can get a log of our model‚Äôs training state using trainer.state.log_history.\nThis will give us a collection of metrics per epoch (as long as we set logging_strategy=\"epoch\" in transformers.TrainingArguments), in particular, it will give us a loss value per epoch.\nWe can extract these values and inspect them visually for a better understanding our model training.\nLet‚Äôs get the training history and inspect it.\n\n# Get training history \ntrainer_history_all = trainer.state.log_history \ntrainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics (we've seen these already)\ntrainer_history_training_time = trainer_history_all[-1] # this is the same value as results.metrics from above\n\n# View the first 4 metrics from the training history\ntrainer_history_metrics[:4]\n\n[{'loss': 0.327,\n  'grad_norm': 1.2806360721588135,\n  'learning_rate': 9e-05,\n  'epoch': 1.0,\n  'step': 7},\n {'eval_loss': 0.03485037386417389,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0128,\n  'eval_samples_per_second': 3911.429,\n  'eval_steps_per_second': 156.457,\n  'epoch': 1.0,\n  'step': 7},\n {'loss': 0.0183,\n  'grad_norm': 0.08053876459598541,\n  'learning_rate': 8e-05,\n  'epoch': 2.0,\n  'step': 14},\n {'eval_loss': 0.004165211692452431,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0127,\n  'eval_samples_per_second': 3927.839,\n  'eval_steps_per_second': 157.114,\n  'epoch': 2.0,\n  'step': 14}]\n\n\nOkay, looks like the metrics are logged every epochs in a list Python dictionaries with interleaving loss (this is the training set loss) and eval_loss values.\nHow about we write some code to separate the training set metrics and the evaluation set metrics?\n\nimport pprint # import pretty print for nice printing of lists\n\n# Extract training and evaluation metrics\ntrainer_history_training_set = []\ntrainer_history_eval_set = []\n\n# Loop through metrics and filter for training and eval metrics\nfor item in trainer_history_metrics:\n    item_keys = list(item.keys())\n    # Check to see if \"eval\" is in the keys of the item\n    if any(\"eval\" in item for item in item_keys):\n        trainer_history_eval_set.append(item)\n    else:\n        trainer_history_training_set.append(item)\n\n# Show the first two items in each metric set\nprint(f\"[INFO] First two items in training set:\")\npprint.pprint(trainer_history_training_set[:2])\n\nprint(f\"\\n[INFO] First two items in evaluation set:\")\npprint.pprint(trainer_history_eval_set[:2])\n\n[INFO] First two items in training set:\n[{'epoch': 1.0,\n  'grad_norm': 1.2806360721588135,\n  'learning_rate': 9e-05,\n  'loss': 0.327,\n  'step': 7},\n {'epoch': 2.0,\n  'grad_norm': 0.08053876459598541,\n  'learning_rate': 8e-05,\n  'loss': 0.0183,\n  'step': 14}]\n\n[INFO] First two items in evaluation set:\n[{'epoch': 1.0,\n  'eval_accuracy': 1.0,\n  'eval_loss': 0.03485037386417389,\n  'eval_runtime': 0.0128,\n  'eval_samples_per_second': 3911.429,\n  'eval_steps_per_second': 156.457,\n  'step': 7},\n {'epoch': 2.0,\n  'eval_accuracy': 1.0,\n  'eval_loss': 0.004165211692452431,\n  'eval_runtime': 0.0127,\n  'eval_samples_per_second': 3927.839,\n  'eval_steps_per_second': 157.114,\n  'step': 14}]\n\n\nBeautiful!\nHow about we take it a step further and turn our metrics into pandas DataFrames so we can view them easier?\n\n# Create pandas DataFrames for the training and evaluation metrics\ntrainer_history_training_df = pd.DataFrame(trainer_history_training_set)\ntrainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n\ntrainer_history_training_df.head() \n\n\n\n\n\n\n\n\nloss\ngrad_norm\nlearning_rate\nepoch\nstep\n\n\n\n\n0\n0.3270\n1.280636\n0.00009\n1.0\n7\n\n\n1\n0.0183\n0.080539\n0.00008\n2.0\n14\n\n\n2\n0.0033\n0.034253\n0.00007\n3.0\n21\n\n\n3\n0.0015\n0.019484\n0.00006\n4.0\n28\n\n\n4\n0.0009\n0.016791\n0.00005\n5.0\n35\n\n\n\n\n\n\n\nNice!\nAnd the evaluation DataFrame?\n\ntrainer_history_eval_df.head()\n\n\n\n\n\n\n\n\neval_loss\neval_accuracy\neval_runtime\neval_samples_per_second\neval_steps_per_second\nepoch\nstep\n\n\n\n\n0\n0.034850\n1.0\n0.0128\n3911.429\n156.457\n1.0\n7\n\n\n1\n0.004165\n1.0\n0.0127\n3927.839\n157.114\n2.0\n14\n\n\n2\n0.001551\n1.0\n0.0118\n4226.340\n169.054\n3.0\n21\n\n\n3\n0.000902\n1.0\n0.0120\n4170.283\n166.811\n4.0\n28\n\n\n4\n0.000657\n1.0\n0.0120\n4183.594\n167.344\n5.0\n35\n\n\n\n\n\n\n\nAnd of course, we‚Äôll have follow the data explorer‚Äôs motto of visualize, visualize, visualize! and inspect our loss curves.\n\n# Plot training and evaluation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\nplt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Text classification with DistilBert training and evaluation loss over time\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nB-e-a-utiful!\nThat is exactly what we wanted.\nTraining and evaluation loss going down over time.\n\n\n6.8 Pushing our model to the Hugging Face Hub\nWe‚Äôve saved our model locally and confirmed that it seems to be performing well on our training metrics but how about we push it to the Hugging Face Hub?\nThe Hugging Face Hub is one of the best sources of machine learning models on the internet.\nAnd we can add our model there so others can use it or we can access it in the future (we could also keep it private on the Hugging Face Hub so only people from our organization can use it).\nSharing models on Hugging Face is also a great way to showcase your skills as a machine learning engineer, it gives you something to show potential employers and say ‚Äúhere‚Äôs what I‚Äôve done‚Äù.\n\n\n\n\n\n\nNote\n\n\n\nBefore sharing a model to the Hugging Face Hub, be sure to go through the following steps:\n\nSetup a Hugging Face token using the huggingface-cli login command.\nRead through the user access tokens guide.\nSet up an access token via https://huggingface.co/settings/tokens (ensure it has ‚Äúwrite‚Äù access).\n\nIf you are using Google Colab, you can add your token under the ‚ÄúSecrets‚Äù tab on the left.\nOn my local computer, my token is saved to /home/daniel/.cache/huggingface/token (thanks to running huggingface-cli login on the command line).\nAnd for more on sharing models to the Hugging Face Hub, be sure to check out the model sharing documentation.\n\n\nWe can push our model, tokenizer and other assosciated files to the Hugging Face Hub using the transformers.Trainer.push_to_hub method.\nWe can also optionally do the following:\n\nAdd a model card (something that describes how the model was created and what it can be used for) using transformers.Trainer.create_model_card.\nAdd a custom README.md file to the model repository to explain more details about the model using huggingface_hub.HfApi.upload_file. This method is similar to model card creation method above but with more customization.\n\nLet‚Äôs save our model to the Hub!\n\n# Save our model to the Hugging Face Hub\n# This will be public, since we set hub_private_repo=False in our TrainingArguments\ntrainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\",\n    # token=\"YOUR_HF_TOKEN_HERE\" # This will default to the token you have saved in your Hugging Face config\n)\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/90630c6063d2498e794ba6c982f1688ee3bcf969', commit_message='Uploading food not food text classifier model', commit_description='', oid='90630c6063d2498e794ba6c982f1688ee3bcf969', pr_url=None, pr_revision=None, pr_num=None)\n\n\nModel pushed to the Hugging Face Hub!\n\n\n\n\n\n\nNote\n\n\n\nYou may see the following error:\n\n403 Forbidden: You don‚Äôt have the rights to create a model under the namespace ‚Äúmrdbourke‚Äù. Cannot access content at: https://huggingface.co/api/repos/create. If you are trying to create or update content, make sure you have a token with the write role.\n\nOr even:\n\nHfHubHTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6699c52XXXXXX)\nInvalid username or password.\n\nIn this case, be sure to go through the setup steps above to make sure you have a Hugging Face access token with ‚Äúwrite‚Äù access.\n\n\nAnd since it‚Äôs public (by default), you can see it at https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased (it gets saved to the same name as our target local directory).\nYou can now share and interact with this model online.\nAs well as download it for use in your own applications.\n\nTK image - model on Hugging Face Hub ready to use/example of using the model already in the Hugging Face models page\n\nBut before we make an application with it, let‚Äôs keep evaluating it.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-and-evaluating-predictions-on-the-test-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-and-evaluating-predictions-on-the-test-data",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "7 Making and evaluating predictions on the test data",
    "text": "7 Making and evaluating predictions on the test data\nModel trained, let‚Äôs now evaluate it on the test data.\nOr step 7 in our workflow:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\n‚úÖ Train the model by calling Trainer.train().\n‚úÖ Save the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nA reminder that the test data is data that our model has never seen before.\nSo it will be a good estimate of how our model will do in a production setting.\nWe can make predictions on the test dataset using transformers.Trainer.predict.\nAnd then we can get the prediction values with the predictions attribute and assosciated metrics with the metrics attribute.\n\n# Perform predictions on the test set\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_values = predictions_all.predictions\nprediction_metrics = predictions_all.metrics\n\nprint(f\"[INFO] Prediction metrics on the test data:\")\nprediction_metrics\n\n\n\n\n[INFO] Prediction metrics on the test data:\n\n\n{'test_loss': 0.00042641552863642573,\n 'test_accuracy': 1.0,\n 'test_runtime': 0.0423,\n 'test_samples_per_second': 1181.814,\n 'test_steps_per_second': 47.273}\n\n\nWoah!\nLooks like our model did an outstanding job!\nAnd it was very quick too.\nThis is one of the benefits of using a smaller pretrained model and customizing it to your own dataset.\nYou can achieve outstanding results in a very quick time as well as have a model capable of performing thousands of predictions per second.\nWe can also calculate the accuracy by hand by comparing the prediction labels to the test labels.\nTo do so, we‚Äôll:\n\nCalculate the prediction probabilities (though this is optional as we could skip straight to 2 and get the same results) by passing the prediction_values to torch.softmax.\nFind the index of the prediction value with the highest value (the index will be equivalent to the predicted label) using torch.argmax (we could also use np.argmax here) to find the predicted labels.\nGet the true labels from the test dataset using dataset[\"test\"][\"label\"].\nCompare the predicted labels from 2 to the true labels from 3 using sklearn.metrics.accuracy_score to find the accuracy.\n\n\nimport torch\nfrom sklearn.metrics import accuracy_score\n\n# 1. Get prediction probabilities (this is optional, could get the same results with step 2 onwards)\npred_probs = torch.softmax(torch.tensor(prediction_values), dim=1)\n\n# 2. Get the predicted labels\npred_labels = torch.argmax(pred_probs, dim=1)\n\n# 3. Get the true labels\ntrue_labels = dataset[\"test\"][\"label\"]\n\n# 4. Compare predicted labels to true labels to get the test accuracy\ntest_accuracy = accuracy_score(y_true=true_labels, \n                               y_pred=pred_labels)\n\nprint(f\"[INFO] Test accuracy: {test_accuracy*100}%\")\n\n[INFO] Test accuracy: 100.0%\n\n\nWoah!\nLooks like our model performs really well on our test set.\nIt will be interesting to see how it goes on real world samples.\nWe‚Äôll test this later on.\nHow about we make a pandas DataFrame out of our test samples, predicted labels and predicted probabilities to further inspect our results?\n\n# Make a DataFrame of test predictions\ntest_predictions_df = pd.DataFrame({\n    \"text\": dataset[\"test\"][\"text\"],\n    \"true_label\": true_labels,\n    \"pred_label\": pred_labels,\n    \"pred_prob\": torch.max(pred_probs, dim=1).values\n})\n\ntest_predictions_df.head()\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.999552\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.999589\n\n\n2\nA bowl of sliced bell peppers with a sprinkle ...\n1\n1\n0.999555\n\n\n3\nSet of mugs hanging on a hook\n0\n0\n0.999628\n\n\n4\nStanding floor lamp providing light next to an...\n0\n0\n0.999625\n\n\n\n\n\n\n\nWe can find the examples with the lowest prediction probability to see where the model is unsure.\n\n# Show 10 examples with low prediction probability\ntest_predictions_df.sort_values(\"pred_prob\", ascending=True).head(10)\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n40\nA bowl of cherries with a sprig of mint for ga...\n1\n1\n0.999523\n\n\n11\nA close-up shot of a cheesy pizza slice being ...\n1\n1\n0.999540\n\n\n14\nTwo handfuls of bananas in a fruit bowl with g...\n1\n1\n0.999545\n\n\n26\nA fruit platter with a variety of exotic fruit...\n1\n1\n0.999549\n\n\n43\nSet of muffin tins stacked together\n0\n0\n0.999549\n\n\n42\nBoxes of apples, pears, pineapple, manadrins a...\n1\n1\n0.999551\n\n\n20\nPizza with a seafood theme, featuring toppings...\n1\n1\n0.999552\n\n\n18\nTraditional Japanese flavored sushi roll with ...\n1\n1\n0.999552\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.999552\n\n\n41\nSushi with a spicy kick, featuring jalapeno pe...\n1\n1\n0.999553\n\n\n\n\n\n\n\nHmmm, it looks like our model has quite a high prediction probability for almost all samples.\nWe can further evalaute our model by making predictions on new custom data.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-and-inspecting-predictions-on-custom-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-and-inspecting-predictions-on-custom-text-data",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "8 Making and inspecting predictions on custom text data",
    "text": "8 Making and inspecting predictions on custom text data\nWe‚Äôve seen how our model performs on the test dataset (quite well).\nBut how might we check its performance on our own custom data?\nFor example, text-based image captions from the wild.\nWell, we‚Äôve got two ways to load our model now too:\n\nLoad model locally from our computer (e.g.¬†via models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\nLoad model from Hugging Face Hub (e.g.¬†via mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\n\nEither way of loading the model results in the same outcome: being able to make predictions on given data.\nSo how about we start by setting up our model paths for both local loading and loading from the Hugging Face Hub.\n\n# Setup local model path\nlocal_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\nhuggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n\n8.1 Discussing ways to make predictions (inference)\nWhen we‚Äôve loaded our trained model, because of the way we‚Äôve set it up, there are two main ways to make predictions on custom data:\n\nPipeline mode using transformers.pipeline and passing it our target model, this allows us to preprocess custom data and make predictions in one step.\nPyTorch mode using a combination of transformers.AutoTokenizer and transformers.AutoModelForSequenceClassification and passing each our target model, this requires us to preprocess our data before passing to a model, however, it offers the most customization.\n\nEach method supports:\n\nPredictions one at a time (batch size of 1), for example, one person using the app at a time.\nBatches of predictions at a time (predictions with a batch size of n where n can be any number, e.g.¬†8, 16, 32), for example, many people using a service simultaneously such as a voice chat and needing to filter comments (predicting on batches of size n is usually much faster than batches of 1).\n\nWhichever method we choose, we‚Äôll have to set the target device we‚Äôd like the operations to happen on.\nIn general, it‚Äôs best to make predictions on the most powerful accelerator you have available.\nAnd in most cases that will be a NVIDIA GPU &gt; Mac GPU &gt; CPU.\nSo let‚Äôs write a small function to pick the target device for us in that order.\n\n\n\n\n\n\nNote\n\n\n\nMaking predictions is also referred to as inference.\nBecause the model is going to infer on some data what the output should be.\nInference is often faster than training on a per sample basis as no model weights are updated (less computation).\nHowever, inference can use more compute than training over the long run because you could train a model once over a few hours (or days or longer) and then use it for inference for several months (or longer), millions of times (or more).\n\n\n\ndef set_device():\n    \"\"\"\n    Set device to CUDA if available, else MPS (Mac), else CPU.\n\n    This defaults to using the best available device (usually).\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n\nDEVICE = set_device()\nprint(f\"[INFO] Using device: {DEVICE}\")\n\n[INFO] Using device: cuda\n\n\nTarget device set!\nLet‚Äôs start predicting.\n\n\n8.2 Making predictions with pipeline\nThe transformers.pipeline method creates a machine learning pipeline.\nData goes in one end and predictions come out the other end.\nYou can create pipelines for many different tasks, such as, text classification, image classification, object detection, text generation and more.\nLet‚Äôs see how we can create a pipeline for our text classification model.\nTo do so we‚Äôll:\n\nInstantiate an instance of transformers.pipeline.\nPass in the task parameter of text-classification (we can do this because our model is already formatted for text classification thanks to using transformers.AutoModelForSequenceClassification).\nSetup the model parameter to be local_model_path (though we could also use huggingface_model_path).\nSet the target device using the device parameter.\nSet top_k=1 to get to the top prediction back (e.g.¬†either \"food\" or \"not_food\", could set this higher to get more labels back).\nSet the BATCH_SIZE=32 so we can pass to the batch_size parameter. This will allow our model to make predictions on up to 32 samples at a time. Predicting on batches of data is usually much faster than single samples at a time, however, this often saturates at a point (e.g.¬†predicting on batches of size 64 may be the same speed as 32 due to memory contraints).\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many more pipelines available in the Hugging Face documentation.\nAs an exericse, I‚Äôd spend 10-15 minutes reading through the pipeline documentation to get familiar with what‚Äôs available.\n\n\nLet‚Äôs setup our pipeline!\n\nimport torch\nfrom transformers import pipeline\n\n# Set the batch size for predictions\nBATCH_SIZE = 32\n\n# Create an instance of transformers.pipeline\nfood_not_food_classifier = pipeline(task=\"text-classification\", # we can use this because our model is an instance of AutoModelForSequenceClassification\n                                    model=local_model_path, # could also pass in huggingface_model_path\n                                    device=DEVICE, # set the target device\n                                    top_k=1, # only return the top predicted value\n                                    batch_size=BATCH_SIZE) # perform predictions on up to BATCH_SIZE number of samples at a time \n\nfood_not_food_classifier\n\n&lt;transformers.pipelines.text_classification.TextClassificationPipeline at 0x7f33da4ed3d0&gt;\n\n\nWe‚Äôve created an instance of transformers.pipelines.text_classification.TextClassificationPipeline!\nNow let‚Äôs test it out by passing it a string of text about food.\n\n# Test our trained model on some example text \nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\nfood_not_food_classifier(sample_text_food)\n\n[[{'label': 'food', 'score': 0.9995404481887817}]]\n\n\nNice! Our model gets it right.\nHow about a string not about food?\n\n# Test the model on some more example text\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[[{'label': 'not_food', 'score': 0.9995775818824768}]]\n\n\nWoohoo!\nCorrect again!\nWhat if we passed in random text?\nAs in, someone types in something random to the model expecting an output.\n\n# Pass in random text to the model\nfood_not_food_classifier(\"cvnhertiejhwgdjshdfgh394587\")\n\n[[{'label': 'not_food', 'score': 0.9935680031776428}]]\n\n\nThe nature of machine learning models is that they are a predictive/generative function.\nIf you input data, they will output something.\n\n\n\n\n\n\nNote\n\n\n\nWhen deploying machine learning models, there are many things to take into consideration.\nOne of the main ones being: ‚Äúwhat data is going to go into the model?‚Äù\nIf this was a public facing model and people could enter any kind of text, they could enter random text rather than a sentence about food or not food.\nSince our main goal of the model is be able to classify image captions into food/not_food, we‚Äôd also have to consider image cpations that are poorly written or contain little text.\nThis is why it‚Äôs important to continually test your models with as much example test/real-world data as you can.\n\n\nOur pipeline can also work with the model we saved to the Hugging Face Hub.\nLet‚Äôs try out the same pipeline with model=hugggingface_model_path.\n\n# Pipeline also works with remote models (will have to laod the model locally first)\nfood_not_food_classifier_remote = pipeline(task=\"text-classification\", \n                                           model=huggingface_model_path, # load the model from Hugging Face Hub (will download the model if it doesn't already exist)\n                                           batch_size=BATCH_SIZE,\n                                           device=DEVICE)\n\nfood_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")\n\n\n\n\n\n\n\n\n\n\n[{'label': 'food', 'score': 0.9995065927505493}]\n\n\nBeautiful!\nOur model loaded from Hugging Face gets it right too!\n\n\n8.3 Making multiple predictions at the same time with batch prediction\nWe can make predictions with our model one at a time but it‚Äôs often much faster to do them in batches.\nTo make predictions in batches, we can set up our transformers.pipeline instance with the batch_size parameter greater than 1.\nThen we‚Äôll be able to pass multiple samples at once in the form of a Python list.\n\n# Create batch size (we don't need to do this again but we're doing it for clarity)\nBATCH_SIZE = 32 # this number is experimental and will require testing on your hardware to find the optimal value (e.g. lower if there are memory issues or higher to try speed up inference)\n\n# Setup pipeline to handle batches (we don't need to do this again either but we're doing it for clarity)\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=local_model_path,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE)\n\nWonderful, now we‚Äôve set up a pipeline instance capable of handling batches, we can pass it a list of samples and it will make predictions on each.\nHow about we try with a collection of sentences which are a bit tricky?\n\n# Create a list of sentences to make predictions on\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"A beautiful array of fake wax foods (shokuhin sampuru) in the front of a Japanese restaurant.\",\n    \"Daniel Bourke is really cool :D\",\n    \"My favoruite food is biltong!\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9972656965255737},\n {'label': 'not_food', 'score': 0.998143196105957},\n {'label': 'not_food', 'score': 0.9920535087585449},\n {'label': 'not_food', 'score': 0.997535228729248},\n {'label': 'not_food', 'score': 0.9985295534133911},\n {'label': 'not_food', 'score': 0.9983918070793152},\n {'label': 'not_food', 'score': 0.7593845725059509},\n {'label': 'food', 'score': 0.9995193481445312},\n {'label': 'not_food', 'score': 0.9990437626838684},\n {'label': 'food', 'score': 0.9853901863098145}]\n\n\nWoah! That was quick!\nAnd it looks like our model performed fairly well.\nThough there was one harder sample which may be deemed as food/not_food, the sentence containing ‚Äúshokuhin sampuru‚Äù (meaning ‚Äúfood model‚Äù in Japanese).\nIs a sentence about food models (fake foods) still about food?\n\n\n8.4 Time our model across larger sample sizes\nWe can say that our model is fast or that making predictions in batches is faster than one at a time.\nBut how about we run some tests to confirm this?\nLet‚Äôs start by making predictions one at a time across 100 sentences (10x our sentences list) and then we‚Äôll write some code to make predictions in batches.\nWe‚Äôll time each and see how they go.\n\nimport time\n\n# Create 1000 sentences\nsentences_1000 = sentences * 100\n\n# Time how long it takes to make predictions on all sentences (one at a time)\nprint(f\"[INFO] Number of sentences: {len(sentences_1000)}\")\nstart_time_one_at_a_time = time.time()\nfor sentence in sentences_1000:\n    # Make a prediction on each sentence one at a time\n    food_not_food_classifier(sentence)\nend_time_one_at_a_time = time.time()\n\nprint(f\"[INFO] Time taken for one at a time prediction: {end_time_one_at_a_time - start_time_one_at_a_time} seconds\")\nprint(f\"[INFO] Avg inference time per sentence: {(end_time_one_at_a_time - start_time_one_at_a_time) / len(sentences_100)} seconds\")\n\n[INFO] Number of sentences: 1000\n[INFO] Time taken for one at a time prediction: 5.6913557052612305 seconds\n[INFO] Avg inference time per sentence: 0.005691355705261231 seconds\n\n\nOk, on my local NVIDIA RTX 4090 GPU, it took around 5.5 seconds to make 1000 predictions one at a time.\nThat‚Äôs pretty good!\nBut let‚Äôs see if we can make it faster with batching.\nTo do so, we can increase the size of our sentences_big list and pass the list directly to the model to enable batched prediction.\n\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    # Predict on all sentences in batches \n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 100\n[INFO] Inference time for 100 sentences: 0.15167 seconds.\n[INFO] Avg inference time per sentence: 0.0015167 seconds.\n\n[INFO] Number of sentences: 1000\n[INFO] Inference time for 1000 sentences: 0.27526 seconds.\n[INFO] Avg inference time per sentence: 0.00027526 seconds.\n\n[INFO] Number of sentences: 10000\n[INFO] Inference time for 10000 sentences: 3.32303 seconds.\n[INFO] Avg inference time per sentence: 0.0003323 seconds.\n\n[INFO] Number of sentences: 100000\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[77], line 7\n      5 start_time = time.time()\n      6 # Predict on all sentences in batches \n----&gt; 7 food_not_food_classifier(sentences_big)\n      8 end_time = time.time()\n     10 print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:156, in TextClassificationPipeline.__call__(self, inputs, **kwargs)\n    122 \"\"\"\n    123 Classify the text(s) given as inputs.\n    124 \n   (...)\n    153     If `top_k` is used, one such dictionary is returned per label.\n    154 \"\"\"\n    155 inputs = (inputs,)\n--&gt; 156 result = super().__call__(*inputs, **kwargs)\n    157 # TODO try and retrieve it in a nicer way from _sanitize_parameters.\n    158 _legacy = \"top_k\" not in kwargs\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1224, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n   1220 if can_use_iterator:\n   1221     final_iterator = self.get_iterator(\n   1222         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n   1223     )\n-&gt; 1224     outputs = list(final_iterator)\n   1225     return outputs\n   1226 else:\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self)\n    121     return self.loader_batch_item()\n    123 # We're out of items within a batch\n--&gt; 124 item = next(self.iterator)\n    125 processed = self.infer(item, **self.params)\n    126 # We now have a batch of \"inferred things\".\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self)\n    123 # We're out of items within a batch\n    124 item = next(self.iterator)\n--&gt; 125 processed = self.infer(item, **self.params)\n    126 # We now have a batch of \"inferred things\".\n    127 if self.loader_batch_size is not None:\n    128     # Try to infer the size of the batch\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1151, in Pipeline.forward(self, model_inputs, **forward_params)\n   1149         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n   1150         model_outputs = self._forward(model_inputs, **forward_params)\n-&gt; 1151         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(\"cpu\"))\n   1152 else:\n   1153     raise ValueError(f\"Framework {self.framework} is not supported\")\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1051, in Pipeline._ensure_tensor_on_device(self, inputs, device)\n   1048 def _ensure_tensor_on_device(self, inputs, device):\n   1049     if isinstance(inputs, ModelOutput):\n   1050         return ModelOutput(\n-&gt; 1051             {name: self._ensure_tensor_on_device(tensor, device) for name, tensor in inputs.items()}\n   1052         )\n   1053     elif isinstance(inputs, dict):\n   1054         return {name: self._ensure_tensor_on_device(tensor, device) for name, tensor in inputs.items()}\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1051, in &lt;dictcomp&gt;(.0)\n   1048 def _ensure_tensor_on_device(self, inputs, device):\n   1049     if isinstance(inputs, ModelOutput):\n   1050         return ModelOutput(\n-&gt; 1051             {name: self._ensure_tensor_on_device(tensor, device) for name, tensor in inputs.items()}\n   1052         )\n   1053     elif isinstance(inputs, dict):\n   1054         return {name: self._ensure_tensor_on_device(tensor, device) for name, tensor in inputs.items()}\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1062, in Pipeline._ensure_tensor_on_device(self, inputs, device)\n   1060     return tuple([self._ensure_tensor_on_device(item, device) for item in inputs])\n   1061 elif isinstance(inputs, torch.Tensor):\n-&gt; 1062     return inputs.to(device)\n   1063 else:\n   1064     return inputs\n\nKeyboardInterrupt: \n\n\n\nWoah!\nIt looks like inference/prediction time is ~10-20x faster when using batched prediction versus predicting one at a time (on my local NVIDIA RTX 4090).\nI ran some more tests with the same model on a different GPU on Google Colab (NVIDIA L4 GPU) and got similar results.\n\n\n\nNumber of Sentences\nTotal Prediction Time\nPrediction Type\n\n\n\n\n100\n0.62\none at a time\n\n\n1000\n6.19\none at a time\n\n\n10000\n61.08\none at a time\n\n\n100000\n605.46\none at a time\n\n\n100\n0.06\nbatch\n\n\n1000\n0.51\nbatch\n\n\n10000\n4.97\nbatch\n\n\n100000\n49.7\nbatch\n\n\n\nTesting the speed of a custom text classifier model on different numbers of sentences with one at a time or batched prediction. Tests conducted on Google Colab with a NVIDIA L4 GPU. See the notebook for code to reproduce.\n\n\n8.5 Making predictions with PyTorch\nWe‚Äôve seen how to make predictions/perform inference with transformers.pipeline, now let‚Äôs see how to do the same with PyTorch.\nPerforming predictions with PyTorch requires an extra step compared to pipeline, we have to prepare our inputs first (turn the text into numbers).\nGood news is, we can prepare our inputs with the tokenizer that got automatically saved with our model.\nAnd since we‚Äôve already trained a model and uploaded it to the Hugging Face Hub, we can load our model and tokenizer with transformers.AutoTokenizer and transformers.AutoModelForSequenceClassification passing it the saved path we used (mine is mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\nLet‚Äôs start by loading the tokenizer and see what it looks like to tokenize a piece of sample text.\n\nfrom transformers import AutoTokenizer\n\n# Setup model path (can be local or on Hugging Face)\nmodel_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create an example to predict on\nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n# Prepare the tokenizer and tokenize the inputs\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\ninputs = tokenizer(sample_text_food, \n                   return_tensors=\"pt\") # return the output as PyTorch tensors \ninputs\n\n{'input_ids': tensor([[  101,  1037, 12090,  6302,  1997,  1037,  5127,  1997, 13501,  6763,\n          1010, 11611,  1998, 15174,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\nNice!\nText tokenized!\nWe get a dictionary of input_ids (our text in token form) and attention_mask (tells the model which tokens to pay attention to, 1 = pay attention, 0 = no attention).\nNow we can load the model with the same path.\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Load our text classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n\nModel loaded!\nLet‚Äôs make a prediction.\nWe can do so using the context manager torch.no_grad() (because no gradients/weights get updated during inference) and passing our model out inputs dictionary.\n\n\n\n\n\n\nNote\n\n\n\nA little tidbit about using dictionaries as function inputs in Python is the ability to unpack the keys of the dictionary into function arguments.\nThis is possible using **TARGET_DICTIONARY syntax. Where the ** means ‚Äúuse all the keys in the dictionary as function parameters‚Äù.\nFor example, the following two lines are equivalent:\n# Using ** notation\noutputs = model(**inputs)\n\n# Using explicit notation\noutputs = model(input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"])\n\n\nLet‚Äôs make a prediction with PyTorch!\n\nimport torch\n\nwith torch.no_grad():\n    outputs = model(**inputs) # '**' means input all of the dictionary keys as arguments to the function\n    # outputs = model(input_ids=inputs[\"input_ids\"],\n    #                 attention_mask=inputs[\"attention_mask\"]) # same as above, but explicitly passing in the keys\n\noutputs\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-3.4825,  4.2022]]), hidden_states=None, attentions=None)\n\n\nBeautiful, we‚Äôve got some outputs, which contain logits with two values (one for each class).\nThe index of the higher value is our model‚Äôs predicted class.\nWe can find it by taking the outputs.logits and calling argmax().item() on it.\nWe can also find the prediction probability by passing outputs.logits to torch.softmax.\n\n# Get predicted class and prediction probability\npredicted_class_id = outputs.logits.argmax().item()\nprediction_probability = torch.softmax(outputs.logits, dim=1).max().item()\n\nprint(f\"Text: {sample_text_food}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\nprint(f\"Prediction probability: {prediction_probability}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food\nPrediction probability: 0.9995404481887817\n\n\nBeautiful! A prediction made with pure PyTorch! It looks very much correct too.\nHow about we put it all together?\n\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n\n# Make sample text and tokenize it\nsample_text = \"A photo of a broccoli, salmon, rice and radish dish\"\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n# Make a prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get predicted class and prediction probability\noutput_logits = outputs.logits\npredicted_class_id = torch.argmax(output_logits, dim=1).item()\npredicted_class_label = model.config.id2label[predicted_class_id]\npredicted_probability = torch.softmax(output_logits, dim=1).max().item()\n\n# Print outputs\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted class: {predicted_class_label} (prob: {predicted_probability * 100:.2f}%)\")\n\nText: A photo of a broccoli, salmon, rice and radish dish\nPredicted class: food (prob: 99.96%)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 Turning our model into a demo",
    "text": "9 Turning our model into a demo\nOnce you‚Äôve trained and saved a model, one of the best ways to continue to test it and show/share it with others is to create a demo.\nOr step number 8 in our workflow:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\n‚úÖ Train the model by calling Trainer.train().\n‚úÖ Save the model (to our local machine or to the Hugging Face Hub).\n‚úÖ Evaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nA demo is a small application with the focus of showing the workflow of your model from data in to data out.\nIt‚Äôs also one way to start testing your model in the wild.\nYou may know where it works and where it doesn‚Äôt but chances are someone out there will find a new bug before you do.\nTo build our demo, we‚Äôre going to use an open-source library called Gradio.\nGradio allows you to make machine learning demo apps with Python code and best of all, it‚Äôs part of the Hugging Face ecosystem so you can share your demo to the public directly through Hugging Face.\nTK image - showcase workflow of data -&gt; model -&gt; save -&gt; demo -&gt; share\nGradio works on the premise of input -&gt; function (this could be a model) -&gt; output.\nIn our case:\n\nInput = A string of text.\nFunction = Our trained text classification model.\nOutput = Predicted output of food/not_food with prediction probability.\n\n\n9.1 Creating a simple function to perform inference\nLet‚Äôs create a function to take an input of text, process it with our model and return a dictionary of the predicted labels.\nOur function will:\n\nTake an input of a string of text.\nSetup a text classification pipeline using transformers.pipeline as well as our trained model (this can be from our local machine or loaded from Hugging Face). We‚Äôll return all the probabilities from the output using top_k=None.\nGet the outputs of the text classification pipeline from 2 as a list of dictionaries (e.g.¬†[{'label': 'food', 'score': 0.999105}, {'label': 'not_food', 'score': 0.00089}]).\nFormat and return the list of dictionaries from 3 to be compatible with Gradio‚Äôs gr.Label output (we‚Äôll see this later) which requires a dictionary in the form [{\"label_1\": probability_1, \"label_2\": probability_2}].\n\nOnward!\n\nfrom typing import Dict\n\n# 1. Create a function which takes text as input \ndef food_not_food_classifier(text: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Takes an input string of text and classifies it into food/not_food in the form of a dictionary.\n    \"\"\"\n\n    # 2. Setup the pipeline to use the local model (or Hugging Face model path)\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=local_model_path,\n                                        batch_size=32,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\", # set the device to work in any environment\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # 3. Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n    \n    # 4. Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\n# Test out the function\nfood_not_food_classifier(\"My lunch today was chicken and salad\")\n\n{'food': 0.9991052746772766, 'not_food': 0.0008946915622800589}\n\n\nBeautiful!\nLooks like our function is working.\n\n\n9.2 Building a small Gradio demo to run locally\nWe‚Äôve got a working function to go from text to predicted labels and probabilities.\nLet‚Äôs now build a Gradio interface to showcase our model.\nWe can do so by:\n\nImporting Gradio (using import gradio as gr).\nCreating an instance of gr.Interface with parameters inputs=\"text\" (for our text-based inputs) called demo and outputs=gr.Label(num_top_classes=2) to display our output dictionary. We can also add some descriptive aspects to our demo with the title, description and examples parameters.\nRunning/launching the demo with gr.Interface.launch().\n\n\n# 1. Import Gradio as the common alias \"gr\"\nimport gradio as gr\n\n# 2. Setup a Gradio interface to accept text and output labels\ndemo = gr.Interface(\n    fn=food_not_food_classifier, \n    inputs=\"text\", \n    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n    title=\"Food or Not Food Classifier\",\n    description=\"A text classifier to determine if a sentence is about food or not food.\",\n    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\n# 3. Launch the interface\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7863\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\nWoohoo!\nWe‚Äôve made a very clean way of interacting with our model.\nHowever, our model is still only largely accessible to us (except for the model file we‚Äôve uploaded to Hugging Face).\nHow about we make our demo publicly available so it‚Äôs even easier for people to interact with our model?\n\n\n\n\n\n\nNote\n\n\n\nThe gradio.Interface class is full of many different options, I‚Äôd highly recommend reading through the documentation for 10-15 minutes to get an idea of it.\nIf your workflow requires inputs -&gt; function (e.g.¬†a model making predictions on the input) -&gt; output, chances are, you can build it with Gradio.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-our-demo-publicly-accessible",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-our-demo-publicly-accessible",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "10 Making our demo publicly accessible",
    "text": "10 Making our demo publicly accessible\nOne of the best ways to share your machine learning work is by creating an application.\nAnd one of the best places to share your applications is Hugging Face Spaces.\nHugging Face Spaces allows you to host machine learning (and non-machine learning) applications for free (with optional paid hardware upgrades).\nIf you‚Äôre familiar with GitHub, Hugging Face Spaces works similar to a GitHub repository (each Space is a Git repository itself).\nIf not, that‚Äôs okay, think of Hugging Face Spaces as an online folder where you can upload your files and have them accessed by others.\nCreating a Hugging Face Space can be done in two main ways:\n\nManually - By going to the Hugging Face Spaces website and clicking ‚ÄúCreate new space‚Äù. Or by going directly to https://www.huggingface.co/new-space. Here, you‚Äôll be able to setup a few settings for your Space and choose the framework/runtime (e.g.¬†Streamlit, Gradio, Docker and more).\nProgrammatically - By using the Hugging Face Hub Python API we can write code to directly upload files to the Hugging Face Hub, including Hugging Face Spaces.\n\nBoth are great options but we‚Äôre going to take the second approach.\nThis is so we can create our Hugging Face Space right from this notebook.\nTo do so, we‚Äôll create three files:\n\napp.py - This will be the Python file which will be the main running file on our Hugging Face Space. Inside we‚Äôll include all the code necessary to run our Gradio demo (as above). Hugging Face Spaces will automatically recoginize the app.py file and run it for us.\nrequirements.txt - This text file will include all of the Python packages we need to run our app.py file. Before our Space starts to run, all of the packages in this file will be installed.\nREADME.md - This markdown file will include details about our Space as well as specific Space-related metadata (we‚Äôll see this later on).\n\nWe‚Äôll create these files with the following file structure:\ndemos/\n‚îî‚îÄ‚îÄ food_not_food_text_classifier/\n    ‚îú‚îÄ‚îÄ app.py\n    ‚îú‚îÄ‚îÄ README.md\n    ‚îî‚îÄ‚îÄ requirements.txt\nWhy this way?\nDoing it in the above style means we‚Äôll have a directory which contains all of our demos (demos/) as well as a dedicated directory which contains our food/not_food demo application (food_not_food_text_classifier/).\nThis way, we‚Äôll be able to upload the whole demos/food_not_food_text_classifier/ folder to Hugging Face Spaces.\nLet‚Äôs start by making a directory to store our demo application files.\n\nfrom pathlib import Path\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\nDemo directory created, let‚Äôs now create our requried files.\n\n10.1 Making an app file\nOur app.py file will be the main part of our Hugging Face Space.\nThe good news is, we‚Äôve already created most of it when we created our original demo.\nInside the app.py folder we‚Äôll:\n\nImport the required libraries/packages for running our demo app.\nSetup a function for going from text to our trained model‚Äôs predicted outputs. And because our model is already hosted on the Hugging Face Hub, we can pass pipeline our model‚Äôs name (e.g.¬†mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased) and when we upload our app.py file to Hugging Face Spaces, it will load the model directly from the Hub.\nCreate a demo just as before with gr.Interface.\nLaunch our demo with gr.Interface.launch.\n\nWe can write all of the above in a notebook cell.\nAnd we can turn it into a file by using the %%writefile magic command and passing it our target filepath.\nLet‚Äôs do it!\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\n# 1. Import the required packages\nimport torch\nimport gradio as gr\n\nfrom typing import Dict\nfrom transformers import pipeline\n\n# 2. Define function to use our model on given text \ndef food_not_food_classifier(text: str) -&gt; Dict[str, float]:\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        # Because our model is on Hugging Face already, we can pass in the model name directly\n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\n# 3. Create a Gradio interface with details about our app\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food. \n\nFine-tuned from [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on a [small dataset of food and not food text](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nTK - See source code:\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\n# 4. Launch the interface\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\napp.py file created!\nNow let‚Äôs setup the requirements file.\n\n\n10.2 Making a requirements file\nWhen you upload an app.py file to Hugging Face Spaces, it will attempt to run it automatically.\nAnd just like running the file locally, we need to make sure all of the required packages are available.\nOtherwise our Space will produce an error like the following:\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\nGood news is, our demo only has three requirements: gradio, torch, transformers.\nLet‚Äôs create a requirements.txt file with the packages we need and save it to the same directory as our app.py file.\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nBeautiful!\nHugging Face Spaces will automatically recognize the requirements.txt file and install the listed packages into our Space.\n\n\n10.3 Making a README file\nOur app.py can contain information about our demo, however, we can also use a README.md file to further communicate our work.\n\n\n\n\n\n\nNote\n\n\n\nIt is common practice in Git repositories (including GitHub and Hugging Face Hub) to add a README.md file to your project so people can read more (hence ‚Äúread me‚Äù) about what your project is about.\n\n\nWe can include anything in markdown-style text in the README.md file.\nHowever, Spaces also have a special YAML block at the top of the README.md file in the root directory with configuration details.\nInside the YAML block you can put special metadata details about your Space including:\n\ntitle - The title of your Space (e.g.¬†title: Food Not Food Text Classifier).\nemoji - The emoji to display on your Space (e.g.¬†emoji: üçóüö´ü•ë).\napp_file - The target app file for Spaces to run (set to app_file: app.py by default).\n\nAnd there are plenty more in the Spaces Configuration References documentation.\n\nTK image - YAML block + markdown text underneath\n\nLet‚Äôs create a README.md file with a YAML block at the top detailing some of the metadata about our project.\n\n\n\n\n\n\nNote\n\n\n\nThe YAML block at the top of the README.md can take some practice.\nIf you want to see a demo of how one gets created, try making a Hugging Face Space with the ‚ÄúCreate new Space‚Äù button on the https://huggingface.co/spaces page and seeing what the README.md file starts with (that‚Äôs how I found out what to do!).\n\n\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\nsdk_version: 4.36.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nTK - see the demo notebook on how to create this\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\nREADME.md created!\nNow let‚Äôs check out the files we have in our demos/food_not_food_text_classifier/ folder.\n\n!ls ../demos/food_not_food_text_classifier\n\nREADME.md  app.py  requirements.txt\n\n\nPerfect!\nLooks like we‚Äôve got all the files we need to create our Space.\nLet‚Äôs upload them to the Hugging Face Hub.\n\n\n10.4 Uploading our demo to Hugging Face Spaces\nWe‚Äôve created all of the files required for our demo, now for the fun part!\nLet‚Äôs upload them to Hugging Face Spaces.\nTo do so programmatically, we can use the Hugging Face Hub Python API.\n\n\n\n\n\n\nNote\n\n\n\nThe Hugging Face Hub Python API has many different options for interacting with the Hugging Face Hub programmatically.\nYou can create repositories, upload files, upload folders, add comments, change permissions and much much more.\nBe sure to explore the documentation for at least 10-15 minutes to get an idea of what‚Äôs possible.\n\n\nTo get our demo hosted on Hugging Face Spaces we‚Äôll go through the following steps:\n\nImport the required methods from the huggingface_hub package, including create_repo, get_full_repo_name, upload_file (optional, we‚Äôll be using upload_folder) and upload_folder.\nDefine the demo folder we‚Äôd like to upload as well as the different parameters for the Hugging Face Space such as repo type (\"space\"), our target Space name, the target Space SDK (\"gradio\"), our Hugging Face token with write access (optional if it already isn‚Äôt setup).\nCreate a repository on Hugging Face Spaces using the huggingface_hub.create_repo method and filling out the appropriate parameters.\nGet the full name of our created repository using the huggingface_hub.get_full_repo_name method (we could hard code this but I like to get it programmatically incase things change).\nUpload the contents of our target demo folder (../demos/food_not_food_text_classifier/) to Hugging Face Hub with huggingface_hub.upload_folder.\nHope it all works and inspect the results! ü§û\n\nA fair few steps but we‚Äôve got this!\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"../demos/food_not_food_text_classifier\"\nHF_TARGET_SPACE_NAME = \"learn_hf_food_not_food_text_classifier_demo\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full Hugging Face Hub repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo/tree/main/.\n\n\nExcellent!\nLooks like all of the files in our target demo folder were uploaded!\nOnce this happens, Hugging Face Spaces will take a couple of minutes to build our application.\nIf there are any errors, it will let us know.\nOtherwise, our demo application should be running live and be ready to test at a URL similar to: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo (though you may have to swap my username ‚Äúmrdbourke‚Äù for your own as well as the name you chose for the Space).\n\n\n10.5 Testing our hosted demo\nOne of the really cool things about Hugging Face Spaces is that we can share our demo application as a link so others can try it out.\nWe can also embed it right into our notebook.\nTo do so, we can go to the three dots in the top right of our hosted Space and select ‚ÄúEmbed this Space‚Äù.\nWe then have the option to embed our Space using a JavaScript web component, HTML iframe or via the direct URL.\nSince Jupyter notebooks have the ability to render HTML via IPython.display.HTML, let‚Äôs embed our Space with HTML.\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML(data='''\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n&gt;&lt;/iframe&gt;     \n''')\n\n\n     \n\n\nNow that‚Äôs cool!\nWe can try out our Food Not Food Text Classifier app from right within our notebook!",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#summary",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#summary",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "11 Summary",
    "text": "11 Summary\nYou should be very proud of yourself!\nWe‚Äôve just gone end-to-end on a machine learning workflow with Hugging Face.\nFrom loading a dataset to training a model to deploying that model in the form of a public demo.\nHere are some of the main takeaways from this project.\nThe Hugging Face ecosystem is a collection of powerful and open-source tools for machine learning workflows.\n\nHugging Face datasets helps you to store and preprocess datasets of almost any shape and size.\nHugging Face transformers has many built-in pretrained models for many different use cases and components such as transformers.Trainer help you to tailor those models to your own custom use cases.\nHugging Face tokenizers works closely with transformers and allows the efficient conversion of raw text data into numerical representation (which is required for machine learning models).\nThe Hugging Face Hub is a great place to share your models and machine learning projects. Over time, you can build up a portfolio of machine learning-based projects to show future employers or clients and to help the community grow.\nThere are many more, but I‚Äôll leave these for you to explore as extra-curriculum.\n\nA common machine learning workflow: dataset -&gt; model -&gt; demo.\nBefore a machine learning model is incorporated into a larger application, a very common workflow is to:\n\nFind an existing or create a new dataset for your specific problem.\nTrain/fine-tune and evaluate an existing model on your dataset.\nCreate a small demo application to test your trained model.\n\nWe‚Äôve just gone through all of these steps for text classification!\nText classification is a very common problem in many business settings. If you have a similar problem but a different dataset, you can replicate this workflow.\nBuilding your own model has several advantages over using APIs.\nAPIs are very helpful to try something out.\nHowever, depending on your use case, you may often want to create your own custom model.\nTraining your own model can often result in faster predictions and far less running costs over time.\nThe Hugging Face ecosystem enables the creation of custom models for almost any kind of machine learning problem.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#exercises-and-extensions",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#exercises-and-extensions",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "12 Exercises and Extensions",
    "text": "12 Exercises and Extensions\nThere‚Äôs no better way to improve other than practicing what you‚Äôve learned.\nThe following exercises and extensions are designed for you to practice the things we‚Äôve covered in this project.\n\nOur text classification model works on food/not_food text samples. How would you create your own binary text classification model on different classes?\n\nCreate ~10 or samples of your own text classes (e.g.¬†10 samples each of spam/not_spam emails) and retrain a text classification model.\nBonus: Share the model you‚Äôve made in a demo just like we did here. Send it to me, I‚Äôd love to see it! My email is on my website.\n\nWe‚Äôve trained our model on two classes (binary classification) but how might we increase that to 3 or more classes (multi-class classification)?\n\nHint: see the num_labels parameter in transformers.AutoModelForSequenceClassification.\n\nOur model seems to work pretty good on our test data and on the few number of examples we tried manually. Can you find any examples where our model fails? For example, what kind of sentences does it struggle with? How could you fix this?\n\nHint: Our model has been trained on examples with at least 5-12 words, does it still work with short sentences? (e.g.¬†‚Äúpie‚Äù).\nBonus: If you find any cases where our model doesn‚Äôt perform well, make an extra 10-20 examples of these and add them to the dataset and then retrain the model (you‚Äôll have to lookup ‚Äúhow to add rows to an existing Hugging Face dataset‚Äù). How does the model perform after adding these additional samples?\n\nDatasets are fundamental to any machine learning project, getting to know how to process and interact with them is a fundamental skill. Spend 1 hour going through the Hugging Face Datasets tutorial.\n\nWrite 5 things you can do with Hugging Face Datasets and where they might come in handy.\n\nThe Hugging Face transformers library has many features. The following readings are to help understand a handful of them.\n\nSpend 10 minutes exploring the transformers.TrainingArguments documentation.\nSpend 10 minutes reading the transformers.Trainer documentation.\n\nSpend 10 minutes reading the Hugging Face model sharing documentation.\n\nSpend 10 minutes reading the Hugging Face transformers.pipeline documentation.\n\nWhat does a pipeline do?\nName 3 different kinds of pipelines and describe what they do in a sentence\n\n\nGradio is a powerful library for making machine learning demos, learning more about it will help you in future creations. Spend 10-15 minutes reading the Gradio quickstart documentation.\n\nWhat are 3 kinds of demos you can create?\nWhat are 3 different inputs and outputs you can make?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#extra-resources",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#extra-resources",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "13 Extra resources",
    "text": "13 Extra resources\nThere are many things we touched over but didn‚Äôt go into much depth in this notebook.\nThe following resources are for those who‚Äôd like to learn a little bit more.\n\nSee how the food not food image caption dataset was created with synthetic text data (image captions generated by a Large Language Model) in the example Google Colab notebook.\nHugging Face have a great guide on sequence classification (it‚Äôs what this notebook was built on).\nFor more on the concept of padding and truncation in sequence processing, I‚Äôd recommend the Hugging Face padding and truncation guide.\nFor more on Transformers (the architecture) as well as the DistilBert model:\n\nRead Transformers from scratch by Peter Bloem.\nWatch Andrej Karpathy‚Äôs lecture on Transformers and their history.\nRead the original Attention is all you need paper (the paper that introduced the Transformer architecture).\nRead the DistilBert paper from the Hugging Face team (paper that introduced the DistilBert architecture and training setup).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  }
]