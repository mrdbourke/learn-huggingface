[
  {
    "objectID": "extras/resources.html",
    "href": "extras/resources.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "See the Pytorch extra resources for some ideas: https://www.learnpytorch.io/pytorch_extra_resources/"
  },
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\nTokenization = turning a series of data (text or image) into a series of tokens, where a token is a numerical representation of the input data, for example, in the case of text, tokenization could mean turning the words in a sentence into numbers (e.g.¬†‚Äúhello world‚Äù -&gt; [101, 102])\nTokens = a token is a letter, word or word-piece (word) that a model uses to represent input data, for example, in the case of text, a token could be a word (e.g.¬†‚Äúhello‚Äù) or a word-piece (e.g.¬†‚Äúhell‚Äù and ‚Äúo‚Äù), see: https://platform.openai.com/tokenizer for an example\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\ntorch = PyTorch, an open-source machine learning library\nHugging Face Hub = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared\nHugging Face Spaces = A platform to share and run machine learning apps/demos, usually built with Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit\nPrediction probability = the probability of a model‚Äôs prediction for a given input, is a score between 0 and 1 with 1 being the highest, for example, a model may have a prediction probability of 0.95, this would mean it‚Äôs quite confident with its prediction but it doesn‚Äôt mean it‚Äôs correct. A good way to inspect potential issues with a dataset is to show examples in the test set which have a high prediction probability but are wrong (e.g.¬†pred prob = 0.98 but the prediction was incorrect).\nHugging Face Pipeline (pipeline) = A high-level API for using model for various tasks (e.g.¬†text-classification, audio-classification, image-classification, object-detection and more), see the docs: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/pipelines#transformers.pipeline"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Next:\n# Add tools used in this overview (e.g. overview of the project)\n# Create a small dataset with text generation, e.g. 50x spam/not_spam emails and train a classifier on it ‚úÖ\n   # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n# Save the dataset to Hugging Face Datasets ‚úÖ\n   # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n# Train a classifier on it ‚úÖ\n# Save the model to the Hugging Face Model Hub ‚úÖ\n# Create a with Gradio and test the model in the wild ‚úÖ",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 TK - Overview",
    "text": "1 TK - Overview\n\n1.1 TK - What we‚Äôre going to build\nIn this project, we‚Äôre going to learn various aspects of the Hugging Face ecosystem whilst building a text classification model.\nTo keep things as practical as possible, we‚Äôre going to be bulding a food/not_food text classification model.\nGiven a piece of a text, our model will be able to predict if it‚Äôs about food or not.\nThis is the same kind of model I use in my own work on Nutrify (an app to help people learn about food).\nMore specifically, we‚Äôre going to follow the following steps:\n\nProblem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nFinding, training and evaluating a model - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\nCreating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others.\nTK image - see the finished product (demo)\n\n\n\n\n\n\nNote\n\n\n\nNote this is a hands-on project, so we‚Äôll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we‚Äôre doing, I‚Äôll leave links in the extra-curriculum section.\n\n\n\n\n1.2 TK - What is Hugging Face?\nHugging Face is a platform that offers access to many different kinds of open-source machine learning models and datasets.\nThey‚Äôre also the creators of the popular transformers library which is a Python-based library for working with pre-trained models as well as custom models and datasets.\nIf you‚Äôre getting into the world of AI and machine learning, you‚Äôre going to come across Hugging Face.\n\n\n1.3 TK - Why Hugging Face?\nMany of the biggest companies in the world use Hugging Face for their open-source machine learning projects including Apple, Google, Facebook (Meta), Microsoft, OpenAI, ByteDance and more.\nTK image - image of people using Hugging Face\nNot only does Hugging Face make it so you can use state-of-the-art machine learning models such as Stable Diffusion (for image generation) and Whipser (for audio transcription) easily, it also makes it so you can share your own models, datasets and resources.\nConsider Hugging Face the homepage of your AI/machine learning profile.\n\n\n1.4 TK - What is text classification?\nText classification is the process of assigning a category to a piece of text.\nWhere a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\nTK image - example of text classification\nExample text classification problems include:\n\n\n\n\n\n\n\n\nProblem\nDescription\nProblem Type\n\n\n\n\nSpam email detection\nIs an email spam or not spam?\nBinary classification (one thing or another)\n\n\nSentiment analysis\nIs a piece of text positive, negative or neutral?\nMulti-class classification (one thing from many)\n\n\nLanguage detection\nWhat language is a piece of text written in?\nMulti-class classification (one thing from many)\n\n\nTopic classification\nWhat topic(s) does a news article belong to?\nMulti-label classification (one or more things from many)\n\n\nHate speech detection\nIs a comment hateful or not hateful?\nBinary classification (one thing or another)\n\n\nProduct categorization\nWhat categories does a product belong to?\nMulti-label classification (one or more things from many)\n\n\n\nThere are several different kinds of models you can use for text classification.\nAnd each will have its pros and cons depending on the problem you‚Äôre working on.\nExample text classification models include:\n\n\n\nModel\nDescription\nPros\nCons\n\n\n\n\nRule-based\nUses a set of rules to classify text (e.g.¬†if text contains ‚Äúsad‚Äù -&gt; sentiment = low)\nSimple, easy to understand\nRequires manual creation of rules\n\n\nBag of Words\nCounts the frequency of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nTF-IDF\nWeighs the importance of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nDeep learning-based models\nUses neural networks to learn patterns in text\nCan learn complex patterns at scale\nCan require large amounts of data/compute power to run, not as easy to understand (can be hard to debug)\n\n\n\nWe‚Äôre going to use a deep learning model our case.\nWhy?\nBecause Hugging Face helps us do so.\nAnd in most cases, with a large enough dataset, a deep learning model will often perform better than a rule-based or other model.\n\n\n1.5 TK - Why train your own text classification models?\nYou can use pre-trained models for text classification as well as API-powered models and LLMs such as GPT-4 or Gemini.\nHowever, it‚Äôs often a good idea to train your own text classification models for a few reasons:\n\nThey can be much faster than API-powered models (since they‚Äôre running on your own hardware, this can save on costs and time).\nThey‚Äôre customized to your own data.\nThey don‚Äôt require you to send your data elsewhere (privacy).\nIf a service goes down, you‚Äôll still have access to your model (reliability).\n\nTK image - example of training your own model vs using an API-powered model",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 TK - Importing necessary libraries",
    "text": "2 TK - Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide (tk - link to getting setup guide) to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport transformers\n\nfrom datasets import Dataset\n\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\n\nUsing transformers version: 4.40.2\nUsing datasets version: 2.19.1\nUsing torch version: 2.2.0+cu121\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 TK - Getting a dataset",
    "text": "3 TK - Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor a text classificaiton problem, your dataset will likely come in the form of text (e.g.¬†a paragraph, sentence or phrase) and a label (e.g.¬†what category the text belongs to).\n\nTK image - showcase what a supervised dataset looks like (e.g.¬†text and label, this can be the dataset we‚Äôve got on Hugging Face hub, showcase the different parts of the dataset as well including the name etc)\n\nIn our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\nThis is a dataset I‚Äôve created earlier to help us practice building a text classification model.\nYou can find it on Hugging Face under the name mrdbourke/learn_hf_food_not_food_image_captions.\n\n\n\n\n\n\nResource\n\n\n\nSee how the food/not_food image caption dataset was created in the (TK - add notebook link and title, make this available on the website)\n\nTK - see dataset creation:\n\nDone, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing\nDone, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n\n\n\n\n\n3.1 Where can you get more datasets?\nThe are many different places you can get datasets for text-based problems.\nOne of the best places is on the Hugging Face Hub, specifically huggingface.co/datasets.\nHere you can find many different kinds of problem specific data such as text classification.\nTK image - show example image of text classification datasets\n\n\n3.2 Loading the dataset\nOnce we‚Äôve found/prepared a dataset on the Hugging Face Hub, we can use the datasets library to load it.\nTo load a dataset we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/learn_hf_food_not_food_image_captions.\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n# Load the dataset from Hugging Face Hub\ndataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n\n# Inspect the dataset\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 250\n    })\n})\n\n\nDataset loaded!\nLooks like our dataset has two features, text and label.\nAnd 250 total rows (the number of examples in our dataset).\nWe can check the column names with dataset.column_names.\n\n# What features are there?\ndataset.column_names\n\n{'train': ['text', 'label']}\n\n\nLooks like our dataset comes with a train split already (the whole dataset).\nWe can access the train split with dataset[\"train\"] (some datasets also come with built-in \"test\" splits too).\n\n# Access the training split\ndataset[\"train\"]\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 250\n})\n\n\nHow about we check out a single sample?\nWe can do so with indexing.\n\ndataset[\"train\"][0]\n\n{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n 'label': 'food'}\n\n\nNice! We get back a dictionary with the keys text and label.\nThe text key contains the text of the image caption and the label key contains the label (food or not food).\n\n\n3.3 TK - Inspect random examples from the dataset\nAt 250 total samples, our dataset isn‚Äôt too large.\nSo we could sit there and explore the samples one by one.\nBut whenever I interact with a new dataset, I like to view a bunch of random examples and get a feel of the data.\nDoing so is inline with the data explorer‚Äôs motto: visualize, visualize, visualize!\nAs a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\nLet‚Äôs write some code to view 5 random indexes of our data and their corresponding text and labels at a time.\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: Set of knitting needles with yarn waiting to be knitted | Label: not_food\nText: A bowl of sliced pears with a sprinkle of ginger and a side of honey | Label: food\nText: Sweet and spicy sushi roll with ingredients like mango and jalapeno. | Label: food\nText: Vibrant red curry with tofu and bell peppers, featuring tofu and sweet bell peppers in a rich coconut milk sauce. | Label: food\nText: Lawn mower stored in a shed | Label: not_food\n\n\nBeautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\nWe can get the unique labels in our dataset with dataset[\"train\"].unique(\"label\").\n\n# Get unique label values\ndataset[\"train\"].unique(\"label\")\n\n['food', 'not_food']\n\n\nIf our dataset is small enough to fit into memory, we can count the number of different labels with Python‚Äôs collections.Counter (a method for counting objects in an iterable or mapping).\n\n# Check number of each label\nfrom collections import Counter\n\nCounter(dataset[\"train\"][\"label\"])\n\nCounter({'food': 125, 'not_food': 125})\n\n\nExcellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\nIn a binary classification case, this is ideal.\nIf the classes were dramatically unbalanced (e.g.¬†90% food and 10% not food) we might have to consider collecting/creating more data.\nBut best to train a model and see how it goes before making any drastic dataset changes.\nBecause our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets).\n\n# Turn our dataset into a DataFrame and get a random sample\nfood_not_food_df = pd.DataFrame(dataset[\"train\"])\nfood_not_food_df.sample(7)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n160\nSet of speakers perched on a shelf\nnot_food\n\n\n91\nGarage door with a remote control ready for use\nnot_food\n\n\n37\nGuitar leaning casually against a couch\nnot_food\n\n\n151\nRound wooden dining table with chairs gathered...\nnot_food\n\n\n199\nCrunchy sushi roll with a creamy filling, feat...\nfood\n\n\n116\nA girl feeding her rabbit in the garden\nnot_food\n\n\n109\nJicama in a bowl, sprinkled with chili powder ...\nfood\n\n\n\n\n\n\n\n\n# Get the value counts of the label column\nfood_not_food_df[\"label\"].value_counts()\n\nlabel\nfood        125\nnot_food    125\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 TK - Preparing data for text classification",
    "text": "4 TK - Preparing data for text classification\nWe‚Äôve got our data ready but there are a few steps we‚Äôll need to take before we can model it.\nThe main two being:\n\nTokenization - turning our text into a numerical representation (machines prefer numbers rather than words), for example, {\"a\": 0, \"b\": 1, \"c\": 2...}.\nCreating a train/test split - right now our data is in a training split only but we‚Äôll create a test set to evaluate our model‚Äôs performance.\n\nThese don‚Äôt necessarily have to be in order either.\nBefore we get to them, let‚Äôs create a small mapping from our labels to numbers.\nIn the same way we need to tokenize our text into numerical representation, we also need to do the same for our labels.\n\n4.1 TK - Creating a mapping from labels to numbers\nOur machine learning model will want to see all numbers.\nThis goes for text as well as label input.\nSo let‚Äôs create a mapping from our labels to numbers.\nSince we‚Äôve only got a couple of labels (\"food\" and \"not_food\"), we can create a dictionary to map them to numbers, however, if you‚Äôve got a fair few labels, you may want to make this mapping programmatically.\nWe can use these dictionaries later on for our model training as well as evaluation.\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a binary classification task, the positive class, in our case \"food\", is usually given the label 1 and the negative class (\"not_food\") is given the label 0.\n\n\n\n# Create mappings programmatically from dataset\nid2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\nlabel2id = {label: idx for idx, label in id2label.items()}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\nWith our dictionary mappings created, we can update the labels of our dataset to be numeric.\nWe can do this using the datasets.Dataset.map method and passing it a function to apply to each example.\nLet‚Äôs create a small function which turns an example label into a number.\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\nexample_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n\n# Test the function\nmap_labels_to_number(example_sample)\n\n{'text': 'This is a sentence about my favourite food: honey.', 'label': 1}\n\n\nLooks like our function works!\nHow about we map it to the whole dataset?\n\n# Map our dataset labels to numbers\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\nNice! Looks like our labels are all numerical now.\nWe can check a few random samples using dataset.shuffle() and indexing for the first few.\n\n# Shuffle the dataset and view the first 5 samples (will return different results each time) \ndataset.shuffle()[:5]\n\n{'text': ['A plate of sliced pineapple with a side of whipped cream and a cherry on top',\n  'Walking in the park, a man jogs with his energetic dog',\n  'A square slice of Sicilian-style pizza with a thick and fluffy crust',\n  'Basketball hoop set up in a driveway',\n  'Remote control placed on a couch cushion'],\n 'label': [1, 0, 1, 0, 0]}\n\n\n\n\n4.2 TK - Split the dataset into training and test sets\nRight now our dataset only has a training split.\nHowever, we‚Äôd like to create a test split so we can evaluate our model.\nIn essence, our model will learn patterns (the relationship between text captions and their labels of food/not_food) on the training data.\nAnd we will evaluate those learned patterns on the test data.\nWe can split our data using the datasets.Dataset.train_test_split method.\nWe can use the test_size parameter to define the percentage of data we‚Äôd like to use in our test set (e.g.¬†test_size=0.2 would mean 20% of the data goes to the test set).\n\n# Create train/test splits\ndataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\n\n\nPerfect!\nOur dataset has been split into 200 training examples and 50 testing examples.\nLet‚Äôs visualize a few random examples to make sure they still look okay.\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: Uniquely presented sushi roll, such as a hand roll or sushi burrito.\nLabel: 1 (food)\n\n[INFO] Random sample from testing dataset:\nText: Creamy mild korma curry, featuring tender chicken chunks in a rich sauce made with yogurt and cream, sprinkled with sliced almonds.\nLabel: 1 (food)\n\n\n\n\n4.3 TK - Tokenizing text data\nLabels numericalized, dataset split, time to turn our text into numbers.\nTokenization is the process of converting a non-numerical data source into numbers.\nWhy?\nBecause machines (especially machine learning models) prefer numbers to human-style data.\nIn the case of the text \"I love pizza\" a very simple method of tokenization might be to convert each word to a number.\nFor example, {\"I\": 0, \"love\": 1, \"pizza\": 2}.\nHowever, for most modern machine learning models, the tokenization process is a bit more nuanced.\nFor example, the text \"I love pizza\" might be tokenized into something more like [101, 1045, 2293, 10733, 102].\nTK image - showcase an example using OpenAI‚Äôs tokenization tool and what this looks like with ‚ÄúI love pizza‚Äù: https://platform.openai.com/tokenizer\n\n\n\n\n\n\nNote\n\n\n\nDepending on the model you use, the tokenization process could be different. For example, one model might turn \"I love pizza\" into [40, 3021, 23317], where as another model might turn it into [101, 1045, 2293, 10733, 102].\nTo deal with this, Hugging Face models often pair models with their own tokenizers by pairing a tokenizer configuration with a model‚Äôs weights. Such is the case with distilbert/distilbert-base-uncased (there is a tokenizer.json file as well as a tokenizer_config.json file which contains all of the tokenizer implementation details).\nFor more examples of tokenization, you can see OpenAI‚Äôs tokenization visualizer tool as well as their open-source library tiktoken, Google also have an open-source tokenization library called sentencepiece, finally Hugging Face‚Äôs tokenizers library is also a great resource (this is what we‚Äôll be using behind the scenes).\n\n\nUPTOHERE, next: * go through docs on autotokenizer * explain why to use distilbert/distilbert-base-uncased (there are plenty of model options you can choose) * show details about the tokenizer * show examples of tokenizing text * make a function to tokenize text * TK - docs on autotokenizer: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer\n\nfrom transformers import AutoTokenizer\n\n# TK - why this model?\n# TK - see tokenizer: https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/distilbert#transformers.DistilBertTokenizerFast \ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n\ntokenizer\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nDistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\n\n# Check the vocab\n# TK - what other parameters are there for the tokenizer \ntokenizer.vocab, len(tokenizer.vocab), tokenizer.model_max_length, tokenizer.special_tokens \n\nAttributeError: 'DistilBertTokenizerFast' object has no attribute 'special_tokens'\n\n\n\n# Let's try it out\ntokenizer(\"I love pizza\")\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\n\ndef preprocess_function(examples):\n  return tokenizer(examples[\"text\"], truncation=True)\n\n\nexample_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n\n# Test the function\npreprocess_function(example_sample_2)\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\ntokenized_dataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\n\ntokenized_dataset[\"train\"][0], tokenized_dataset[\"test\"][0]\n\n({'text': 'Set of headphones placed on a desk',\n  'label': 0,\n  'input_ids': [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'A slice of pepperoni pizza with a layer of melted cheese',\n  'label': 1,\n  'input_ids': [101,\n   1037,\n   14704,\n   1997,\n   11565,\n   10698,\n   10733,\n   2007,\n   1037,\n   6741,\n   1997,\n   12501,\n   8808,\n   102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n\n\n\n4.4 TK - Make sure all text is the same length\n\n# Collate examples and pad them each batch\n# TK - this is not 100% needed as the tokenizer can handle padding, but it's good to know how to do it\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer,\n                                        padding=True)\ndata_collator\n\nDataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---setup-evaluation-metric",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---setup-evaluation-metric",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "5 TK - Setup Evaluation Metric",
    "text": "5 TK - Setup Evaluation Metric\n\nTK - What evaluation metrics are there?\n\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---training-our-model",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---training-our-model",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "6 TK - Training our model",
    "text": "6 TK - Training our model\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\nSteps for training:\n\nDefine model\nDefine training arguments\nPass training arguments to Trainer\nCall train()\n\n\nTK - What kind of training are we doing? Supervised learning + fine-tuning an existing model\n\n\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label,\n    label2id=label2id\n)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nTK - notice this output on pretraining advice\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: [‚Äòclassifier.bias‚Äô, ‚Äòclassifier.weight‚Äô, ‚Äòpre_classifier.bias‚Äô, ‚Äòpre_classifier.weight‚Äô] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nLet‚Äôs try and make a prediction with our model and see what happens.\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][:2])\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\n6.1 TK - Create a directory for saving models\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n\n6.2 TK - Setup training arguments\n\nTK - add markdown table of different parameters and what they do (e.g.¬†most of the common ones but add a note that these may want to be changed depending on the problem + there are many more in the docs)\n\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n# TODO: Turn off Weights & Biases logging? Or add it in?\n# TK - exercise: spend 10 minutes reading the TrainingArguments documentation\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir, # TODO: change this path to model save path, e.g. 'learn_hf_food_not_food_text_classifier_model' \n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\", # log training results every epoch\n    report_to=\"none\" # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n)\n\n\n\n6.3 TK - Setup trainer class\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator, # not necessary if using pre-built tokenizer padding (default)\n    compute_metrics=compute_metrics\n)\n\n\n\n6.4 TK - Train the model\n\nresults = trainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.615200\n0.450918\n1.000000\n\n\n2\n0.405600\n0.257541\n1.000000\n\n\n3\n0.219900\n0.123121\n1.000000\n\n\n4\n0.108100\n0.062602\n1.000000\n\n\n5\n0.056800\n0.036242\n1.000000\n\n\n6\n0.035900\n0.025235\n1.000000\n\n\n7\n0.026700\n0.019986\n1.000000\n\n\n8\n0.021900\n0.017336\n1.000000\n\n\n9\n0.019400\n0.016042\n1.000000\n\n\n10\n0.018200\n0.015633\n1.000000\n\n\n\n\n\n\n\n\n6.5 TK - Inspect the model results\n\n# TK - go through these\ntotal_train_time = results.metrics[\"train_runtime\"]\ntrain_samples_per_second = results.metrics[\"train_samples_per_second\"]\n\nprint(f\"Total training time: {total_train_time} seconds\")\nprint(f\"Training samples per second: {train_samples_per_second}\")\n\nTotal training time: 6.7168 seconds\nTraining samples per second: 297.761\n\n\n\n# TK - get loss curves\ntrainer_history = trainer.state.log_history[:-1]\ntrainer_training_time = trainer_history[-1]\ntrainer_history[:5]\n\n[{'loss': 0.6152,\n  'grad_norm': 3.3377952575683594,\n  'learning_rate': 1.8e-05,\n  'epoch': 1.0,\n  'step': 7},\n {'eval_loss': 0.45091766119003296,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0113,\n  'eval_samples_per_second': 4423.998,\n  'eval_steps_per_second': 176.96,\n  'epoch': 1.0,\n  'step': 7},\n {'loss': 0.4056,\n  'grad_norm': 2.4789676666259766,\n  'learning_rate': 1.6000000000000003e-05,\n  'epoch': 2.0,\n  'step': 14},\n {'eval_loss': 0.25754112005233765,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0124,\n  'eval_samples_per_second': 4023.931,\n  'eval_steps_per_second': 160.957,\n  'epoch': 2.0,\n  'step': 14},\n {'loss': 0.2199,\n  'grad_norm': 1.6385667324066162,\n  'learning_rate': 1.4e-05,\n  'epoch': 3.0,\n  'step': 21}]\n\n\n\n# Extract training and evaluation metrics\ntrainer_history_training_set = []\ntrainer_history_eval_set = []\n\nfor item in trainer_history[:-1]:\n    item_keys = list(item.keys())\n    if any(\"eval\" in item for item in item_keys):\n        trainer_history_eval_set.append(item)\n    else:\n        trainer_history_training_set.append(item)\n\n\ntrainer_history_training_df = pd.DataFrame(trainer_history_training_set)\ntrainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n\ntrainer_history_training_df\n\n\n\n\n\n\n\n\nloss\ngrad_norm\nlearning_rate\nepoch\nstep\n\n\n\n\n0\n0.6152\n3.337795\n0.000018\n1.0\n7\n\n\n1\n0.4056\n2.478968\n0.000016\n2.0\n14\n\n\n2\n0.2199\n1.638567\n0.000014\n3.0\n21\n\n\n3\n0.1081\n0.902428\n0.000012\n4.0\n28\n\n\n4\n0.0568\n0.546689\n0.000010\n5.0\n35\n\n\n5\n0.0359\n0.347724\n0.000008\n6.0\n42\n\n\n6\n0.0267\n0.309794\n0.000006\n7.0\n49\n\n\n7\n0.0219\n0.273363\n0.000004\n8.0\n56\n\n\n8\n0.0194\n0.244860\n0.000002\n9.0\n63\n\n\n9\n0.0182\n0.245236\n0.000000\n10.0\n70\n\n\n\n\n\n\n\n\ntrainer_history_eval_df\n\n\n\n\n\n\n\n\neval_loss\neval_accuracy\neval_runtime\neval_samples_per_second\neval_steps_per_second\nepoch\nstep\n\n\n\n\n0\n0.450918\n1.0\n0.0113\n4423.998\n176.960\n1.0\n7\n\n\n1\n0.257541\n1.0\n0.0124\n4023.931\n160.957\n2.0\n14\n\n\n2\n0.123121\n1.0\n0.0115\n4338.068\n173.523\n3.0\n21\n\n\n3\n0.062602\n1.0\n0.0115\n4349.855\n173.994\n4.0\n28\n\n\n4\n0.036242\n1.0\n0.0112\n4448.585\n177.943\n5.0\n35\n\n\n5\n0.025235\n1.0\n0.0122\n4100.485\n164.019\n6.0\n42\n\n\n6\n0.019986\n1.0\n0.0116\n4327.147\n173.086\n7.0\n49\n\n\n7\n0.017336\n1.0\n0.0113\n4406.522\n176.261\n8.0\n56\n\n\n8\n0.016042\n1.0\n0.0116\n4315.128\n172.605\n9.0\n63\n\n\n\n\n\n\n\n\n# Plot training and evaluation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\nplt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and evaluation loss over time\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.6 TK - Save the model for later use\n\n# Save model\n# See docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.save_model \ntrainer.save_model(model_save_dir)\n\n\n\n6.7 TK - Push the model to Hugging Face Hub\nTK - optional to share the model/use elsewhere\n\nsee here: https://huggingface.co/docs/transformers/en/model_sharing\nalso see here for how to setup huggingface-cli so you can write your model to your account\n\n\n# TK - have a note here for the errors\n# Note: you may see the following error\n# 403 Forbidden: You don't have the rights to create a model under the namespace \"mrdbourke\".\n# Cannot access content at: https://huggingface.co/api/repos/create.\n# If you are trying to create or update content,make sure you have a token with the `write` role.\n\n\n# TK - Push model to hub (for later re-use)\n# TODO: Push this model to the hub to be able to use it later\n# TK - this requires a \"write\" token from the Hugging Face Hub\n# TK - see docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub \n# TK - for example, on my local computer, my token is saved to: \"/home/daniel/.cache/huggingface/token\"\n\n# TK - Can create a model card with create_model_card()\n# see here: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.Trainer.create_model_card \n\ntrainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\" # set to False if you want the model to be public\n    # token=\"YOUR_HF_TOKEN_HERE\" # note: this will default to the token you have saved in your Hugging Face config\n)\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/7c9a4a6b17da981559f484538d51f6ff9a14c12d', commit_message='Uploading food not food text classifier model', commit_description='', oid='7c9a4a6b17da981559f484538d51f6ff9a14c12d', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nTK - note: this will make the model public, to make it private,\n\nSee the model here saved for later: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\n6.8 TK - Make and evaluate predictions on the test set\n\n# Perform predictions on the test set\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_metrics = predictions_all.metrics\nprediction_metrics\n\n\n\n\n{'test_loss': 0.015632618218660355,\n 'test_accuracy': 1.0,\n 'test_runtime': 0.0391,\n 'test_samples_per_second': 1280.07,\n 'test_steps_per_second': 51.203}\n\n\n\npredictions_all\n\nPredictionOutput(predictions=array([[-2.261428 ,  1.890655 ],\n       [ 1.8613493, -1.8532594],\n       [-2.2970695,  1.9171791],\n       [ 2.187019 , -2.1593657],\n       [ 2.1193414, -2.1615388],\n       [-2.2868803,  1.9454829],\n       [ 2.0827348, -2.1099336],\n       [ 2.154141 , -2.1266923],\n       [-2.279855 ,  1.9362432],\n       [-2.277952 ,  1.9518106],\n       [-2.2772808,  1.9423369],\n       [-1.9777709,  1.5732591],\n       [ 2.1512635, -2.0508409],\n       [-2.3032587,  1.9534686],\n       [-2.138177 ,  1.7531359],\n       [ 2.194142 , -2.1277084],\n       [-2.2709608,  1.9498663],\n       [ 1.9596925, -1.919577 ],\n       [-2.2827635,  1.9249418],\n       [-2.290854 ,  1.9592198],\n       [-2.2823153,  1.8799024],\n       [-2.3003585,  1.9387653],\n       [ 2.043029 , -2.0384376],\n       [ 2.0885575, -2.1244206],\n       [-2.2873669,  1.9443382],\n       [-2.2972584,  1.9009027],\n       [-2.2450745,  1.8596792],\n       [ 2.1050394, -2.040059 ],\n       [-2.2972147,  1.8946056],\n       [ 2.130832 , -2.133735 ],\n       [-2.2846339,  1.9422101],\n       [-2.2931519,  1.9279182],\n       [-2.3040657,  1.9485677],\n       [ 2.1816792, -2.141174 ],\n       [-2.3019922,  1.9271733],\n       [-2.2885954,  1.9124153],\n       [-2.2813184,  1.9542999],\n       [-2.304743 ,  1.8892938],\n       [ 2.1249578, -2.089177 ],\n       [ 2.043159 , -1.941504 ],\n       [-2.1469579,  1.8099191],\n       [-2.269732 ,  1.9235427],\n       [-2.0776005,  1.7352381],\n       [ 1.9634217, -2.0820174],\n       [-2.2788396,  1.9341636],\n       [-2.2946444,  1.9408271],\n       [-2.2920046,  1.9059081],\n       [-2.3030152,  1.9264866],\n       [ 2.1768198, -2.1458352],\n       [-2.301217 ,  1.9053475]], dtype=float32), label_ids=array([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 1, 0, 1]), metrics={'test_loss': 0.015632618218660355, 'test_accuracy': 1.0, 'test_runtime': 0.0391, 'test_samples_per_second': 1280.07, 'test_steps_per_second': 51.203})\n\n\n\npredictions_all._asdict().keys()\n\ndict_keys(['predictions', 'label_ids', 'metrics'])\n\n\n\nimport torch\npred_probs = torch.softmax(torch.tensor(predictions_all.predictions), dim=1)\npred_labels = np.argmax(predictions_all.predictions, axis=1)\ntrue_labels = dataset[\"test\"][\"label\"]\n\n# Calculate accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(true_labels, pred_labels)\naccuracy\n\n1.0\n\n\n\n# Make a DataFrame of test predictions\ntest_predictions_df = pd.DataFrame({\n    \"text\": dataset[\"test\"][\"text\"],\n    \"true_label\": true_labels,\n    \"pred_label\": pred_labels,\n    \"pred_prob\": torch.max(pred_probs, dim=1).values\n})\n\ntest_predictions_df.head()\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.984512\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.976215\n\n\n2\nA bowl of sliced bell peppers with a sprinkle ...\n1\n1\n0.985432\n\n\n3\nSet of mugs hanging on a hook\n0\n0\n0.987212\n\n\n4\nStanding floor lamp providing light next to an...\n0\n0\n0.986358\n\n\n\n\n\n\n\n\n# Show 10 examples with low prediction probability\n# TK - this is good to find samples where the model is unsure \ntest_predictions_df.sort_values(\"pred_prob\").head(10)\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n11\nA close-up shot of a cheesy pizza slice being ...\n1\n1\n0.972105\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.976215\n\n\n42\nBoxes of apples, pears, pineapple, manadrins a...\n1\n1\n0.978392\n\n\n17\nRelaxing on the porch, a couple enjoys the com...\n0\n0\n0.979753\n\n\n14\nTwo handfuls of bananas in a fruit bowl with g...\n1\n1\n0.979990\n\n\n40\nA bowl of cherries with a sprig of mint for ga...\n1\n1\n0.981236\n\n\n39\nA close-up of a woman practicing yoga in the l...\n0\n0\n0.981741\n\n\n43\nSet of muffin tins stacked together\n0\n0\n0.982799\n\n\n22\nTwo people sitting at a dining room table with...\n0\n0\n0.983398\n\n\n26\nA fruit platter with a variety of exotic fruit...\n1\n1\n0.983774",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-and-inspect-predictions-on-new-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-and-inspect-predictions-on-new-text-data",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "7 TK - Make and inspect predictions on new text data",
    "text": "7 TK - Make and inspect predictions on new text data\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n# Setup local model path\nlocal_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\nhuggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n\n7.1 TK - Pipeline mode\n\nTk - what is a pipeline?\n\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\ndef set_device():\n    \"\"\"\n    Set device to CUDA if available, else MPS (Mac), else CPU.\n\n    This defaults to using the best available device (usually).\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n\nDEVICE = set_device()\nprint(f\"[INFO] Using device: {DEVICE}\")\n\n[INFO] Using device: cuda\n\n\n\nimport torch\nfrom transformers import pipeline\n\n# Setup batch size for batched inference (can be adjusted depending on how much memory is available)\n# TK - why use batch size? -&gt; multiple samples at inference = faster\nBATCH_SIZE = 64\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=local_model_path,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE)\n\n\nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\nfood_not_food_classifier(sample_text_food)\n\n[{'label': 'food', 'score': 0.99871826171875}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9989410042762756}]\n\n\n\n# Pipeline also works with remote models (will have to laod the model locally first)\nfood_not_food_classifier_remote = pipeline(task=\"text-classification\", \n                                           model=huggingface_model_path,\n                                           batch_size=BATCH_SIZE,\n                                           device=DEVICE)\n\nfood_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")\n\n[{'label': 'food', 'score': 0.9981549382209778}]\n\n\n\n\n7.2 TK - Batch prediction\n\nTK - what is batch prediction?\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"Daniel Bourke is really cool :D\",\n    \"My favoruite food is biltong!\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9410305619239807},\n {'label': 'not_food', 'score': 0.9650871753692627},\n {'label': 'not_food', 'score': 0.9215793609619141},\n {'label': 'not_food', 'score': 0.9115400910377502},\n {'label': 'not_food', 'score': 0.9625208377838135},\n {'label': 'not_food', 'score': 0.9476941823959351},\n {'label': 'not_food', 'score': 0.9451109170913696},\n {'label': 'not_food', 'score': 0.9027702808380127},\n {'label': 'not_food', 'score': 0.9954429864883423},\n {'label': 'food', 'score': 0.7653573155403137}]\n\n\n\n\n7.3 TK - Time our model across larger sample sizes\n\nTK - our model is fast!\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n    print()\n\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n[INFO] Number of sentences: 100\n[INFO] Inference time for 100 sentences: 0.07726 seconds.\n[INFO] Avg inference time per sentence: 0.0007726 seconds.\n\n[INFO] Number of sentences: 1000\n[INFO] Inference time for 1000 sentences: 0.32344 seconds.\n[INFO] Avg inference time per sentence: 0.00032344 seconds.\n\n[INFO] Number of sentences: 10000\n[INFO] Inference time for 10000 sentences: 1.43834 seconds.\n[INFO] Avg inference time per sentence: 0.00014383 seconds.\n\n[INFO] Number of sentences: 100000\n[INFO] Inference time for 100000 sentences: 14.4585 seconds.\n[INFO] Avg inference time per sentence: 0.00014459 seconds.\n\nCPU times: user 15.8 s, sys: 552 ms, total: 16.3 s\nWall time: 16.3 s\n\n\n\n\n7.4 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text_food, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text_food}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "8 TK - Turning our model into a demo",
    "text": "8 TK - Turning our model into a demo\n\nTK - why build a demo?\n\n\ntry our model in the wild, see samples which don‚Äôt work properly, e.g.¬†use cases we didn‚Äôt think of‚Ä¶ ‚Äúpie‚Äù/‚Äútea‚Äù (short words), ‚Äúhjflasdjhfhwerr‚Äù (gibberish)\n\n\nTK - build a demo with Gradio, see it here: https://www.gradio.app/guides/quickstart\nTK - requires pip install gradio\n\n\n# Set top_k=2 to get top 2 predictions (in our case, food and not_food)\nfood_not_food_classifier(\"Testing the pipeline\", top_k=2)\n\n[{'label': 'not_food', 'score': 0.9977033734321594},\n {'label': 'food', 'score': 0.002296620048582554}]\n\n\n\n8.1 TK - Creating a simple function to perform inference\n\nTK - this is required for gradio -&gt; output a dict of {‚Äúlabel_1‚Äù: probability_1, ‚Äúlabel_2‚Äù: probability_2‚Ä¶}\n2 options:\n\nLocal demo (for our own inspection)\nHosted demo on Hugging Face Spaces (for sharing with others)\n\n\n\nimport gradio as gr\n\ndef food_not_food_classifier(text):\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=local_model_path,\n                                        batch_size=64,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\nfood_not_food_classifier(\"My lunch today was bacon and eggs\")\n\n{'food': 0.7966588139533997, 'not_food': 0.20334114134311676}\n\n\n\ndemo = gr.Interface(\n    fn=food_not_food_classifier, \n    inputs=\"text\", \n    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n    title=\"Food or Not Food Classifier\",\n    description=\"A text classifier to determine if a sentence is about food or not food.\",\n    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7863\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n\n\n8.2 TK - Uploading/running the demo\nOptions: * Uploading manually to Hugging Face Spaces - hf.co/new-space * Uploading programmatically to Hugging Face Spaces - https://www.gradio.app/guides/using-hugging-face-integrations#hosting-your-gradio-demos-on-spaces * Running the demo locally - Interface.launch() (only works if you have Gradio installed)\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\nimport torch\nimport gradio as gr\n\nfrom transformers import pipeline\n\ndef food_not_food_classifier(text):\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food.\n\nTK - See source code:\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\nTK - note: you will often need a requirements.txt file\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nCreate a README.md file with metadata instructions (these are specific to Hugging Face Spaces).\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\nsdk_version: 4.36.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nTK - see the demo notebook on how to create this\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\n\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file\n    upload_folder # for uploading multiple files (in a folder)\n)\n\npath_to_demo_folder = \"../demos/food_not_food_text_classifier\"\nrepo_type = \"space\" # we're creating a Hugging Face Space\n\n# Create a repo on Hugging Face\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_repo\ntarget_space_name = \"learn_hf_food_not_food_text_classifier_demo\"\nprint(f\"[INFO] Creating repo: {target_space_name}\")\ncreate_repo(\n    repo_id=target_space_name,\n    #token=\"YOUR_HF_TOKEN\"\n    private=False, # set to True if you want the repo to be private\n    repo_type=repo_type, # create a Hugging Face Space\n    space_sdk=\"gradio\", # we're using Gradio to build our demo \n    exist_ok=True, # set to False if you want to create the repo even if it already exists            \n)\n\n# Get the full repo name (e.g. \"mrdbourke/learn_hf_food_not_food_text_classifier_demo\")\nfull_repo_name = get_full_repo_name(model_id=target_space_name)\nprint(f\"[INFO] Full repo name: {full_repo_name}\")\n\n# Upload a file\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file \nprint(f\"[INFO] Uploading {path_to_demo_folder} to repo: {full_repo_name}\")\nfile_url = upload_folder(\n    folder_path=path_to_demo_folder,\n    path_in_repo=\".\", # save to the root of the repo\n    repo_id=full_repo_name,\n    repo_type=repo_type,\n    #token=\"YOUR_HF_TOKEN\"\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\n\n[INFO] Creating repo: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\n\nTK - see the demo link here: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\n\n8.3 TK - Testing the live demo\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML('''\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n&gt;&lt;/iframe&gt;     \n''')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---exercises-and-extensions",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---exercises-and-extensions",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 TK - Exercises and Extensions",
    "text": "9 TK - Exercises and Extensions\n\nWhere does our model fail? E.g. what kind of sentences does it struggle with? How could you fix this?\n\nMake an extra 10-50 examples of these and add them to the dataset and then retrain the model\nSee here: https://discuss.huggingface.co/t/how-do-i-add-things-rows-to-an-already-saved-dataset/27423\n\nBuild your own text classifier on a different dataset/your own custom dataset\nHow might we make our dataset multi-class? (e.g.¬†more than 2 classes)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---extra-resources",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---extra-resources",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "10 TK - Extra resources",
    "text": "10 TK - Extra resources\n\nHugging Face guide on text classification: https://huggingface.co/docs/transformers/en/tasks/sequence_classification",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nTeaching style: A machine learning cooking show!\nMottos:\nTODO:"
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "extras/setup.html",
    "href": "extras/setup.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "We‚Äôll need to install the following libraries from the Hugging Face ecosystem:"
  },
  {
    "objectID": "extras/setup.html#tk---add-a-getting-started-section-for-mac",
    "href": "extras/setup.html#tk---add-a-getting-started-section-for-mac",
    "title": "Learn Hugging Face ü§ó",
    "section": "TK - Add a getting started section for Mac",
    "text": "TK - Add a getting started section for Mac\nI could make this into a YouTube video for running on your own Mac.\nTK - Getting setup for Hugging Face Transformers on a Mac."
  },
  {
    "objectID": "extras/todo.html",
    "href": "extras/todo.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Add ‚Äúgetting setup‚Äù file to get started locally with the required dependencies\nAdd text classification dataset creation notebook (so people can see where the data comes from)\nAdd a Hugging Face ecosystem overview (transformers, datasets, tokenizers, torch, Hugging Face Hub, Hugging Face Spaces, etc.)\nMake code-only versions of notebooks? e.g.¬†text stripped away and only code is left"
  },
  {
    "objectID": "extras/todo.html#quarto-misc",
    "href": "extras/todo.html#quarto-misc",
    "title": "Learn Hugging Face ü§ó",
    "section": "Quarto misc",
    "text": "Quarto misc\n\nCreate share cards - https://quarto.org/docs/websites/website-tools.html#twitter-cards\n\nSee here for share image - https://quarto.org/docs/websites/website-tools.html#preview-images"
  }
]