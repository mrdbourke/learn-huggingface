[
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\ntorch = PyTorch, an open-source machine learning library\nHugging Face Hub = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared\nHugging Face Spaces = A platform to share and run machine learning apps/demos, usually built with Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nTODO:"
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Goal\n# Start with dataset\n  # Could generate this dataset or pre-existing\n  # Can have the dataset labelled manually or labelled with an LLM\n  # Could label this dataset manually or have it zero-shot labelled\n# Build a custom text classifier on labelled data\n  # Test text classifier on labelled data vs zero-shot model\n# Next:\n# Add tools used in this overview\n# Create a small dataset with text generation, e.g. 50x spam/not_spam emails and train a classifier on it ‚úÖ\n   # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n# Save the dataset to Hugging Face Datasets ‚úÖ\n   # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n# Train a classifier on it ‚úÖ\n# Save the model to the Hugging Face Model Hub\n# Create a with Gradio and test the model in the wild",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-hugging-face",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-hugging-face",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 TK - What is Hugging Face?",
    "text": "1 TK - What is Hugging Face?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-hugging-face",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-hugging-face",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 TK - Why Hugging Face?",
    "text": "2 TK - Why Hugging Face?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 TK - What is text classification?",
    "text": "3 TK - What is text classification?\n\nTK - write example problems (binary classification, multi-class classification, multi-label classification)\nTK - write places to find text classification models\nTK - write about different types of text classification models",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-train-your-own-text-classification-models",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-train-your-own-text-classification-models",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 TK - Why train your own text classification models?",
    "text": "4 TK - Why train your own text classification models?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-were-going-to-build",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-were-going-to-build",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "5 TK - What we‚Äôre going to build",
    "text": "5 TK - What we‚Äôre going to build\n\nTK - food not food image caption classifier",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "6 TK - Importing necessary libraries",
    "text": "6 TK - Importing necessary libraries\n\ntry:\n  import datasets, evaluate, accelerate\nexcept:\n  !pip install -U datasets, evaluate, accelerate\n  # !pip install -U datasets, evaluate, accelerate\n  import datasets, evaluate, accelerate\n\nfrom datasets import Dataset\n\nimport random\nimport pandas as pd\n\nimport transformers\n\n# TK - Write code so that this example works on Google Colab\n# TK - e.g. import/install required libraries \n# from google.colab import drive\n# drive.mount('/content/drive')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "7 TK - Getting a dataset",
    "text": "7 TK - Getting a dataset\n\nTK - show how this dataset was created\nTK image - show an image of example text dataset\n\n\n# Load the dataset\ndataset = datasets.load_dataset(\"mrdbourke/learn_hf_food_not_food_image_captions\")\ndataset\n\n\n7.1 TK - Inspect random examples from the dataset\n\nTK - always spend time with your data, when interacting with a new dataset, view random examples for ~10 minutes or at least 20-100 random examples to get a feel of the data\n\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: A bowl of sliced cantaloupe with a sprinkle of cinnamon and a side of cottage cheese | Label: food\nText: Traditional Japanese flavored sushi roll with pickled plum or fermented soybeans. | Label: food\nText: Set of napkins arranged in a ring | Label: not_food\nText: A close-up of a girl feeding her rabbit in the garden | Label: not_food\nText: A fruit kabob with a variety of fruits, such as grapes, melon, and berries | Label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "8 TK - Preparing data for text classification",
    "text": "8 TK - Preparing data for text classification\nSee docs: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#preprocess\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\"), see: https://huggingface.co/docs/datasets/en/process#map\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n\n\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\n\ndataset.shuffle()[:5]\n\n{'text': ['Working from home at her desk, a woman deals with a cat sitting on the keyboard',\n  'Camera mounted on a tripod',\n  'A girl feeding her rabbit in the garden',\n  'Wooden hanger holding clothes on a rack',\n  'Close-up of a sushi roll with avocado, cucumber, and salmon.'],\n 'label': [0, 0, 0, 0, 1]}\n\n\n\n# Create train/test splits, see: https://huggingface.co/docs/datasets/en/process#split\ndataset = dataset.train_test_split(test_size=0.2)\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\n\n\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']} | Label: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']} | Label: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: Set of knitting needles with yarn waiting to be knitted | Label: 0 (not_food)\n\n[INFO] Random sample from testing dataset:\nText: Set of spatulas kept in a holder | Label: 0 (not_food)\n\n\n\n8.1 TK - Tokenizing text data\n\nTK - what is tokenization? E.g. turning data from text to numbers (machines like numbers)\nTK - see OpenAI guide on tokenization: https://openai.com/tokenization/\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n  return tokenizer(examples[\"text\"], truncation=True)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\ntokenized_dataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\n\ntokenized_dataset[\"train\"][0], tokenized_dataset[\"test\"][0]\n\n({'text': 'Pizza with a stuffed crust, oozing with cheese',\n  'label': 1,\n  'input_ids': [101,\n   10733,\n   2007,\n   1037,\n   11812,\n   19116,\n   1010,\n   1051,\n   18153,\n   2075,\n   2007,\n   8808,\n   102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'A close-up of a cat lounging on a windowsill with a child reading nearby',\n  'label': 0,\n  'input_ids': [101,\n   1037,\n   2485,\n   1011,\n   2039,\n   1997,\n   1037,\n   4937,\n   10223,\n   22373,\n   2006,\n   1037,\n   3645,\n   8591,\n   2007,\n   1037,\n   2775,\n   3752,\n   3518,\n   102],\n  'attention_mask': [1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1]})\n\n\n\n\n8.2 TK - Make sure all text is the same length\n\n# Collate examples and pad them each batch\n# TK - this is not 100% needed as the tokenizer can handle padding, but it's good to know how to do it\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer,\n                                        padding=True)\ndata_collator\n\nDataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 Evaluation",
    "text": "9 Evaluation\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "10 Train",
    "text": "10 Train\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\n3 steps for training:\n\nDefine model\nDefine training arguments\nPass training arguments to Trainer\nCall train()\n\n\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label,\n    label2id=label2id\n)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][:2])\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n# TODO: Turn off Weights & Biases logging? Or add it in?\n# TK - exercise: spend 10 minutes reading the TrainingArguments documentation\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir, # TODO: change this path to model save path, e.g. 'learn_hf_food_not_food_text_classifier_model' \n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\" # log training results every epoch\n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n    # report_to=\"none\" # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n\n)\n\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n\n# Batch size 16\n#  [ 391/15234 00:22 &lt; 14:27, 17.12 it/s, Epoch 0.05/2]\n\n# Batch size 32\n# [ 724/7618 01:08 &lt; 10:51, 10.58 it/s, Epoch 0.19/2]\n\n# Batch size 64\n#  [ 150/3810 00:31 &lt; 12:52, 4.74 it/s, Epoch 0.08/2]\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.012900\n0.006287\n1.000000\n\n\n2\n0.006000\n0.003362\n1.000000\n\n\n3\n0.003400\n0.002199\n1.000000\n\n\n4\n0.002500\n0.001656\n1.000000\n\n\n5\n0.002000\n0.001357\n1.000000\n\n\n6\n0.001600\n0.001173\n1.000000\n\n\n7\n0.001500\n0.001059\n1.000000\n\n\n8\n0.001300\n0.000990\n1.000000\n\n\n9\n0.001300\n0.000951\n1.000000\n\n\n10\n0.001200\n0.000937\n1.000000\n\n\n\n\n\n\nTrainOutput(global_step=70, training_loss=0.003375225713742631, metrics={'train_runtime': 6.734, 'train_samples_per_second': 297.002, 'train_steps_per_second': 10.395, 'total_flos': 17458789182240.0, 'train_loss': 0.003375225713742631, 'epoch': 10.0})\n\n\n\n# Optional: push the model to Hugging Face Hub for re-use later\n# Note: Requires Hugging Face login\n# trainer.push_to_hub()\n\n\n10.1 TK - Save the model for later use\n\n# Save model\n# See: https://discuss.huggingface.co/t/how-to-save-my-model-to-use-it-later/20568/4\n# TODO: Make a models/ dir to save models to (so we don't have to commit them to git)\ntrainer.save_model(model_save_dir)\n\n\n\n10.2 TK - Push the model to Hugging Face Hub\nTK - optional to share the model/use elsewhere\n\nsee here: https://huggingface.co/docs/transformers/en/model_sharing\nalso see here for how to setup huggingface-cli so you can write your model to your account\n\n\n# TK - have a note here for the errors\n# Note: you may see the following error\n# 403 Forbidden: You don't have the rights to create a model under the namespace \"mrdbourke\".\n# Cannot access content at: https://huggingface.co/api/repos/create.\n# If you are trying to create or update content,make sure you have a token with the `write` role.\n\n\n# TK - Push model to hub (for later re-use)\n# TODO: Push this model to the hub to be able to use it later\n# TK - this requires a \"write\" token from the Hugging Face Hub\n# TK - see docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub \n# TK - for example, on my local computer, my token is saved to: \"/home/daniel/.cache/huggingface/token\"\n\n# TK - Can create a model card with create_model_card()\n# see here: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.Trainer.create_model_card \n\ntrainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\" # set to False if you want the model to be public\n    # token=\"YOUR_HF_TOKEN_HERE\" # note: this will default to the token you have saved in your Hugging Face config\n)\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/7c9a4a6b17da981559f484538d51f6ff9a14c12d', commit_message='Uploading food not food text classifier model', commit_description='', oid='7c9a4a6b17da981559f484538d51f6ff9a14c12d', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nTK - note: this will make the model public, to make it private,\n\nSee the model here saved for later: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---inference",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---inference",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "11 TK - Inference",
    "text": "11 TK - Inference\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nsample_text = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n\n11.1 Pipeline mode\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\n\n\nimport torch\nfrom transformers import pipeline\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=\"./learn_hf_food_not_food_text_classifier_model\",\n                                    batch_size=64,\n                                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfood_not_food_classifier(sample_text)\n\n[{'label': 'food', 'score': 0.9857270121574402}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9952113032341003}]\n\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"I'm feeling overwhelmed by this workload ‚Äì it's a real information buffet.\",\n    \"We're brainstorming new content ideas, hoping to cook up something innovative.\",\n    \"Daniel Bourke is really cool :D\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'food', 'score': 0.5004891753196716},\n {'label': 'not_food', 'score': 0.8031825423240662},\n {'label': 'food', 'score': 0.5688743591308594},\n {'label': 'food', 'score': 0.5170369744300842},\n {'label': 'not_food', 'score': 0.6362243890762329},\n {'label': 'not_food', 'score': 0.7544246315956116},\n {'label': 'not_food', 'score': 0.7407550811767578},\n {'label': 'not_food', 'score': 0.5384440422058105},\n {'label': 'not_food', 'score': 0.863006055355072},\n {'label': 'not_food', 'score': 0.9562841653823853},\n {'label': 'not_food', 'score': 0.9076286554336548}]\n\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {end_time - start_time} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {(end_time - start_time) / len(sentences_big)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 110\n[INFO] Inference time for 110 sentences: 0.06326603889465332 seconds.\n[INFO] Avg inference time per sentence: 0.000575145808133212 seconds.\n\n[INFO] Number of sentences: 1100\n[INFO] Inference time for 1100 sentences: 0.341522216796875 seconds.\n[INFO] Avg inference time per sentence: 0.00031047474254261364 seconds.\n\n[INFO] Number of sentences: 11000\n[INFO] Inference time for 11000 sentences: 1.562863826751709 seconds.\n[INFO] Avg inference time per sentence: 0.00014207852970470083 seconds.\n\n[INFO] Number of sentences: 110000\n[INFO] Inference time for 110000 sentences: 15.670900344848633 seconds.\n[INFO] Avg inference time per sentence: 0.00014246273040771485 seconds.\n\nCPU times: user 17.2 s, sys: 493 ms, total: 17.6 s\nWall time: 17.6 s\n\n\n\n\n11.2 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "12 TK - Turning our model into a demo",
    "text": "12 TK - Turning our model into a demo\n\nTK - build a demo with Gradio, see it here: https://www.gradio.app/guides/quickstart\nTK - requires pip install gradio\n\n\n# TODO: Make a demo of the model with Gradio and test it in the wild\n\n\nfood_not_food_classifier(\"Testing the pipeline\", return_all_scores=True)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n\n\n[[{'label': 'not_food', 'score': 0.9902743697166443},\n  {'label': 'food', 'score': 0.00972571037709713}]]\n\n\n\nimport gradio as gr\n\ndef food_not_food_classifier(text):\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=\"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\",\n                                        batch_size=64,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\nfood_not_food_classifier(\"My lunch today was bacon and eggs\")\n\n{'food': 0.7966588139533997, 'not_food': 0.20334114134311676}\n\n\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"Food or Not Food Classifier\",\n             description=\"A text classifier to determine if a sentence is about food or not food.\",\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7862\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\nimport torch\nimport gradio as gr\n\nfrom transformers import pipeline\n\ndef food_not_food_classifier(text):\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food.\n\nTK - See source code:\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\n\n12.1 TK - Uploading/running the demo\nOptions: * Uploading manually to Hugging Face Spaces - hf.co/new-space * Uploading programmatically to Hugging Face Spaces - https://www.gradio.app/guides/using-hugging-face-integrations#hosting-your-gradio-demos-on-spaces * Running the demo locally - Interface.launch() (only works if you have Gradio installed)\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nCreate a README.md file with metadata instructions (these are specific to Hugging Face Spaces).\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\nsdk_version: 4.36.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nTK - see the demo notebook on how to create this\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\n\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file\n    upload_folder # for uploading multiple files (in a folder)\n)\n\npath_to_demo_folder = \"../demos/food_not_food_text_classifier\"\nrepo_type = \"space\" # we're creating a Hugging Face Space\n\n# Create a repo on Hugging Face\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_repo\ntarget_space_name = \"learn_hf_food_not_food_text_classifier_demo\"\nprint(f\"[INFO] Creating repo: {target_space_name}\")\ncreate_repo(\n    repo_id=target_space_name,\n    #token=\"YOUR_HF_TOKEN\"\n    private=False, # set to True if you want the repo to be private\n    repo_type=repo_type, # create a Hugging Face Space\n    space_sdk=\"gradio\", # we're using Gradio to build our demo \n    exist_ok=True, # set to False if you want to create the repo even if it already exists            \n)\n\n# Get the full repo name (e.g. \"mrdbourke/learn_hf_food_not_food_text_classifier_demo\")\nfull_repo_name = get_full_repo_name(model_id=target_space_name)\nprint(f\"[INFO] Full repo name: {full_repo_name}\")\n\n# Upload a file\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file \nprint(f\"[INFO] Uploading {path_to_demo_folder} to repo: {full_repo_name}\")\nfile_url = upload_folder(\n    folder_path=path_to_demo_folder,\n    path_in_repo=\".\", # save to the root of the repo\n    repo_id=full_repo_name,\n    repo_type=repo_type,\n    #token=\"YOUR_HF_TOKEN\"\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\n\n[INFO] Creating repo: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\nTK - note: you may need a requirements.txt file\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\n\n# Next:\n# Change repo name to \"learn_hf...\"\n# Embed the repo here\n# Go back through code and make sure it's clean\n# See demo link: https://huggingface.co/spaces/mrdbourke/learn_food_not_food_text_classifier_demo",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  }
]