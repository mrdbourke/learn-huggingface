[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\ntorch = PyTorch, an open-source machine learning library\nHugging Face Hub = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared\nHugging Face Spaces = A platform to share and run machine learning apps/demos, usually built with Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit\nPrediction probability = the probability of a model‚Äôs prediction for a given input, is a score between 0 and 1 with 1 being the highest, for example, a model may have a prediction probability of 0.95, this would mean it‚Äôs quite confident with its prediction but it doesn‚Äôt mean it‚Äôs correct. A good way to inspect potential issues with a dataset is to show examples in the test set which have a high prediction probability but are wrong (e.g.¬†pred prob = 0.98 but the prediction was incorrect).\nHugging Face Pipeline (pipeline) = A high-level API for using model for various tasks (e.g.¬†text-classification, audio-classification, image-classification, object-detection and more), see the docs: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/pipelines#transformers.pipeline"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "",
    "text": "# Next:\n# Add tools used in this overview (e.g. overview of the project)\n# Create a small dataset with text generation, e.g. 50x spam/not_spam emails and train a classifier on it ‚úÖ\n   # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n# Save the dataset to Hugging Face Datasets ‚úÖ\n   # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n# Train a classifier on it ‚úÖ\n# Save the model to the Hugging Face Model Hub ‚úÖ\n# Create a with Gradio and test the model in the wild ‚úÖ",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-hugging-face",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-hugging-face",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 TK - What is Hugging Face?",
    "text": "1 TK - What is Hugging Face?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-hugging-face",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-hugging-face",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 TK - Why Hugging Face?",
    "text": "2 TK - Why Hugging Face?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-is-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 TK - What is text classification?",
    "text": "3 TK - What is text classification?\n\nTK - write example problems (binary classification, multi-class classification, multi-label classification)\nTK - write places to find text classification models\nTK - write about different types of text classification models",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-train-your-own-text-classification-models",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---why-train-your-own-text-classification-models",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 TK - Why train your own text classification models?",
    "text": "4 TK - Why train your own text classification models?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-were-going-to-build",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---what-were-going-to-build",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "5 TK - What we‚Äôre going to build",
    "text": "5 TK - What we‚Äôre going to build\n\nTK - food not food image caption classifier",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "2 TK - Importing necessary libraries",
    "text": "2 TK - Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide (tk - link to getting setup guide) to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport transformers\n\nfrom datasets import Dataset\n\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\n\nUsing transformers version: 4.40.2\nUsing datasets version: 2.19.1\nUsing torch version: 2.2.0+cu121\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "3 TK - Getting a dataset",
    "text": "3 TK - Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor a text classificaiton problem, your dataset will likely come in the form of text (e.g.¬†a paragraph, sentence or phrase) and a label (e.g.¬†what category the text belongs to).\n\nTK image - showcase what a supervised dataset looks like (e.g.¬†text and label, this can be the dataset we‚Äôve got on Hugging Face hub, showcase the different parts of the dataset as well including the name etc)\n\nIn our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\nThis is a dataset I‚Äôve created earlier to help us practice building a text classification model.\nYou can find it on Hugging Face under the name mrdbourke/learn_hf_food_not_food_image_captions.\n\n\n\n\n\n\nResource\n\n\n\nSee how the food/not_food image caption dataset was created in the (TK - add notebook link and title, make this available on the website)\n\nTK - see dataset creation:\n\nDone, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing\nDone, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n\n\n\n\n\n3.1 Where can you get more datasets?\nThe are many different places you can get datasets for text-based problems.\nOne of the best places is on the Hugging Face Hub, specifically huggingface.co/datasets.\nHere you can find many different kinds of problem specific data such as text classification.\nTK image - show example image of text classification datasets\n\n\n3.2 Loading the dataset\nOnce we‚Äôve found/prepared a dataset on the Hugging Face Hub, we can use the datasets library to load it.\nTo load a dataset we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/learn_hf_food_not_food_image_captions.\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n# Load the dataset from Hugging Face Hub\ndataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n\n# Inspect the dataset\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 250\n    })\n})\n\n\nDataset loaded!\nLooks like our dataset has two features, text and label.\nAnd 250 total rows (the number of examples in our dataset).\nWe can check the column names with dataset.column_names.\n\n# What features are there?\ndataset.column_names\n\n{'train': ['text', 'label']}\n\n\nLooks like our dataset comes with a train split already (the whole dataset).\nWe can access the train split with dataset[\"train\"] (some datasets also come with built-in \"test\" splits too).\n\n# Access the training split\ndataset[\"train\"]\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 250\n})\n\n\nHow about we check out a single sample?\nWe can do so with indexing.\n\ndataset[\"train\"][0]\n\n{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n 'label': 'food'}\n\n\nNice! We get back a dictionary with the keys text and label.\nThe text key contains the text of the image caption and the label key contains the label (food or not food).\n\n\n3.3 TK - Inspect random examples from the dataset\nAt 250 total samples, our dataset isn‚Äôt too large.\nSo we could sit there and explore the samples one by one.\nBut whenever I interact with a new dataset, I like to view a bunch of random examples and get a feel of the data.\nDoing so is inline with the data explorer‚Äôs motto: visualize, visualize, visualize!\nAs a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\nLet‚Äôs write some code to view 5 random indexes of our data and their corresponding text and labels at a time.\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: Set of knitting needles with yarn waiting to be knitted | Label: not_food\nText: A bowl of sliced pears with a sprinkle of ginger and a side of honey | Label: food\nText: Sweet and spicy sushi roll with ingredients like mango and jalapeno. | Label: food\nText: Vibrant red curry with tofu and bell peppers, featuring tofu and sweet bell peppers in a rich coconut milk sauce. | Label: food\nText: Lawn mower stored in a shed | Label: not_food\n\n\nBeautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\nWe can get the unique labels in our dataset with dataset[\"train\"].unique(\"label\").\n\n# Get unique label values\ndataset[\"train\"].unique(\"label\")\n\n['food', 'not_food']\n\n\nIf our dataset is small enough to fit into memory, we can count the number of different labels with Python‚Äôs collections.Counter (a method for counting objects in an iterable or mapping).\n\n# Check number of each label\nfrom collections import Counter\n\nCounter(dataset[\"train\"][\"label\"])\n\nCounter({'food': 125, 'not_food': 125})\n\n\nExcellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\nIn a binary classification case, this is ideal.\nIf the classes were dramatically unbalanced (e.g.¬†90% food and 10% not food) we might have to consider collecting/creating more data.\nBut best to train a model and see how it goes before making any drastic dataset changes.\nBecause our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets).\n\n# Turn our dataset into a DataFrame and get a random sample\nfood_not_food_df = pd.DataFrame(dataset[\"train\"])\nfood_not_food_df.sample(7)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n160\nSet of speakers perched on a shelf\nnot_food\n\n\n91\nGarage door with a remote control ready for use\nnot_food\n\n\n37\nGuitar leaning casually against a couch\nnot_food\n\n\n151\nRound wooden dining table with chairs gathered...\nnot_food\n\n\n199\nCrunchy sushi roll with a creamy filling, feat...\nfood\n\n\n116\nA girl feeding her rabbit in the garden\nnot_food\n\n\n109\nJicama in a bowl, sprinkled with chili powder ...\nfood\n\n\n\n\n\n\n\n\n# Get the value counts of the label column\nfood_not_food_df[\"label\"].value_counts()\n\nlabel\nfood        125\nnot_food    125\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "4 TK - Preparing data for text classification",
    "text": "4 TK - Preparing data for text classification\nUPTOHERE\n\nThere are many ways to get data ready for various machine learning tasks, see: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#preprocess\n\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\"), see: https://huggingface.co/docs/datasets/en/process#map\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\n\ndataset.shuffle()[:5]\n\n{'text': [\"King-size bed with a white comforter inviting a good night's sleep\",\n  'A slice of pizza with a spicy kick, featuring jalapeno peppers',\n  'Red brick fireplace with a mantel serving as a centerpiece',\n  'Creamy spinach and potato curry, featuring fluffy potatoes and nutritious spinach in a rich sauce with cream and garam masala.',\n  'Set of cookie cutters collected in a jar'],\n 'label': [0, 1, 0, 1, 0]}\n\n\n\n4.1 TK - Split the dataset into training and test sets\n\n# Create train/test splits, see: https://huggingface.co/docs/datasets/en/process#split\ndataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\n\n\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']} | Label: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']} | Label: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: Pizza with a unique topping combination of pineapple and ham | Label: 1 (food)\n\n[INFO] Random sample from testing dataset:\nText: Tangy tomato curry with chicken, featuring tender chicken pieces in a zesty tomato-based sauce with onions and spices. | Label: 1 (food)\n\n\n\n\n4.2 TK - Tokenizing text data\n\nTK - what is tokenization? E.g. turning data from text to numbers (machines like numbers)\nTK - see OpenAI guide on tokenization: https://openai.com/tokenization/\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n  return tokenizer(examples[\"text\"], truncation=True)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\ntokenized_dataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\n\ntokenized_dataset[\"train\"][0], tokenized_dataset[\"test\"][0]\n\n({'text': 'Set of headphones placed on a desk',\n  'label': 0,\n  'input_ids': [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'A slice of pepperoni pizza with a layer of melted cheese',\n  'label': 1,\n  'input_ids': [101,\n   1037,\n   14704,\n   1997,\n   11565,\n   10698,\n   10733,\n   2007,\n   1037,\n   6741,\n   1997,\n   12501,\n   8808,\n   102],\n  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n\n\n\n4.3 TK - Make sure all text is the same length\n\n# Collate examples and pad them each batch\n# TK - this is not 100% needed as the tokenizer can handle padding, but it's good to know how to do it\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer,\n                                        padding=True)\ndata_collator\n\nDataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#evaluation",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 Evaluation",
    "text": "9 Evaluation\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#train",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "10 Train",
    "text": "10 Train\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\n3 steps for training:\n\nDefine model\nDefine training arguments\nPass training arguments to Trainer\nCall train()\n\n\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label,\n    label2id=label2id\n)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][:2])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[58], line 2\n      1 # Try and make a prediction with the loaded model (this will error)\n----&gt; 2 model(**tokenized_dataset[\"train\"][:2])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n# TODO: Turn off Weights & Biases logging? Or add it in?\n# TK - exercise: spend 10 minutes reading the TrainingArguments documentation\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir, # TODO: change this path to model save path, e.g. 'learn_hf_food_not_food_text_classifier_model' \n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\" # log training results every epoch\n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n    # report_to=\"none\" # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n\n)\n\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n\n# Batch size 16\n#  [ 391/15234 00:22 &lt; 14:27, 17.12 it/s, Epoch 0.05/2]\n\n# Batch size 32\n# [ 724/7618 01:08 &lt; 10:51, 10.58 it/s, Epoch 0.19/2]\n\n# Batch size 64\n#  [ 150/3810 00:31 &lt; 12:52, 4.74 it/s, Epoch 0.08/2]\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.012900\n0.006287\n1.000000\n\n\n2\n0.006000\n0.003362\n1.000000\n\n\n3\n0.003400\n0.002199\n1.000000\n\n\n4\n0.002500\n0.001656\n1.000000\n\n\n5\n0.002000\n0.001357\n1.000000\n\n\n6\n0.001600\n0.001173\n1.000000\n\n\n7\n0.001500\n0.001059\n1.000000\n\n\n8\n0.001300\n0.000990\n1.000000\n\n\n9\n0.001300\n0.000951\n1.000000\n\n\n10\n0.001200\n0.000937\n1.000000\n\n\n\n\n\n\nTrainOutput(global_step=70, training_loss=0.003375225713742631, metrics={'train_runtime': 6.734, 'train_samples_per_second': 297.002, 'train_steps_per_second': 10.395, 'total_flos': 17458789182240.0, 'train_loss': 0.003375225713742631, 'epoch': 10.0})\n\n\n\n# Optional: push the model to Hugging Face Hub for re-use later\n# Note: Requires Hugging Face login\n# trainer.push_to_hub()\n\n\n10.1 TK - Save the model for later use\n\n# Save model\n# See: https://discuss.huggingface.co/t/how-to-save-my-model-to-use-it-later/20568/4\n# TODO: Make a models/ dir to save models to (so we don't have to commit them to git)\ntrainer.save_model(model_save_dir)\n\n\n\n10.2 TK - Push the model to Hugging Face Hub\nTK - optional to share the model/use elsewhere\n\nsee here: https://huggingface.co/docs/transformers/en/model_sharing\nalso see here for how to setup huggingface-cli so you can write your model to your account\n\n\n# TK - have a note here for the errors\n# Note: you may see the following error\n# 403 Forbidden: You don't have the rights to create a model under the namespace \"mrdbourke\".\n# Cannot access content at: https://huggingface.co/api/repos/create.\n# If you are trying to create or update content,make sure you have a token with the `write` role.\n\n\n# TK - Push model to hub (for later re-use)\n# TODO: Push this model to the hub to be able to use it later\n# TK - this requires a \"write\" token from the Hugging Face Hub\n# TK - see docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub \n# TK - for example, on my local computer, my token is saved to: \"/home/daniel/.cache/huggingface/token\"\n\n# TK - Can create a model card with create_model_card()\n# see here: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.Trainer.create_model_card \n\ntrainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\" # set to False if you want the model to be public\n    # token=\"YOUR_HF_TOKEN_HERE\" # note: this will default to the token you have saved in your Hugging Face config\n)\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/7c9a4a6b17da981559f484538d51f6ff9a14c12d', commit_message='Uploading food not food text classifier model', commit_description='', oid='7c9a4a6b17da981559f484538d51f6ff9a14c12d', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nTK - note: this will make the model public, to make it private,\n\nSee the model here saved for later: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---inference",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---inference",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "11 TK - Inference",
    "text": "11 TK - Inference\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nsample_text = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n\n11.1 Pipeline mode\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\n\n\nimport torch\nfrom transformers import pipeline\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=\"./learn_hf_food_not_food_text_classifier_model\",\n                                    batch_size=64,\n                                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfood_not_food_classifier(sample_text)\n\n[{'label': 'food', 'score': 0.9857270121574402}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9952113032341003}]\n\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"I'm feeling overwhelmed by this workload ‚Äì it's a real information buffet.\",\n    \"We're brainstorming new content ideas, hoping to cook up something innovative.\",\n    \"Daniel Bourke is really cool :D\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'food', 'score': 0.5004891753196716},\n {'label': 'not_food', 'score': 0.8031825423240662},\n {'label': 'food', 'score': 0.5688743591308594},\n {'label': 'food', 'score': 0.5170369744300842},\n {'label': 'not_food', 'score': 0.6362243890762329},\n {'label': 'not_food', 'score': 0.7544246315956116},\n {'label': 'not_food', 'score': 0.7407550811767578},\n {'label': 'not_food', 'score': 0.5384440422058105},\n {'label': 'not_food', 'score': 0.863006055355072},\n {'label': 'not_food', 'score': 0.9562841653823853},\n {'label': 'not_food', 'score': 0.9076286554336548}]\n\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {end_time - start_time} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {(end_time - start_time) / len(sentences_big)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 110\n[INFO] Inference time for 110 sentences: 0.06326603889465332 seconds.\n[INFO] Avg inference time per sentence: 0.000575145808133212 seconds.\n\n[INFO] Number of sentences: 1100\n[INFO] Inference time for 1100 sentences: 0.341522216796875 seconds.\n[INFO] Avg inference time per sentence: 0.00031047474254261364 seconds.\n\n[INFO] Number of sentences: 11000\n[INFO] Inference time for 11000 sentences: 1.562863826751709 seconds.\n[INFO] Avg inference time per sentence: 0.00014207852970470083 seconds.\n\n[INFO] Number of sentences: 110000\n[INFO] Inference time for 110000 sentences: 15.670900344848633 seconds.\n[INFO] Avg inference time per sentence: 0.00014246273040771485 seconds.\n\nCPU times: user 17.2 s, sys: 493 ms, total: 17.6 s\nWall time: 17.6 s\n\n\n\n\n11.2 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "8 TK - Turning our model into a demo",
    "text": "8 TK - Turning our model into a demo\n\nTK - why build a demo?\n\n\ntry our model in the wild, see samples which don‚Äôt work properly, e.g.¬†use cases we didn‚Äôt think of‚Ä¶ ‚Äúpie‚Äù/‚Äútea‚Äù (short words), ‚Äúhjflasdjhfhwerr‚Äù (gibberish)\n\n\nTK - build a demo with Gradio, see it here: https://www.gradio.app/guides/quickstart\nTK - requires pip install gradio\n\n\n# Set top_k=2 to get top 2 predictions (in our case, food and not_food)\nfood_not_food_classifier(\"Testing the pipeline\", top_k=2)\n\n[{'label': 'not_food', 'score': 0.9977033734321594},\n {'label': 'food', 'score': 0.002296620048582554}]\n\n\n\n8.1 TK - Creating a simple function to perform inference\n\nTK - this is required for gradio -&gt; output a dict of {‚Äúlabel_1‚Äù: probability_1, ‚Äúlabel_2‚Äù: probability_2‚Ä¶}\n2 options:\n\nLocal demo (for our own inspection)\nHosted demo on Hugging Face Spaces (for sharing with others)\n\n\n\nimport gradio as gr\n\ndef food_not_food_classifier(text):\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=local_model_path,\n                                        batch_size=64,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\nfood_not_food_classifier(\"My lunch today was bacon and eggs\")\n\n{'food': 0.7966588139533997, 'not_food': 0.20334114134311676}\n\n\n\ndemo = gr.Interface(\n    fn=food_not_food_classifier, \n    inputs=\"text\", \n    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n    title=\"Food or Not Food Classifier\",\n    description=\"A text classifier to determine if a sentence is about food or not food.\",\n    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7863\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n\n\n8.2 TK - Uploading/running the demo\nOptions: * Uploading manually to Hugging Face Spaces - hf.co/new-space * Uploading programmatically to Hugging Face Spaces - https://www.gradio.app/guides/using-hugging-face-integrations#hosting-your-gradio-demos-on-spaces * Running the demo locally - Interface.launch() (only works if you have Gradio installed)\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\nimport torch\nimport gradio as gr\n\nfrom transformers import pipeline\n\ndef food_not_food_classifier(text):\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food.\n\nTK - See source code:\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\nTK - note: you will often need a requirements.txt file\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nCreate a README.md file with metadata instructions (these are specific to Hugging Face Spaces).\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\nsdk_version: 4.36.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nTK - see the demo notebook on how to create this\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\n\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file\n    upload_folder # for uploading multiple files (in a folder)\n)\n\npath_to_demo_folder = \"../demos/food_not_food_text_classifier\"\nrepo_type = \"space\" # we're creating a Hugging Face Space\n\n# Create a repo on Hugging Face\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_repo\ntarget_space_name = \"learn_hf_food_not_food_text_classifier_demo\"\nprint(f\"[INFO] Creating repo: {target_space_name}\")\ncreate_repo(\n    repo_id=target_space_name,\n    #token=\"YOUR_HF_TOKEN\"\n    private=False, # set to True if you want the repo to be private\n    repo_type=repo_type, # create a Hugging Face Space\n    space_sdk=\"gradio\", # we're using Gradio to build our demo \n    exist_ok=True, # set to False if you want to create the repo even if it already exists            \n)\n\n# Get the full repo name (e.g. \"mrdbourke/learn_hf_food_not_food_text_classifier_demo\")\nfull_repo_name = get_full_repo_name(model_id=target_space_name)\nprint(f\"[INFO] Full repo name: {full_repo_name}\")\n\n# Upload a file\n# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file \nprint(f\"[INFO] Uploading {path_to_demo_folder} to repo: {full_repo_name}\")\nfile_url = upload_folder(\n    folder_path=path_to_demo_folder,\n    path_in_repo=\".\", # save to the root of the repo\n    repo_id=full_repo_name,\n    repo_type=repo_type,\n    #token=\"YOUR_HF_TOKEN\"\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\n\n[INFO] Creating repo: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\n\nTK - see the demo link here: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo\n\n\n\n8.3 TK - Testing the live demo\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML('''\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n&gt;&lt;/iframe&gt;     \n''')",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nTeaching style: A machine learning cooking show!\nMottos:\nTODO:"
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-predictions-on-new-text-data-inference",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-predictions-on-new-text-data-inference",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "11 TK - Make predictions on new text data (inference)",
    "text": "11 TK - Make predictions on new text data (inference)\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nsample_text = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n\n11.1 Pipeline mode\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\n\n\nimport torch\nfrom transformers import pipeline\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=\"./learn_hf_food_not_food_text_classifier_model\",\n                                    batch_size=64,\n                                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfood_not_food_classifier(sample_text)\n\n[{'label': 'food', 'score': 0.9857270121574402}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9952113032341003}]\n\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"I'm feeling overwhelmed by this workload ‚Äì it's a real information buffet.\",\n    \"We're brainstorming new content ideas, hoping to cook up something innovative.\",\n    \"Daniel Bourke is really cool :D\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'food', 'score': 0.5004891753196716},\n {'label': 'not_food', 'score': 0.8031825423240662},\n {'label': 'food', 'score': 0.5688743591308594},\n {'label': 'food', 'score': 0.5170369744300842},\n {'label': 'not_food', 'score': 0.6362243890762329},\n {'label': 'not_food', 'score': 0.7544246315956116},\n {'label': 'not_food', 'score': 0.7407550811767578},\n {'label': 'not_food', 'score': 0.5384440422058105},\n {'label': 'not_food', 'score': 0.863006055355072},\n {'label': 'not_food', 'score': 0.9562841653823853},\n {'label': 'not_food', 'score': 0.9076286554336548}]\n\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {end_time - start_time} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {(end_time - start_time) / len(sentences_big)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 110\n[INFO] Inference time for 110 sentences: 0.06326603889465332 seconds.\n[INFO] Avg inference time per sentence: 0.000575145808133212 seconds.\n\n[INFO] Number of sentences: 1100\n[INFO] Inference time for 1100 sentences: 0.341522216796875 seconds.\n[INFO] Avg inference time per sentence: 0.00031047474254261364 seconds.\n\n[INFO] Number of sentences: 11000\n[INFO] Inference time for 11000 sentences: 1.562863826751709 seconds.\n[INFO] Avg inference time per sentence: 0.00014207852970470083 seconds.\n\n[INFO] Number of sentences: 110000\n[INFO] Inference time for 110000 sentences: 15.670900344848633 seconds.\n[INFO] Avg inference time per sentence: 0.00014246273040771485 seconds.\n\nCPU times: user 17.2 s, sys: 493 ms, total: 17.6 s\nWall time: 17.6 s\n\n\n\n\n11.2 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---evaluation",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---evaluation",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 TK - Evaluation",
    "text": "9 TK - Evaluation\n\nTK - What evaluation metrics are there?\n\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---training-our-model",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---training-our-model",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "6 TK - Training our model",
    "text": "6 TK - Training our model\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#train\nSteps for training:\n\nDefine model\nDefine training arguments\nPass training arguments to Trainer\nCall train()\n\n\nTK - What kind of training are we doing? Supervised learning + fine-tuning an existing model\n\n\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label,\n    label2id=label2id\n)\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nTK - notice this output on pretraining advice\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: [‚Äòclassifier.bias‚Äô, ‚Äòclassifier.weight‚Äô, ‚Äòpre_classifier.bias‚Äô, ‚Äòpre_classifier.weight‚Äô] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nLet‚Äôs try and make a prediction with our model and see what happens.\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][:2])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[43], line 2\n      1 # Try and make a prediction with the loaded model (this will error)\n----&gt; 2 model(**tokenized_dataset[\"train\"][:2])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\n\n6.1 TK - Create a directory for saving models\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n\n6.2 TK - Setup training arguments\n\nTK - add markdown table of different parameters and what they do (e.g.¬†most of the common ones but add a note that these may want to be changed depending on the problem + there are many more in the docs)\n\n\n# Create training arguments\n# See: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n# TODO: Turn off Weights & Biases logging? Or add it in?\n# TK - exercise: spend 10 minutes reading the TrainingArguments documentation\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir, # TODO: change this path to model save path, e.g. 'learn_hf_food_not_food_text_classifier_model' \n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\", # log training results every epoch\n    report_to=\"none\" # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n)\n\n\n\n6.3 TK - Setup trainer class\n\n# Setup Trainer\n# Note: Trainer applies dynamic padding by default when you pass `tokenizer` to it.\n# In this case, you don't need to specify a data collator explicitly.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator, # not necessary if using pre-built tokenizer padding (default)\n    compute_metrics=compute_metrics\n)\n\n\n\n6.4 TK - Train the model\n\nresults = trainer.train()\n\n\n    \n      \n      \n      [70/70 00:06, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.615200\n0.450918\n1.000000\n\n\n2\n0.405600\n0.257541\n1.000000\n\n\n3\n0.219900\n0.123121\n1.000000\n\n\n4\n0.108100\n0.062602\n1.000000\n\n\n5\n0.056800\n0.036242\n1.000000\n\n\n6\n0.035900\n0.025235\n1.000000\n\n\n7\n0.026700\n0.019986\n1.000000\n\n\n8\n0.021900\n0.017336\n1.000000\n\n\n9\n0.019400\n0.016042\n1.000000\n\n\n10\n0.018200\n0.015633\n1.000000\n\n\n\n\n\n\n\n\n6.5 TK - Inspect the model results\n\n# TK - go through these\ntotal_train_time = results.metrics[\"train_runtime\"]\ntrain_samples_per_second = results.metrics[\"train_samples_per_second\"]\n\nprint(f\"Total training time: {total_train_time} seconds\")\nprint(f\"Training samples per second: {train_samples_per_second}\")\n\nTotal training time: 6.7168 seconds\nTraining samples per second: 297.761\n\n\n\n# TK - get loss curves\ntrainer_history = trainer.state.log_history[:-1]\ntrainer_training_time = trainer_history[-1]\ntrainer_history[:5]\n\n[{'loss': 0.6152,\n  'grad_norm': 3.3377952575683594,\n  'learning_rate': 1.8e-05,\n  'epoch': 1.0,\n  'step': 7},\n {'eval_loss': 0.45091766119003296,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0113,\n  'eval_samples_per_second': 4423.998,\n  'eval_steps_per_second': 176.96,\n  'epoch': 1.0,\n  'step': 7},\n {'loss': 0.4056,\n  'grad_norm': 2.4789676666259766,\n  'learning_rate': 1.6000000000000003e-05,\n  'epoch': 2.0,\n  'step': 14},\n {'eval_loss': 0.25754112005233765,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0124,\n  'eval_samples_per_second': 4023.931,\n  'eval_steps_per_second': 160.957,\n  'epoch': 2.0,\n  'step': 14},\n {'loss': 0.2199,\n  'grad_norm': 1.6385667324066162,\n  'learning_rate': 1.4e-05,\n  'epoch': 3.0,\n  'step': 21}]\n\n\n\n# Extract training and evaluation metrics\ntrainer_history_training_set = []\ntrainer_history_eval_set = []\n\nfor item in trainer_history[:-1]:\n    item_keys = list(item.keys())\n    if any(\"eval\" in item for item in item_keys):\n        trainer_history_eval_set.append(item)\n    else:\n        trainer_history_training_set.append(item)\n\n\ntrainer_history_training_df = pd.DataFrame(trainer_history_training_set)\ntrainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n\ntrainer_history_training_df\n\n\n\n\n\n\n\n\nloss\ngrad_norm\nlearning_rate\nepoch\nstep\n\n\n\n\n0\n0.6152\n3.337795\n0.000018\n1.0\n7\n\n\n1\n0.4056\n2.478968\n0.000016\n2.0\n14\n\n\n2\n0.2199\n1.638567\n0.000014\n3.0\n21\n\n\n3\n0.1081\n0.902428\n0.000012\n4.0\n28\n\n\n4\n0.0568\n0.546689\n0.000010\n5.0\n35\n\n\n5\n0.0359\n0.347724\n0.000008\n6.0\n42\n\n\n6\n0.0267\n0.309794\n0.000006\n7.0\n49\n\n\n7\n0.0219\n0.273363\n0.000004\n8.0\n56\n\n\n8\n0.0194\n0.244860\n0.000002\n9.0\n63\n\n\n9\n0.0182\n0.245236\n0.000000\n10.0\n70\n\n\n\n\n\n\n\n\ntrainer_history_eval_df\n\n\n\n\n\n\n\n\neval_loss\neval_accuracy\neval_runtime\neval_samples_per_second\neval_steps_per_second\nepoch\nstep\n\n\n\n\n0\n0.450918\n1.0\n0.0113\n4423.998\n176.960\n1.0\n7\n\n\n1\n0.257541\n1.0\n0.0124\n4023.931\n160.957\n2.0\n14\n\n\n2\n0.123121\n1.0\n0.0115\n4338.068\n173.523\n3.0\n21\n\n\n3\n0.062602\n1.0\n0.0115\n4349.855\n173.994\n4.0\n28\n\n\n4\n0.036242\n1.0\n0.0112\n4448.585\n177.943\n5.0\n35\n\n\n5\n0.025235\n1.0\n0.0122\n4100.485\n164.019\n6.0\n42\n\n\n6\n0.019986\n1.0\n0.0116\n4327.147\n173.086\n7.0\n49\n\n\n7\n0.017336\n1.0\n0.0113\n4406.522\n176.261\n8.0\n56\n\n\n8\n0.016042\n1.0\n0.0116\n4315.128\n172.605\n9.0\n63\n\n\n\n\n\n\n\n\n# Plot training and evaluation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\nplt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and evaluation loss over time\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.6 TK - Save the model for later use\n\n# Save model\n# See docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.save_model \ntrainer.save_model(model_save_dir)\n\n\n\n6.7 TK - Push the model to Hugging Face Hub\nTK - optional to share the model/use elsewhere\n\nsee here: https://huggingface.co/docs/transformers/en/model_sharing\nalso see here for how to setup huggingface-cli so you can write your model to your account\n\n\n# TK - have a note here for the errors\n# Note: you may see the following error\n# 403 Forbidden: You don't have the rights to create a model under the namespace \"mrdbourke\".\n# Cannot access content at: https://huggingface.co/api/repos/create.\n# If you are trying to create or update content,make sure you have a token with the `write` role.\n\n\n# TK - Push model to hub (for later re-use)\n# TODO: Push this model to the hub to be able to use it later\n# TK - this requires a \"write\" token from the Hugging Face Hub\n# TK - see docs: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub \n# TK - for example, on my local computer, my token is saved to: \"/home/daniel/.cache/huggingface/token\"\n\n# TK - Can create a model card with create_model_card()\n# see here: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.Trainer.create_model_card \n\ntrainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\" # set to False if you want the model to be public\n    # token=\"YOUR_HF_TOKEN_HERE\" # note: this will default to the token you have saved in your Hugging Face config\n)\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/7c9a4a6b17da981559f484538d51f6ff9a14c12d', commit_message='Uploading food not food text classifier model', commit_description='', oid='7c9a4a6b17da981559f484538d51f6ff9a14c12d', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nTK - note: this will make the model public, to make it private,\n\nSee the model here saved for later: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\n6.8 TK - Make and evaluate predictions on the test set\n\n# Perform predictions on the test set\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_metrics = predictions_all.metrics\nprediction_metrics\n\n\n\n\n{'test_loss': 0.015632618218660355,\n 'test_accuracy': 1.0,\n 'test_runtime': 0.0391,\n 'test_samples_per_second': 1280.07,\n 'test_steps_per_second': 51.203}\n\n\n\npredictions_all\n\nPredictionOutput(predictions=array([[-2.261428 ,  1.890655 ],\n       [ 1.8613493, -1.8532594],\n       [-2.2970695,  1.9171791],\n       [ 2.187019 , -2.1593657],\n       [ 2.1193414, -2.1615388],\n       [-2.2868803,  1.9454829],\n       [ 2.0827348, -2.1099336],\n       [ 2.154141 , -2.1266923],\n       [-2.279855 ,  1.9362432],\n       [-2.277952 ,  1.9518106],\n       [-2.2772808,  1.9423369],\n       [-1.9777709,  1.5732591],\n       [ 2.1512635, -2.0508409],\n       [-2.3032587,  1.9534686],\n       [-2.138177 ,  1.7531359],\n       [ 2.194142 , -2.1277084],\n       [-2.2709608,  1.9498663],\n       [ 1.9596925, -1.919577 ],\n       [-2.2827635,  1.9249418],\n       [-2.290854 ,  1.9592198],\n       [-2.2823153,  1.8799024],\n       [-2.3003585,  1.9387653],\n       [ 2.043029 , -2.0384376],\n       [ 2.0885575, -2.1244206],\n       [-2.2873669,  1.9443382],\n       [-2.2972584,  1.9009027],\n       [-2.2450745,  1.8596792],\n       [ 2.1050394, -2.040059 ],\n       [-2.2972147,  1.8946056],\n       [ 2.130832 , -2.133735 ],\n       [-2.2846339,  1.9422101],\n       [-2.2931519,  1.9279182],\n       [-2.3040657,  1.9485677],\n       [ 2.1816792, -2.141174 ],\n       [-2.3019922,  1.9271733],\n       [-2.2885954,  1.9124153],\n       [-2.2813184,  1.9542999],\n       [-2.304743 ,  1.8892938],\n       [ 2.1249578, -2.089177 ],\n       [ 2.043159 , -1.941504 ],\n       [-2.1469579,  1.8099191],\n       [-2.269732 ,  1.9235427],\n       [-2.0776005,  1.7352381],\n       [ 1.9634217, -2.0820174],\n       [-2.2788396,  1.9341636],\n       [-2.2946444,  1.9408271],\n       [-2.2920046,  1.9059081],\n       [-2.3030152,  1.9264866],\n       [ 2.1768198, -2.1458352],\n       [-2.301217 ,  1.9053475]], dtype=float32), label_ids=array([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 1, 0, 1]), metrics={'test_loss': 0.015632618218660355, 'test_accuracy': 1.0, 'test_runtime': 0.0391, 'test_samples_per_second': 1280.07, 'test_steps_per_second': 51.203})\n\n\n\npredictions_all._asdict().keys()\n\ndict_keys(['predictions', 'label_ids', 'metrics'])\n\n\n\nimport torch\npred_probs = torch.softmax(torch.tensor(predictions_all.predictions), dim=1)\npred_labels = np.argmax(predictions_all.predictions, axis=1)\ntrue_labels = dataset[\"test\"][\"label\"]\n\n# Calculate accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(true_labels, pred_labels)\naccuracy\n\n1.0\n\n\n\n# Make a DataFrame of test predictions\ntest_predictions_df = pd.DataFrame({\n    \"text\": dataset[\"test\"][\"text\"],\n    \"true_label\": true_labels,\n    \"pred_label\": pred_labels,\n    \"pred_prob\": torch.max(pred_probs, dim=1).values\n})\n\ntest_predictions_df.head()\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.984512\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.976215\n\n\n2\nA bowl of sliced bell peppers with a sprinkle ...\n1\n1\n0.985432\n\n\n3\nSet of mugs hanging on a hook\n0\n0\n0.987212\n\n\n4\nStanding floor lamp providing light next to an...\n0\n0\n0.986358\n\n\n\n\n\n\n\n\n# Show 10 examples with low prediction probability\n# TK - this is good to find samples where the model is unsure \ntest_predictions_df.sort_values(\"pred_prob\").head(10)\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n11\nA close-up shot of a cheesy pizza slice being ...\n1\n1\n0.972105\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.976215\n\n\n42\nBoxes of apples, pears, pineapple, manadrins a...\n1\n1\n0.978392\n\n\n17\nRelaxing on the porch, a couple enjoys the com...\n0\n0\n0.979753\n\n\n14\nTwo handfuls of bananas in a fruit bowl with g...\n1\n1\n0.979990\n\n\n40\nA bowl of cherries with a sprig of mint for ga...\n1\n1\n0.981236\n\n\n39\nA close-up of a woman practicing yoga in the l...\n0\n0\n0.981741\n\n\n43\nSet of muffin tins stacked together\n0\n0\n0.982799\n\n\n22\nTwo people sitting at a dining room table with...\n0\n0\n0.983398\n\n\n26\nA fruit platter with a variety of exotic fruit...\n1\n1\n0.983774",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---setup-evaluation-metrics",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---setup-evaluation-metrics",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 TK - Setup Evaluation Metrics",
    "text": "9 TK - Setup Evaluation Metrics\n\nTK - What evaluation metrics are there?\n\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---setup-evaluation-metric",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---setup-evaluation-metric",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "5 TK - Setup Evaluation Metric",
    "text": "5 TK - Setup Evaluation Metric\n\nTK - What evaluation metrics are there?\n\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#evaluate\n\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  predictions = np.argmax(predictions, axis=1)\n  return accuracy.compute(predictions=predictions, references=labels)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-predictions-on-new-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-predictions-on-new-text-data",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "11 TK - Make predictions on new text data",
    "text": "11 TK - Make predictions on new text data\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nsample_text = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n\n11.1 Pipeline mode\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\n\n\nimport torch\nfrom transformers import pipeline\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=\"./learn_hf_food_not_food_text_classifier_model\",\n                                    batch_size=64,\n                                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfood_not_food_classifier(sample_text)\n\n[{'label': 'food', 'score': 0.9857270121574402}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9952113032341003}]\n\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"I'm feeling overwhelmed by this workload ‚Äì it's a real information buffet.\",\n    \"We're brainstorming new content ideas, hoping to cook up something innovative.\",\n    \"Daniel Bourke is really cool :D\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'food', 'score': 0.5004891753196716},\n {'label': 'not_food', 'score': 0.8031825423240662},\n {'label': 'food', 'score': 0.5688743591308594},\n {'label': 'food', 'score': 0.5170369744300842},\n {'label': 'not_food', 'score': 0.6362243890762329},\n {'label': 'not_food', 'score': 0.7544246315956116},\n {'label': 'not_food', 'score': 0.7407550811767578},\n {'label': 'not_food', 'score': 0.5384440422058105},\n {'label': 'not_food', 'score': 0.863006055355072},\n {'label': 'not_food', 'score': 0.9562841653823853},\n {'label': 'not_food', 'score': 0.9076286554336548}]\n\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {end_time - start_time} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {(end_time - start_time) / len(sentences_big)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 110\n[INFO] Inference time for 110 sentences: 0.06326603889465332 seconds.\n[INFO] Avg inference time per sentence: 0.000575145808133212 seconds.\n\n[INFO] Number of sentences: 1100\n[INFO] Inference time for 1100 sentences: 0.341522216796875 seconds.\n[INFO] Avg inference time per sentence: 0.00031047474254261364 seconds.\n\n[INFO] Number of sentences: 11000\n[INFO] Inference time for 11000 sentences: 1.562863826751709 seconds.\n[INFO] Avg inference time per sentence: 0.00014207852970470083 seconds.\n\n[INFO] Number of sentences: 110000\n[INFO] Inference time for 110000 sentences: 15.670900344848633 seconds.\n[INFO] Avg inference time per sentence: 0.00014246273040771485 seconds.\n\nCPU times: user 17.2 s, sys: 493 ms, total: 17.6 s\nWall time: 17.6 s\n\n\n\n\n11.2 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-and-inspect-predictions-on-new-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---make-and-inspect-predictions-on-new-text-data",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "7 TK - Make and inspect predictions on new text data",
    "text": "7 TK - Make and inspect predictions on new text data\nUPTOHERE - load the model (locally + from Hub) - make sure to change the save paths when loading the model to the new paths - make predictions on new text data - build a demo with Gradio (optional)\nMaking predictions on our own text options.\nSee: https://huggingface.co/docs/transformers/en/tasks/sequence_classification#inference\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n# Setup local model path\nlocal_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\nhuggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n\n7.1 TK - Pipeline mode\n\nTk - what is a pipeline?\n\n\n# TODO: TK - set device agnostic code for CUDA/Mac/CPU?\ndef set_device():\n    \"\"\"\n    Set device to CUDA if available, else MPS (Mac), else CPU.\n\n    This defaults to using the best available device (usually).\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n\nDEVICE = set_device()\nprint(f\"[INFO] Using device: {DEVICE}\")\n\n[INFO] Using device: cuda\n\n\n\nimport torch\nfrom transformers import pipeline\n\n# Setup batch size for batched inference (can be adjusted depending on how much memory is available)\n# TK - why use batch size? -&gt; multiple samples at inference = faster\nBATCH_SIZE = 64\n\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=local_model_path,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE)\n\n\nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\nfood_not_food_classifier(sample_text_food)\n\n[{'label': 'food', 'score': 0.99871826171875}]\n\n\n\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[{'label': 'not_food', 'score': 0.9989410042762756}]\n\n\n\n# Pipeline also works with remote models (will have to laod the model locally first)\nfood_not_food_classifier_remote = pipeline(task=\"text-classification\", \n                                           model=huggingface_model_path,\n                                           batch_size=BATCH_SIZE,\n                                           device=DEVICE)\n\nfood_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")\n\n[{'label': 'food', 'score': 0.9981549382209778}]\n\n\n\n\n7.2 TK - Batch prediction\n\nTK - what is batch prediction?\n\n\n# Predicting works with lists\n# Can find the examples with highest confidence and keep those\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"Daniel Bourke is really cool :D\",\n    \"My favoruite food is biltong!\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9410305619239807},\n {'label': 'not_food', 'score': 0.9650871753692627},\n {'label': 'not_food', 'score': 0.9215793609619141},\n {'label': 'not_food', 'score': 0.9115400910377502},\n {'label': 'not_food', 'score': 0.9625208377838135},\n {'label': 'not_food', 'score': 0.9476941823959351},\n {'label': 'not_food', 'score': 0.9451109170913696},\n {'label': 'not_food', 'score': 0.9027702808380127},\n {'label': 'not_food', 'score': 0.9954429864883423},\n {'label': 'food', 'score': 0.7653573155403137}]\n\n\n\n\n7.3 TK - Time our model across larger sample sizes\n\nTK - our model is fast!\n\n\n%%time\nimport time\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n    print()\n\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n[INFO] Number of sentences: 100\n[INFO] Inference time for 100 sentences: 0.07726 seconds.\n[INFO] Avg inference time per sentence: 0.0007726 seconds.\n\n[INFO] Number of sentences: 1000\n[INFO] Inference time for 1000 sentences: 0.32344 seconds.\n[INFO] Avg inference time per sentence: 0.00032344 seconds.\n\n[INFO] Number of sentences: 10000\n[INFO] Inference time for 10000 sentences: 1.43834 seconds.\n[INFO] Avg inference time per sentence: 0.00014383 seconds.\n\n[INFO] Number of sentences: 100000\n[INFO] Inference time for 100000 sentences: 14.4585 seconds.\n[INFO] Avg inference time per sentence: 0.00014459 seconds.\n\nCPU times: user 15.8 s, sys: 552 ms, total: 16.3 s\nWall time: 16.3 s\n\n\n\n\n7.4 PyTorch mode\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\ninputs = tokenizer(sample_text_food, return_tensors=\"pt\")\n\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\nwith torch.no_grad():\n  logits = model(**inputs).logits\n\n\n# Get predicted class\npredicted_class_id = logits.argmax().item()\nprint(f\"Text: {sample_text_food}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---exercises-and-extensions",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---exercises-and-extensions",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "9 TK - Exercises and Extensions",
    "text": "9 TK - Exercises and Extensions\n\nWhere does our model fail? E.g. what kind of sentences does it struggle with? How could you fix this?\n\nMake an extra 10-50 examples of these and add them to the dataset and then retrain the model\nSee here: https://discuss.huggingface.co/t/how-do-i-add-things-rows-to-an-already-saved-dataset/27423\n\nBuild your own text classifier on a different dataset/your own custom dataset\nHow might we make our dataset multi-class? (e.g.¬†more than 2 classes)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---extra-resources",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---extra-resources",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "10 TK - Extra resources",
    "text": "10 TK - Extra resources",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#tk---overview",
    "title": "Text Classification with Hugging Face Transformers",
    "section": "1 TK - Overview",
    "text": "1 TK - Overview\n\n1.1 TK - What we‚Äôre going to build\nIn this project, we‚Äôre going to learn various aspects of the Hugging Face ecosystem whilst building a text classification model.\nTo keep things as practical as possible, we‚Äôre going to be bulding a food/not_food text classification model.\nGiven a piece of a text, our model will be able to predict if it‚Äôs about food or not.\nThis is the same kind of model I use in my own work on Nutrify (an app to help people learn about food).\nMore specifically, we‚Äôre going to follow the following steps:\n\nProblem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nFinding, training and evaluating a model - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\nCreating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others.\nTK image - see the finished product (demo)\n\n\n\n\n\n\nNote\n\n\n\nNote this is a hands-on project, so we‚Äôll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we‚Äôre doing, I‚Äôll leave links in the extra-curriculum section.\n\n\n\n\n1.2 TK - What is Hugging Face?\nHugging Face is a platform that offers access to many different kinds of open-source machine learning models and datasets.\nThey‚Äôre also the creators of the popular transformers library which is a Python-based library for working with pre-trained models as well as custom models and datasets.\nIf you‚Äôre getting into the world of AI and machine learning, you‚Äôre going to come across Hugging Face.\n\n\n1.3 TK - Why Hugging Face?\nMany of the biggest companies in the world use Hugging Face for their open-source machine learning projects including Apple, Google, Facebook (Meta), Microsoft, OpenAI, ByteDance and more.\nTK image - image of people using Hugging Face\nNot only does Hugging Face make it so you can use state-of-the-art machine learning models such as Stable Diffusion (for image generation) and Whipser (for audio transcription) easily, it also makes it so you can share your own models, datasets and resources.\nConsider Hugging Face the homepage of your AI/machine learning profile.\n\n\n1.4 TK - What is text classification?\nText classification is the process of assigning a category to a piece of text.\nWhere a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\nTK image - example of text classification\nExample text classification problems include:\n\n\n\n\n\n\n\n\nProblem\nDescription\nProblem Type\n\n\n\n\nSpam email detection\nIs an email spam or not spam?\nBinary classification (one thing or another)\n\n\nSentiment analysis\nIs a piece of text positive, negative or neutral?\nMulti-class classification (one thing from many)\n\n\nLanguage detection\nWhat language is a piece of text written in?\nMulti-class classification (one thing from many)\n\n\nTopic classification\nWhat topic(s) does a news article belong to?\nMulti-label classification (one or more things from many)\n\n\nHate speech detection\nIs a comment hateful or not hateful?\nBinary classification (one thing or another)\n\n\nProduct categorization\nWhat categories does a product belong to?\nMulti-label classification (one or more things from many)\n\n\n\nThere are several different kinds of models you can use for text classification.\nAnd each will have its pros and cons depending on the problem you‚Äôre working on.\nExample text classification models include:\n\n\n\nModel\nDescription\nPros\nCons\n\n\n\n\nRule-based\nUses a set of rules to classify text (e.g.¬†if text contains ‚Äúsad‚Äù -&gt; sentiment = low)\nSimple, easy to understand\nRequires manual creation of rules\n\n\nBag of Words\nCounts the frequency of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nTF-IDF\nWeighs the importance of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nDeep learning-based models\nUses neural networks to learn patterns in text\nCan learn complex patterns at scale\nCan require large amounts of data/compute power to run, not as easy to understand (can be hard to debug)\n\n\n\nWe‚Äôre going to use a deep learning model our case.\nWhy?\nBecause Hugging Face helps us do so.\nAnd in most cases, with a large enough dataset, a deep learning model will often perform better than a rule-based or other model.\n\n\n1.5 TK - Why train your own text classification models?\nYou can use pre-trained models for text classification as well as API-powered models and LLMs such as GPT-4 or Gemini.\nHowever, it‚Äôs often a good idea to train your own text classification models for a few reasons:\n\nThey can be much faster than API-powered models (since they‚Äôre running on your own hardware, this can save on costs and time).\nThey‚Äôre customized to your own data.\nThey don‚Äôt require you to send your data elsewhere (privacy).\nIf a service goes down, you‚Äôll still have access to your model (reliability).\n\nTK image - example of training your own model vs using an API-powered model",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Text Classification (work in progress)"
    ]
  },
  {
    "objectID": "extras/todo.html",
    "href": "extras/todo.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Add ‚Äúgetting setup‚Äù file to get started locally with the required dependencies\nAdd text classification dataset creation notebook (so people can see where the data comes from)\nAdd a Hugging Face ecosystem overview (transformers, datasets, tokenizers, torch, Hugging Face Hub, Hugging Face Spaces, etc.)"
  },
  {
    "objectID": "extras/todo.html#quarto-misc",
    "href": "extras/todo.html#quarto-misc",
    "title": "Learn Hugging Face ü§ó",
    "section": "Quarto misc",
    "text": "Quarto misc\n\nCreate share cards - https://quarto.org/docs/websites/website-tools.html#twitter-cards\n\nSee here for share image - https://quarto.org/docs/websites/website-tools.html#preview-images"
  }
]